{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94abf3ee",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_41897800/article/details/114333091\n",
    "\n",
    "torch里面的Transformer库要怎么用？例子如下：\n",
    "https://github.com/CVxTz/time_series_forecasting/blob/26de501ec28bab1153a8a7abb2ad664d446ada57/time_series_forecasting/model.py#L69\n",
    "\n",
    "看了一些实现，发现torch自带的transformer encoder decoder是没有pad，mask这样的功能的，得我们自己加进去。tnnd，这不误事嘛。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/353365423 关于一些mask要怎么用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f61d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np      # tang.npz的压缩格式处理\n",
    "import os       # 打开文件\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchnet import meter\n",
    "import tqdm, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6351db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    if os.path.exists(data_path):\n",
    "        datas = np.load(data_path, allow_pickle=True)      # 加载数据\n",
    "        data = datas['data']  # numpy.ndarray\n",
    "        word2ix = datas['word2ix'].item()   # dic\n",
    "        ix2word = datas['ix2word'].item()  # dic\n",
    "        return data, word2ix, ix2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a40d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(dt):\n",
    "    '''\n",
    "    给出一个pad的mask，就是把pad符号位置标为true。\n",
    "    '''\n",
    "    pad_attn_mask = dt.data.eq(8292)\n",
    "    return pad_attn_mask\n",
    "\n",
    "def replace_start_with_pad(dt):\n",
    "    '''\n",
    "    就是把start符号转为pad符号。\n",
    "    '''\n",
    "    return dt.masked_fill_(dt.eq(8291), 8292)\n",
    "\n",
    "def move_pad_to_tail(dt_ori):\n",
    "    '''\n",
    "    本来的数据是pad填充在前面，这个函数可以把pad挪移到每一句的后面。\n",
    "    '''\n",
    "    dt = dt_ori.clone() ## https://zhuanlan.zhihu.com/p/344458484\n",
    "    for i in range(dt.shape[0]):\n",
    "        idx = int(torch.nonzero(dt[i]==8291).squeeze())\n",
    "        new_line = torch.cat(\n",
    "            (dt[i][idx:], dt[i][:idx]), 0\n",
    "        )\n",
    "        dt[i] = new_line\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3f4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_trg_mask(length, device = \"cpu\"):\n",
    "    mask = torch.tril(torch.ones(length, length, device=device)) == 1\n",
    "\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "\n",
    "    return mask\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        Unmasked positions are filled with float(0.0).\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8d9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../originalDataset/tang.npz'\n",
    "data, word2ix, ix2word = get_data()\n",
    "data = torch.from_numpy(data)\n",
    "dataloader = torch.utils.data.DataLoader(data, batch_size=10, shuffle=True, num_workers=1)  # shuffle=True随机打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9570ff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([57580, 125])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd1df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缓步出居处，过原边雁行。夕阳投草木，远水迎苍茫。野寺同蟾宿，云溪劚药尝。萧条霜景暮，极目尽堪伤。\n",
      "\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><START>缓步出居处，过原边雁行。夕阳投草木，远水迎苍茫。野寺同蟾宿，云溪劚药尝。萧条霜景暮，极目尽堪伤。<EOP>\n",
      "\n",
      "tensor([8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292,\n",
      "        8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292,\n",
      "        8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292,\n",
      "        8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292,\n",
      "        8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292,\n",
      "        8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292, 8292,\n",
      "        8292, 8292, 8292, 8291,  239, 6018, 6673, 5951, 6144, 7066,  351, 1897,\n",
      "        3754, 5642, 7905, 7435, 6640, 3788, 1891,  925, 5913, 7066, 4503, 2219,\n",
      "        7933, 5261, 2323, 7435, 6522, 7062, 4518, 3805, 2423, 7066, 3534, 5283,\n",
      "        7605,  396, 1523, 7435, 1989, 5617, 3951, 6278, 2325, 7066,  401, 5448,\n",
      "        2581, 2543, 7834, 7435, 8290], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "def print_origin_poem(i):\n",
    "    '''\n",
    "    data里面每一行就是一首诗，总共57580首。\n",
    "    本函数可以打印出data[i]对应的诗。\n",
    "    '''\n",
    "    poem_txt = [ix2word[int(i)] for i in data[i]]\n",
    "    print(\"\".join(poem_txt[poem_txt.index(\"<START>\") + 1:-1]))\n",
    "    print()\n",
    "    print(\"\".join(poem_txt[:]))\n",
    "    print()\n",
    "    print(data[i])\n",
    "print_origin_poem(45423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724e6a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/5758 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 125])\n",
      "torch.Size([125, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 看看总共循环多少次\n",
    "for dt in tqdm.tqdm(dataloader):\n",
    "    print(dt.shape)\n",
    "    print(dt.transpose(1, 0).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ba308",
   "metadata": {},
   "source": [
    "8291是start，8290是end，8292是pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b30e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=2, batch_first=False)\n",
    "        # lstm输入为：seq, batch, input_size\n",
    "        # lstm输出为：seq * batch * 256; (2 * batch * 256,...)\n",
    "        self.linear1 = nn.Linear(self.hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        seq_len, batch_size = input.size()\n",
    "        if hidden is None:\n",
    "            h_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            h_0, c_0 = Variable(h_0), Variable(c_0)\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "        embeds = self.embeddings(input)     # (seq_len, batch_size, embedding_dim), (124,128,128) ## 难道不应该是(124, 10, 128)\n",
    "        output, hidden = self.lstm(embeds, (h_0, c_0))      #(seq_len, batch_size, hidden_dim), (124,128,256)\n",
    "        output = self.linear1(output.view(seq_len*batch_size, -1))      # ((seq_len * batch_size),hidden_dim), (15872,256) → (15872,8293)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a7c7be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'秦川雄帝宅，函谷壮皇居。绮殿千寻起，离宫百雉余。连薨遥接汉，飞观迥凌虚。云日隐层阙，风烟出绮疏。岩廊罢机务，崇文聊驻辇。玉匣启龙图，金绳披凤篆。韦编断仍续，缥帙舒还卷。对此乃淹留，欹案观坟典。移步出词林，停舆欣武宴。雕弓写明月，骏马疑流电。惊雁落虚弦，啼猿悲急箭。阅赏诚多美，于兹乃忘倦。鸣笳临乐馆，眺听欢芳节。急管韵朱弦，清歌凝白雪。彩凤肃来仪，玄鹤纷成列。去兹郑卫声，雅音方可悦。芳辰追逸趣，禁苑信多奇。桥形通汉上，峰势接云危。烟霞交隐映，花鸟自参差。何如肆辙迹，万里赏瑶池。飞盖去芳园，兰桡游翠渚。萍间日彩乱，荷处香风举。桂楫满中川，弦歌振长屿。岂必汾河曲，方为欢宴所。落日双阙昏，回舆九重暮。长烟散初碧，皎月澄轻素。搴幌玩琴书，开轩引云雾。斜汉耿层阁，清风摇玉树。欢乐难再逢，芳辰良可惜。玉酒泛云罍，兰殽陈绮席。千钟合尧禹，百兽谐金石。得志重寸阴，忘怀轻尺璧。建章欢赏夕，二八尽妖妍。罗绮昭阳殿，芬芳玳瑁筵。佩移星正动，扇掩月初圆。无劳上悬圃，即此对神仙。以兹游观极，悠然独长想。披卷览前踪，抚躬寻既往。望古茅茨约，瞻今兰殿广。人道恶高危，虚心戒盈荡。奉天竭诚敬，临民思惠养。纳善察忠谏，明科慎刑赏。六五诚难继，四三非易仰。广待淳化敷，方嗣云亭响。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import opencc, re\n",
    "cc = opencc.OpenCC('t2s')\n",
    "cc.convert(\"秦川雄帝宅，函谷壮皇居。绮殿千寻起，离宫百雉余。连薨遥接汉，飞观迥凌虚。云日隐层阙，风烟出绮疏。岩廊罢机务，崇文聊驻辇。玉匣启龙图，金绳披凤篆。韦编断仍续，缥帙舒还卷。对此乃淹留，欹案观坟典。移步出词林，停舆欣武宴。雕弓写明月，骏马疑流电。惊雁落虚弦，啼猿悲急箭。阅赏诚多美，于兹乃忘倦。鸣笳临乐馆，眺听欢芳节。急管韵朱弦，清歌凝白雪。彩凤肃来仪，玄鹤纷成列。去兹郑卫声，雅音方可悦。芳辰追逸趣，禁苑信多奇。桥形通汉上，峰势接云危。烟霞交隐映，花鸟自参差。何如肆辙迹，万里赏瑶池。飞盖去芳园，兰桡游翠渚。萍间日彩乱，荷处香风举。桂楫满中川，弦歌振长屿。岂必汾河曲，方为欢宴所。落日双阙昏，回舆九重暮。长烟散初碧，皎月澄轻素。搴幌玩琴书，开轩引云雾。斜汉耿层阁，清风摇玉树。欢乐难再逢，芳辰良可惜。玉酒泛云罍，兰殽陈绮席。千钟合尧禹，百兽谐金石。得志重寸阴，忘怀轻尺璧。建章欢赏夕，二八尽妖妍。罗绮昭阳殿，芬芳玳瑁筵。佩移星正动，扇掩月初圆。无劳上悬圃，即此对神仙。以兹游观极，悠然独长想。披卷览前踪，抚躬寻既往。望古茅茨约，瞻今兰殿广。人道恶高危，虚心戒盈荡。奉天竭诚敬，临民思惠养。纳善察忠谏，明科慎刑赏。六五诚难继，四三非易仰。广待淳化敷，方嗣云亭响。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb9b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_cn(w):\n",
    "  #将繁体字转换为简体字  \n",
    "    w = cc.convert(w)\n",
    "    w = ' '.join(list(w))\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = w.strip().rstrip()\n",
    "    w = '<START> ' + w + ' <EOP>'\n",
    "    new_list = []\n",
    "    for i in w.split():\n",
    "        new_list.append(\n",
    "            word2ix.get(i, 8292)\n",
    "        )\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7dd2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def create_dataset(path):\n",
    "    lines = io.open(path,encoding='utf8').read().strip().split('\\n')\n",
    "   \n",
    "    sentence_pairs = [[preprocess_sentence_cn(w) for w in line.replace(\"，\",\"。\").replace(\"。\", \" \").strip().split(' ')] for line in lines]\n",
    "    # print(sentence_pairs)\n",
    "    return zip(*sentence_pairs)\n",
    "\n",
    "a,b = create_dataset('../originalDataset/poem5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3577e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.array(list(a)), np.array(list(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb64074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_1 = torch.utils.data.DataLoader(a, batch_size=10, shuffle=True, num_workers=1)  # shuffle=True随机打乱\n",
    "dataloader_2 = torch.utils.data.DataLoader(b, batch_size=10, shuffle=True, num_workers=1)  # shuffle=True随机打乱"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37772a17",
   "metadata": {},
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, num_layers, d_model, num_heads, dff, \n",
    "        input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    " \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    " \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    " \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    " \n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    " \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    " \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    " \n",
    "        return final_output, attention_weights\n",
    "# ————————————————\n",
    "# 版权声明：本文为CSDN博主「孙宝龙」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "# 原文链接：https://blog.csdn.net/amao1998/article/details/116061920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520696f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651fbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "043ac7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Trsfmr_encoder_decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, nheads = 8, nlayers=6):\n",
    "        \n",
    "        super(Trsfmr_encoder_decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.poc_enc = PositionalEncoding(self.embedding_dim)\n",
    "        self.encode_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nheads) # , dim_feedforward=hidden_dim\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encode_layer, num_layers=nlayers)\n",
    "        \n",
    "        self.poc_dec = PositionalEncoding(self.embedding_dim)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=nheads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=nlayers)\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, input_att, input_real, hidden = None):\n",
    "#         print(input_att.shape, input_real.shape)\n",
    "        seq_len, batch_size = input_real.size()\n",
    "        \n",
    "        att_mask = gen_trg_mask(input_att.shape[0])\n",
    "        real_mask = gen_trg_mask(input_real.shape[0])\n",
    "        \n",
    "        ## encoder\n",
    "        # input_att = replace_start_with_pad(input_real)\n",
    "        embeds_ = self.embeddings(input_att)\n",
    "        embeds_ = self.poc_enc(embeds_)\n",
    "        pad_mask = get_attn_pad_mask(\n",
    "            input_att.long().transpose(0, 1).contiguous()\n",
    "        )\n",
    "        memory = self.transformer_encoder(\n",
    "            embeds_, \n",
    "            mask = att_mask,\n",
    "            src_key_padding_mask = pad_mask\n",
    "        )\n",
    "        \n",
    "        ## decoder\n",
    "        embeds = self.embeddings(input_real)\n",
    "        embeds = self.poc_dec(embeds)\n",
    "        # print(input_att.shape, input_real.shape, embeds.shape)\n",
    "        output = self.transformer_decoder(\n",
    "            ## \n",
    "            tgt = embeds, \n",
    "            memory = memory, \n",
    "            ## \n",
    "            tgt_mask = real_mask,\n",
    "            memory_mask = att_mask,\n",
    "            ## \n",
    "            tgt_key_padding_mask = get_attn_pad_mask(\n",
    "                input_real.long().transpose(0, 1).contiguous()\n",
    "            ),\n",
    "            memory_key_padding_mask = pad_mask,\n",
    "        )\n",
    "        \n",
    "        output = self.linear1(output.view(seq_len*batch_size, -1))\n",
    "        \n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ec36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (i, j) in tqdm.tqdm(enumerate(zip(dataloader_1, dataloader_2))):\n",
    "#     print(i.shape, j.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6eb4f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8292]], dtype=torch.int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "dt[\n",
    "    random.randint(0, dt.shape[0]-1),\n",
    "    random.randint(0, dt.shape[1]-1)\n",
    "].unsqueeze(0).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a34fb625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "056c97c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0, dt.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33d6ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_att, start_words, ix2word, word2ix):     # 给定几个词，根据这几个词生成一首完整的诗歌\n",
    "    txt = []\n",
    "    for word in start_words: # 机器学习\n",
    "        txt.append(word) ## txt: [机,器,学,习]\n",
    "    print(txt)\n",
    "    input = Variable(torch.Tensor([word2ix['<START>']]).view(1,1).long())      # tensor([8291.]) → tensor([[8291.]]) → tensor([[8291]])\n",
    "    print(input)\n",
    "    hidden = None\n",
    "    num = len(txt) # 5\n",
    "    print(num)\n",
    "    for i in range(48):      # 最大生成长度\n",
    "        # print(input_att.shape, input.shape)\n",
    "        output, hidden = model(\n",
    "            input_att[\n",
    "                random.randint(0, input_att.shape[0]-1),\n",
    "                random.randint(0, input_att.shape[1]-1)\n",
    "            ].unsqueeze(0).unsqueeze(1), \n",
    "#             torch.Tensor([[5836]]).long(),\n",
    "            input, \n",
    "            hidden\n",
    "        )\n",
    "        # print(output.shape) # [1, 8291]\n",
    "        if i < num:\n",
    "            w = txt[i]\n",
    "            input = Variable(input.data.new([word2ix[w]])).view(1, 1)\n",
    "        else:\n",
    "            top_index = output.data[0].topk(1)[1][0]\n",
    "            # print(top_index)\n",
    "            w = ix2word[top_index.item()]\n",
    "            txt.append(w)\n",
    "            # print(w)\n",
    "            input = Variable(input.data.new([top_index])).view(1, 1)\n",
    "        if w == '<EOP>':\n",
    "            break\n",
    "    return ''.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b0eeb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242.75"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "31072/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66bc102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # 模型定义：vocab_size, embedding_dim, hidden_dim —— 8293 * 128 * 256\n",
    "    modle = Trsfmr_encoder_decoder(len(word2ix), 128, 256)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(modle.parameters(), lr=1e-3)  # 学习率1e-3\n",
    "    loss_meter = meter.AverageValueMeter()\n",
    "\n",
    "    period = []\n",
    "    loss2 = []\n",
    "    for epoch in range(8):     # 最大迭代次数为8\n",
    "        loss_meter.reset()\n",
    "        for i, (data_enc, data_dec) in tqdm.tqdm(enumerate(zip(dataloader_1, dataloader_2))):    # data: torch.Size([128, 125]), dtype=torch.int32            \n",
    "            # data = move_pad_to_tail(data_) # [:, :20]\n",
    "            \n",
    "            data_enc = data_enc.long().transpose(0, 1).contiguous()      # long为默认tensor类型，并转置, [125, 10]\n",
    "            data_dec = data_dec.long().transpose(0, 1).contiguous()\n",
    "            \n",
    "            input_real, target = Variable(data_dec[:-1, :]), Variable(data_dec[1:, :])\n",
    "            output, _ = modle(data_enc[:-1, :], input_real)\n",
    "            \n",
    "            loss = criterion(output, target.view(-1))       # torch.Size([15872, 8293]), torch.Size([15872])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_meter.add(loss.item())    # loss:tensor(3.3510, device='cuda:0', grad_fn=<NllLossBackward>)loss.data:tensor(3.0183, device='cuda:0')\n",
    "\n",
    "            period.append(i + epoch * len(dataloader))\n",
    "            loss2.append(loss_meter.value()[0])\n",
    "            \n",
    "            if (1 + i) % 2 == 0:       # 每575个batch可视化一次\n",
    "                print(str(i) +':' + generate(modle, data_enc, '风花雪月', ix2word, word2ix))\n",
    "\n",
    "        torch.save(modle.state_dict(), 'model_poet_2.pth')\n",
    "        break ## 我们暂时只跑1个epoch\n",
    "    plt.plot(period, loss2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_path = '../originalDataset/tang.npz'\n",
    "    data, word2ix, ix2word = get_data()\n",
    "    data = torch.from_numpy(data)\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=10, shuffle=True, num_workers=1)  # shuffle=True随机打乱\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955e248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
