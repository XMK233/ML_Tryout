{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5182c5f7-564a-4b86-a730-619c040fb4d2",
   "metadata": {},
   "source": [
    "https://github.com/2noise/ChatTTS/blob/main/README_CN.md\n",
    "\n",
    "https://github.com/2noise/ChatTTS/issues/216\n",
    "\n",
    "~~要用chattts环境来跑。~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24366b55-b61e-4a40-a42c-a27002d6f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:250: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:250: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/s1/1jpfx0m52rj4k7cgqkh7g3q40000gn/T/ipykernel_7200/2990379771.py:250: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  *[int(x) for x in re.findall(\"\\d+\", zh_date_str)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /Users/minkexiu/Downloads/GitHub/ML_Tryout/LLM/20240604_ChatTTS\n",
      "code dir: /Users/minkexiu/Documents/GitHub/ML_Tryout/LLM/20240604_ChatTTS \n",
      "\n",
      "02 21 21\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "变爻: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>泽风大过</th>\n",
       "      <th>天天乾</th>\n",
       "      <th>泽水困</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☱兑金</td>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☱兑金</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☴巽木</td>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☵坎水</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   泽风大过  天天乾  泽水困\n",
       "上卦  ☱兑金  ☰乾金  ☱兑金\n",
       "下卦  ☴巽木  ☰乾金  ☵坎水"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 02 12 亥时\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "变爻: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>火泽睽</th>\n",
       "      <th>水火既济</th>\n",
       "      <th>雷泽归妹</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☲离火</td>\n",
       "      <td>☵坎水</td>\n",
       "      <td>☳震木</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☱兑金</td>\n",
       "      <td>☲离火</td>\n",
       "      <td>☱兑金</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    火泽睽 水火既济 雷泽归妹\n",
       "上卦  ☲离火  ☵坎水  ☳震木\n",
       "下卦  ☱兑金  ☲离火  ☱兑金"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('110101', '101010', '110100')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random, os, tqdm, time, json, re, IPython, zhdate, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "# from train_predict_tools_lgbm import *\n",
    "# from train_predict_tools import * \n",
    "# from perf_eval_tools import * \n",
    "# from fea_verification import *\n",
    "# from third_party_data_verify import * \n",
    "\n",
    "random.seed(618)\n",
    "np.random.seed(907)\n",
    "\n",
    "tqdm.tqdm.pandas() ## 引入这个，就可以在apply的时候用progress_apply了。\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "new_base_path = os.path.join(\n",
    "    \"/Users/minkexiu/Downloads/GitHub/\",\n",
    "    \"/\".join(\n",
    "        os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "    ),\n",
    ")\n",
    "print(\"storage dir:\", new_base_path)\n",
    "print(\"code dir:\", os.getcwd(), \"\\n\")\n",
    "\n",
    "## 创建文件夹。\n",
    "if not os.path.exists(new_base_path):\n",
    "    os.makedirs(\n",
    "        new_base_path\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"preprocessedData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"preprocessedData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"originalData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"originalData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"trained_models\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"trained_models\")\n",
    "    )\n",
    "\n",
    "def load_data_from_newbasepath(filename, dirname = new_base_path, foldername = \"originalData\", fmt = \"csv\"):\n",
    "    valid_format = [\"csv\", \"parquet\"]\n",
    "    assert fmt in valid_format, f\"invalid format {fmt}, should be {valid_format}\"\n",
    "    file_path = os.path.join(dirname, foldername, filename + f\".{fmt}\")\n",
    "    if fmt == \"csv\":\n",
    "        cmd = f'pd.read_{fmt}(\"{file_path}\", quoting=3, lineterminator=\"\\\\n\")'\n",
    "    else:\n",
    "        cmd = f'pd.read_{fmt}(\"{file_path}\")'\n",
    "    print(cmd)\n",
    "    return eval(cmd)\n",
    "def store_data_to_newbasepath(df, filename, dirname = new_base_path, foldername = \"preprocessedData\", fmt = \"parquet\", index=False):\n",
    "    valid_format = [\"csv\", \"parquet\"]\n",
    "    assert fmt in valid_format, f\"invalid format {fmt}, should be {valid_format}\"\n",
    "    file_path = os.path.join(dirname, foldername, filename + f\".{fmt}\")\n",
    "    cmd = f'df.to_{fmt}(\"{file_path}\", index={index})'\n",
    "    print(cmd)\n",
    "    eval(cmd)\n",
    "    print(\"data saved.\")\n",
    "    return file_path\n",
    "def load_data_from_newbasepath__waitUntilDownloaded(filename, dirname = new_base_path, foldername = \"originalData\", fmt = \"csv\"):\n",
    "    flag_path = os.path.join(dirname, foldername, filename + \"---downloan_finish_flag.txt\")\n",
    "    print(\"Downloading, please wait a moment...\")\n",
    "    while True:\n",
    "#         print(flag_path)\n",
    "        if os.path.exists(flag_path):\n",
    "            print(\"Downloading finished.\")\n",
    "            break\n",
    "        time.sleep(10)\n",
    "    valid_format = [\"csv\", \"parquet\"]\n",
    "    assert fmt in valid_format, f\"invalid format {fmt}, should be {valid_format}\"\n",
    "    file_path = os.path.join(dirname, foldername, filename + f\".{fmt}\")\n",
    "    if fmt == \"csv\":\n",
    "        cmd = f'pd.read_{fmt}(\"{file_path}\", quoting=3, lineterminator=\"\\\\n\")'\n",
    "    else:\n",
    "        cmd = f'pd.read_{fmt}(\"{file_path}\")'\n",
    "    print(cmd)\n",
    "    return eval(cmd)\n",
    "def load_data_from_originalData(filename, dirname = new_base_path, foldername = \"originalData\", fmt = \"csv\"):\n",
    "    valid_format = [\"csv\", \"parquet\"]\n",
    "    assert fmt in valid_format, f\"invalid format {fmt}, should be {valid_format}\"\n",
    "    file_path = os.path.join(dirname, foldername, filename + f\".{fmt}\")\n",
    "    if fmt == \"csv\":\n",
    "        cmd = f'pd.read_{fmt}(\"{file_path}\", quoting=3, lineterminator=\"\\\\n\")'\n",
    "    else:\n",
    "        cmd = f'pd.read_{fmt}(\"{file_path}\")'\n",
    "    print(cmd)\n",
    "    return eval(cmd)\n",
    "def load_data_from_preprocessedData(filename, dirname = new_base_path, foldername = \"preprocessedData\", fmt = \"parquet\", use_cols = None):\n",
    "    valid_format = [\"csv\", \"parquet\"]\n",
    "    assert fmt in valid_format, f\"invalid format {fmt}, should be {valid_format}\"\n",
    "    file_path = os.path.join(dirname, foldername, filename + f\".{fmt}\")\n",
    "    if fmt == \"csv\":\n",
    "        if use_cols is None:\n",
    "            cmd = f'pd.read_{fmt}(\"{file_path}\", quoting=3, lineterminator=\"\\\\n\")'\n",
    "        else:\n",
    "            cmd = f'pd.read_{fmt}(\"{file_path}\", usecols = {use_cols}, quoting=3, lineterminator=\"\\\\n\")'\n",
    "    else:\n",
    "        if use_cols is None:\n",
    "            cmd = f'pd.read_{fmt}(\"{file_path}\")'\n",
    "        else:\n",
    "            cmd = f'pd.read_{fmt}(\"{file_path}\", columns={use_cols})'\n",
    "    print(cmd)\n",
    "    return eval(cmd)\n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "\n",
    "def run_finish():\n",
    "    # 假设你的字体文件是 'myfont.ttf' 并且位于当前目录下  \n",
    "    font = FontProperties(fname=\"/Users/minkexiu/Documents/GitHub/ML_Tryout/SimHei.ttf\", size=24)  \n",
    "    # 创建一个空白的图形  \n",
    "    fig, ax = plt.subplots()  \n",
    "    ax.imshow(\n",
    "        plt.imread(\"/Users/minkexiu/Downloads/wallhaven-dgxpyg.jpg\")\n",
    "    )\n",
    "    # 在图形中添加文字  \n",
    "    ax.text(\n",
    "        ax.get_xlim()[1] * 0.5, \n",
    "        ax.get_ylim()[0] * 0.5, \n",
    "        f\"程序于这个点跑完：\\n{millisec2datetime(time.time()*1000)}\", fontproperties=font, ha=\"center\", va=\"center\", color=\"red\"\n",
    "    )  \n",
    "    # 设置图形的布局  \n",
    "    # ax.set_xlim(0, 1)  \n",
    "    # ax.set_ylim(0, 1)  \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_yticks([])  \n",
    "    ax.patch.set_color(\"blue\")\n",
    "    # 显示图形  \n",
    "    plt.show()\n",
    "\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "    \n",
    "def simply_show_data(df1):\n",
    "    print(df1.shape)\n",
    "    display(df1.head())\n",
    "    \n",
    "def wait_flag(saved_flag_path, time_interval_sec=10):\n",
    "    print(\"waiting for\", saved_flag_path)\n",
    "    time_count = 0\n",
    "    while True:\n",
    "        if os.path.exists(saved_flag_path):\n",
    "            break\n",
    "        time.sleep(time_interval_sec)\n",
    "        time_count+=time_interval_sec\n",
    "        print(time_count, end=\" \")\n",
    "    print(\"finish!!\")\n",
    "\n",
    "def parallelly_run_multiple_similar_python_code(codes, nb_workers = 4):\n",
    "    '''\n",
    "    codes是多条相似的python代码。\n",
    "    这个函数的作用就是将其平行地跑，每一条python代码就对应一个线程。或许可以后续优化，比如固定线程数为一个特定值。\n",
    "    nb_workers 如果赋值为\n",
    "    '''\n",
    "    assert (isinstance(nb_workers, int)), \"`nb_workers' should be int.\"\n",
    "    df_sqls = pd.DataFrame(\n",
    "        {\n",
    "            \"func\": codes\n",
    "\n",
    "        }\n",
    "    )\n",
    "    display(df_sqls)\n",
    "    from pandarallel import pandarallel\n",
    "    pandarallel.initialize(nb_workers = df_sqls.shape[0] if nb_workers<0 else nb_workers, progress_bar = True)\n",
    "    def run_sql_prlly(row):\n",
    "        try: \n",
    "            cmd = f'{row[\"func\"]}'\n",
    "            print(cmd, \"\\n\")\n",
    "            eval(cmd)\n",
    "            return \"0-success\"\n",
    "        except Exception as e:\n",
    "            return e\n",
    "    df_sqls[\"run_rsts\"] = df_sqls.parallel_apply(lambda row: run_sql_prlly(row), axis = 1)\n",
    "    display(df_sqls)\n",
    "    \n",
    "class TimerContext:  \n",
    "    def __enter__(self):  \n",
    "        self.start_time = str(datetime.now())\n",
    "        print(\"start time:\", self.start_time)\n",
    "        return self  \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  \n",
    "        print(\"start time:\", self.start_time)\n",
    "        print(\"end time\", str(datetime.now()))\n",
    "\n",
    "def three_num_get_gua(a, b, c):\n",
    "    '''梅花易数三数起卦，以取本、互、变。'''\n",
    "    bagua = [\"111\", \"110\", \"101\", \"100\", \"011\", \"010\", \"001\", \"000\"]\n",
    "    guatu = {\n",
    "        \"111\": (\"☰\", \"天\", \"乾金\"), \n",
    "        \"110\": (\"☱\", \"泽\", \"兑金\"),\n",
    "        \"101\": (\"☲\", \"火\", \"离火\"),\n",
    "        \"100\": (\"☳\" , \"雷\", \"震木\"),\n",
    "        \"011\": (\"☴\", \"风\", \"巽木\"),\n",
    "        \"010\": (\"☵\", \"水\", \"坎水\"),\n",
    "        \"001\": (\"☶\", \"山\", \"艮土\"),\n",
    "        \"000\": (\"☷\", \"地\", \"坤土\"),\n",
    "    }\n",
    "    print(\n",
    "        \"先天八卦数:\", \", \".join([f\"{i}{guatu[j][-1][0]}\"for i, j in zip(range(1,9), bagua)])\n",
    "    )\n",
    "    ## https://zhuanlan.zhihu.com/p/457104350\n",
    "    gua_64 = \"天天乾，天风姤，天山遁，天地否，风地观，山地剥，火地晋，火天大有，水水坎，水泽节，水雷屯，水火既济，泽火革，雷火丰，地火明夷，地水师，山山艮，山火贲，山天大畜，山泽损，火泽睽，天泽履，风则中孚，风山渐，雷雷震，雷地豫，雷水解，雷风恒，地风升，水风井，泽风大过，泽雷随，风风巽，风天小畜，风火家人，风雷益，天雷无妄，火雷噬嗑，山雷顾，山风蛊，火火离，火山旅，火风鼎，火水未济，山水蒙，风水涣，天水松，天火同人，地地坤，地雷复，地泽临，地天泰，雷天大壮，泽天夬，水天需，水地比，泽泽兑，泽水困，泽地萃，泽山咸，水山蹇，地山谦，雷山小过，雷泽归妹\"\n",
    "    gua_64_dict = {x[:2]: x[2:]for x in gua_64.split(\"，\")}\n",
    "    \n",
    "    shanggua_idx = 7 if (a % 8 == 0) else (a % 8 - 1)\n",
    "    xiagua_idx = 7 if (b % 8 == 0) else (b % 8 - 1)\n",
    "    bianyao_idx = 5 if (c % 6 == 0) else (c % 6 - 1)\n",
    "    print(\"变爻:\", bianyao_idx+1)\n",
    "    bengua = bagua[xiagua_idx] + bagua[shanggua_idx]\n",
    "    hugua = bengua[1:-1][:3] + bengua[1:-1][1:]\n",
    "    biangua = list(bengua)\n",
    "    biangua[bianyao_idx] = str(1 - int(biangua[bianyao_idx]))\n",
    "    biangua = \"\".join(biangua)\n",
    "    df = pd.DataFrame([[\n",
    "        guatu[bengua[3:]][0]+guatu[bengua[3:]][2], guatu[hugua[3:]][0]+guatu[hugua[3:]][2], guatu[biangua[3:]][0]+guatu[biangua[3:]][2], \n",
    "    ],[\n",
    "        guatu[bengua[:3]][0]+guatu[bengua[:3]][2], guatu[hugua[:3]][0]+guatu[hugua[:3]][2], guatu[biangua[:3]][0]+guatu[biangua[:3]][2], \n",
    "    ]], index=[\"上卦\", \"下卦\"], columns = [\n",
    "        guatu[bengua[3:]][1] + guatu[bengua[:3]][1] + gua_64_dict[guatu[bengua[3:]][1] + guatu[bengua[:3]][1]],\n",
    "        guatu[hugua[3:]][1] + guatu[hugua[:3]][1] + gua_64_dict[guatu[hugua[3:]][1] + guatu[hugua[:3]][1]],\n",
    "        guatu[biangua[3:]][1] + guatu[biangua[:3]][1] + gua_64_dict[guatu[biangua[3:]][1] + guatu[biangua[:3]][1]],\n",
    "    ])\n",
    "    display(df)\n",
    "    return bengua, hugua, biangua\n",
    "    \n",
    "def easy_start_gua():\n",
    "    \"\"\"用公历的日、时、分来起卦。\"\"\"\n",
    "    n1, n2, n3 = str(datetime.now())[8:10], str(datetime.now())[11:13], str(datetime.now())[14:16]\n",
    "    print(n1, n2, n3)\n",
    "    return three_num_get_gua(int(n1), int(n2), int(n3))\n",
    "easy_start_gua()\n",
    "\n",
    "def easy_start_gua_lunar():\n",
    "    '''用农历的月、日、时辰来起卦。'''\n",
    "    time_now = datetime.now()\n",
    "    zh_date_str = str(zhdate.ZhDate.from_datetime(time_now))\n",
    "    zh_date_str_1 = datetime.strftime(\n",
    "        datetime(\n",
    "            *[int(x) for x in re.findall(\"\\d+\", zh_date_str)]\n",
    "        ),\n",
    "        '%Y-%m-%d'\n",
    "    )\n",
    "    zh_hour = (time_now.hour + 1)//2%12+1\n",
    "    zh_hour_dizhi = \"子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥\".split(\"、\")[zh_hour-1]\n",
    "    \n",
    "    n1, n2, n3 = zh_date_str_1[5:7], zh_date_str_1[8:10], zh_hour\n",
    "    print(n1, n2, n3, f\"{zh_hour_dizhi}时\")\n",
    "    return three_num_get_gua(int(n1), int(n2), int(n3))\n",
    "easy_start_gua_lunar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90d3509-81ea-409a-8aea-29b657dee141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 如果没有下载过模型，就在这里下载。\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('pzc163/chatTTS')\n",
    "# ## 下载的位置在 /Users/minkexiu/.cache/modelscope/hub/pzc163/chatTTS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dda8a4-9565-4787-9146-5ef32ba1dc67",
   "metadata": {},
   "source": [
    "# 第一种跑法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9fced3-b34a-43eb-859a-4916acef9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ChatTTS\n",
    "import torch\n",
    "import torchaudio\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9688c4f-104e-45dd-8a9c-ddf66017fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from epub2txt import epub2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c82351-a3ef-47f4-ab2b-5d29fe1c4e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatTTS.Chat()\n",
    "chat.load(\n",
    "    compile=True, \n",
    "    source=\"custom\", \n",
    "    custom_path=create_trained_models_path(\"chatTTS\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e223a1-cac4-48ea-a37f-06223657aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43ebd08-b83b-4546-8afb-11de9f2e5e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/ebooklib/epub.py:1410: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "# from a url to epub\n",
    "# url = \"https://github.com/ffreemt/tmx2epub/raw/master/tests/1.tmx.epub\"\n",
    "# res = epub2txt(url)\n",
    "\n",
    "# from a local epub file\n",
    "filepath = r'/Users/minkexiu/Desktop/长城之外-北境与大明边防（1368—1644） (【美】窦德士) (Z-Library).epub'\n",
    "res = epub2txt(filepath)\n",
    "\n",
    "# output as a list of chapters\n",
    "ch_list = epub2txt(filepath, outputlist=True)\n",
    "# chapter titles will be available as epub2txt.content_titles if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420698f5-b50a-4803-b447-d903666244db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ori_str = \"。\".join(ch_list).replace(\n",
    "    \"\\n\", \" \"\n",
    ").replace(\n",
    "    \"\\xa0\", \" \"\n",
    ").replace(\n",
    "    \"1\", \"一\"\n",
    ").replace(\n",
    "    \"2\", \"二\"\n",
    ").replace(\n",
    "    \"3\", \"三\"\n",
    ").replace(\n",
    "    \"4\", \"四\"\n",
    ").replace(\n",
    "    \"5\", \"五\"\n",
    ").replace(\n",
    "    \"6\", \"六\"\n",
    ").replace(\n",
    "    \"7\", \"七\"\n",
    ").replace(\n",
    "    \"8\", \"八\"\n",
    ").replace(\n",
    "    \"9\", \"九\"\n",
    ").replace(\n",
    "    \"0\", \"零\"\n",
    ").replace(\n",
    "    \". \", \"。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e30fb1d-1447-453f-8e48-c73b752963fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks_by_sentences(text, chunk_size=150):\n",
    "    \"\"\"\n",
    "    将字符串按照句号拆分，并进一步组合成长度大致为chunk_size的若干子串。\n",
    "\n",
    "    参数:\n",
    "    text (str): 要处理的字符串。\n",
    "    chunk_size (int): 每个子串的大致长度，默认为150。\n",
    "\n",
    "    返回:\n",
    "    list: 包含长度大致为chunk_size的子串的列表。\n",
    "    \"\"\"\n",
    "    # 使用句号拆分字符串为句子列表（考虑句号后可能有空格或换行符）\n",
    "    sentences = [sentence.strip() for sentence in text.split('。') if sentence.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 计算如果添加当前句子后子串的长度\n",
    "        new_chunk_length = current_chunk_length + len(sentence) + 1  # +1 是为了考虑添加句号作为分隔符\n",
    "        \n",
    "        # 如果添加后长度超过chunk_size，则保存当前子串并开始新的子串\n",
    "        if new_chunk_length > chunk_size:\n",
    "            chunks.append('. '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_length = len(sentence)\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_length = new_chunk_length - 1  # 不再需要+1，因为我们在最后会添加句号作为分隔符（但在实际拼接时不加在最后一个句子后）\n",
    "    \n",
    "    # 添加最后一个子串（如果有的话）\n",
    "    if current_chunk:\n",
    "        chunks.append('. '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eda54b2-5872-4265-a554-fac3561952dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "strs = split_into_chunks_by_sentences(ori_str, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9bcb168-ae15-4f92-8fea-1c9908b35521",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end = (10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f7712b8-5c32-41f3-9a5f-d1683734dc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在明朝[uv_break]“鞑靼”多用于指称由北元（逃往漠北的元朝统治集团的后裔）政权统治的蒙古高原东部诸部落[uv_break]他们自称蒙古[uv_break]而明人称之为鞑靼',\n",
       " '而下文所见的“瓦剌”多活动在蒙古高原西部[uv_break]元称“斡亦剌惕”[uv_break]明称“瓦剌”[uv_break]清以后称“卫拉特”[uv_break]成为广义的蒙古族的一部分',\n",
       " '——译者注[uv_break]从长时段看[uv_break]层出不穷的边防安全事件可分为三个典型时期',\n",
       " '第一个典型时期在洪武元年（一三六八年）到宣德十年（一四三五年）[uv_break]这一时期[uv_break]明朝皇帝掌控着全局',\n",
       " '太祖[uv_break]永乐皇帝和继任的宣德皇帝以其恩威之势主导着东半球大部的政治格局',\n",
       " '太祖和永乐皇帝还曾先后一一次大规模远征漠北[uv_break]这使得鞑靼人（即当时的蒙古人）[uv_break]几乎不怀疑中原政权也具备远程作战的能力',\n",
       " '第二个典型时期在正统十四年（一四四九年）后.[uv_break]是年[uv_break]瓦剌领袖也先在土木堡俘虏了年轻的英宗皇帝[uv_break]这一事件成为一大转折点',\n",
       " '此后直到隆庆五年（一五七一年）[uv_break]明朝逐渐由攻转守[uv_break]在防线上构筑大量军事防御设施',\n",
       " '这一时期[uv_break]皇帝难以恩泽天下[uv_break]明朝逐渐转向内敛并采取闭关锁国政策[uv_break]“华夷之辨”的保守思想抬头[uv_break]渐次取代此前“天下一家”的开放包容氛围',\n",
       " '这一趋势在嘉靖皇帝在位时（一五二一—一五六七年）达到峰值[uv_break]中原因而变成一个固若金汤的“大堡垒”']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_see = [x.replace(\n",
    "    \"，\", \"[uv_break]\"\n",
    ").replace(\n",
    "    \"。\", \"[lbreak]\"\n",
    ").replace(\n",
    "    \"、\", \"[uv_break]\"\n",
    ").replace(\n",
    "    \" \", \"[uv_break]\"\n",
    ") for x in strs[start_end[0]:start_end[1]]]\n",
    "to_see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85478c12-2bdf-4a99-80f8-d67e51ae0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/2noise/ChatTTS/issues/705\n",
    "\n",
    "# 默认种子\n",
    "DEFAULT_AUDIO_SEED_INPUT = 47\n",
    "DEFAULT_TEXT_SEED_INPUT = 24\n",
    "DEFAULT_SEED = 425\n",
    "\n",
    "def generate_audio_file(chat, texts=None, output_path=\"word_level_output.wav\",\n",
    "                        audio_seed=DEFAULT_AUDIO_SEED_INPUT,\n",
    "                        text_seed=DEFAULT_TEXT_SEED_INPUT,\n",
    "                        seed=DEFAULT_SEED):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    temperature = 0.3\n",
    "    top_P = 0.7\n",
    "    top_K = 20\n",
    "    refine_text_flag = False\n",
    "\n",
    "    torch.manual_seed(audio_seed)\n",
    "    rand_spk = chat.sample_random_speaker()\n",
    "    params_infer_code = ChatTTS.Chat.InferCodeParams(\n",
    "        spk_emb=rand_spk,\n",
    "        temperature=temperature,\n",
    "        top_P=top_P,\n",
    "        top_K=top_K)\n",
    "    # params_refine_text = ChatTTS.Chat.RefineTextParams(\n",
    "    #     prompt='',) ## [oral_2][laugh_0][break_6]\n",
    "\n",
    "    torch.manual_seed(text_seed)\n",
    "    if refine_text_flag:\n",
    "        text = chat.infer(texts,\n",
    "                          # skip_refine_text=False,\n",
    "                          # refine_text_only=True,\n",
    "                          # params_refine_text=params_refine_text,\n",
    "                          params_infer_code=params_infer_code\n",
    "                          )\n",
    "    else:\n",
    "        text = texts\n",
    "\n",
    "    wavs = chat.infer(text,\n",
    "                      # skip_refine_text=True,\n",
    "                      # params_refine_text=params_refine_text,\n",
    "                      params_infer_code=params_infer_code,)\n",
    "    \n",
    "    return wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31f3a0df-ee92-4b79-a140-ec50950723eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "found invalid characters: {'）', '”', '（', '“'}\n",
      "found invalid characters: {'”', '“'}\n",
      "found invalid characters: {'—'}\n",
      "found invalid characters: {'）', '（'}\n",
      "found invalid characters: {'）', '（'}\n",
      "found invalid characters: {'）', '（'}\n",
      "found invalid characters: {'）', '（'}\n",
      "found invalid characters: {'”', '“'}\n",
      "found invalid characters: {'—', '“', '）', '”', '（'}\n",
      "text:  19%|███████                              | 73/384(max) [00:07,  9.59it/s]\n",
      "code:  28%|█████████▉                         | 582/2048(max) [01:41,  5.73it/s]\n"
     ]
    }
   ],
   "source": [
    "wavs = generate_audio_file(\n",
    "    chat, \n",
    "    to_see, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c8296ac-e989-4e50-b8ed-286a98acb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.wavfile.write(\n",
    "    filename=create_preprocessedData_path(f\"x.wav\"), \n",
    "    rate=24_000, \n",
    "    data=np.concatenate([np.concatenate([wavs[i].T, np.zeros(10000)]) for i in range(len(to_see))]) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002a252-6e7a-4f1a-840e-1bdb3eab813a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71a1e7-6fa8-46e3-941d-855875cc37df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a86ec6-0179-4c81-85f6-3c423078c0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22c08f-bcf8-4edc-a757-d57f287c20a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba3ccf-9507-42db-b6fb-c86d31a8628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    " \n",
    "def shorten_flat_regions(input_file, threshold, output_file):\n",
    "    # 打开波形文件\n",
    "    with wave.open(input_file, 'rb') as wav_read:\n",
    "        params = wav_read.getparams()\n",
    "        nchannels, sampwidth, framerate, nframes = params[:4]\n",
    "        \n",
    "        # 读取波形数据\n",
    "        str_data = wav_read.readframes(nframes)\n",
    "        wave_data = np.frombuffer(str_data, dtype=np.int16)\n",
    "        \n",
    "        # 将数据转换为一维数组\n",
    "        wave_data = wave_data * (1.0 / max(abs(wave_data)))\n",
    "        \n",
    "        # 找到连续的零或者在阈值以下的点\n",
    "        crossings = np.where(np.diff(np.abs(wave_data) < threshold))[0] + 1\n",
    "        \n",
    "        # 计算每个平波区域的长度\n",
    "        flat_regions = np.diff(crossings)\n",
    "        \n",
    "        # 缩短平波区域，这里以去掉25%的长度为例\n",
    "        new_flat_regions = [int(len * 0.75) for len in flat_regions]\n",
    "        \n",
    "        # 构造新的波形数据\n",
    "        new_wave_data = []\n",
    "        start = 0\n",
    "        for length in new_flat_regions:\n",
    "            new_wave_data.extend(wave_data[start:start + length])\n",
    "            start += length\n",
    "        \n",
    "        # 将新数据写入新的波形文件\n",
    "        with wave.open(output_file, 'wb') as wav_write:\n",
    "            wav_write.setnchannels(nchannels)\n",
    "            wav_write.setsampwidth(sampwidth)\n",
    "            wav_write.setframerate(framerate)\n",
    "            wav_write.writeframes(np.array(new_wave_data).tobytes())\n",
    " \n",
    "# 使用函数处理波形文件\n",
    "# 假设阈值设为0.1，输入输出文件分别为'input.wav'和'output.wav'\n",
    "shorten_flat_regions(create_preprocessedData_path(f\"x.wav\"), 0.1, 'output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c1ec5-9490-422a-b331-fe65e6f072c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac321491-c84a-4e3f-85a8-aa86f33ea7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cae00-093c-4e0d-8fcf-11f0e4a4753f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1ab76-2333-43ba-a01d-ed2274788dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4608a228-c3b6-4b7b-81f5-fc0df63dfd34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 下面的代码的接口过时了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af3de19-7ebb-496b-b55a-75394b0b7424",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Chat.load() got an unexpected keyword argument 'vocos_config_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m create_trained_models_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpzc163/chatTTS\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# '/Users/minkexiu/.cache/modelscope/hub/pzc163/chatTTS' ## 下载下来的模型的路径。\u001b[39;00m\n\u001b[1;32m      5\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatTTS\u001b[38;5;241m.\u001b[39mChat()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocos_config_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/config/vocos.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocos_ckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/asset/Vocos.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdvae_config_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/config/dvae.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdvae_ckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/asset/DVAE.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt_config_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/config/gpt.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt_ckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/asset/GPT.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_config_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/config/decoder.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_ckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/asset/Decoder.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/asset/tokenizer.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Chat.load() got an unexpected keyword argument 'vocos_config_path'"
     ]
    }
   ],
   "source": [
    "## 如果pip装chattts库的话，可以用modelscope先把模型下载下来，然后再跑下面代码：\n",
    "import ChatTTS\n",
    "import scipy\n",
    "model_path = create_trained_models_path(\"pzc163/chatTTS\") # '/Users/minkexiu/.cache/modelscope/hub/pzc163/chatTTS' ## 下载下来的模型的路径。\n",
    "chat = ChatTTS.Chat()\n",
    "chat.load(\n",
    "    vocos_config_path=f\"{model_path}/config/vocos.yaml\",\n",
    "    vocos_ckpt_path=f\"{model_path}/asset/Vocos.pt\",\n",
    "    dvae_config_path=f\"{model_path}/config/dvae.yaml\",\n",
    "    dvae_ckpt_path=f\"{model_path}/asset/DVAE.pt\",\n",
    "    gpt_config_path=f\"{model_path}/config/gpt.yaml\",\n",
    "    gpt_ckpt_path=f\"{model_path}/asset/GPT.pt\",\n",
    "    decoder_config_path=f\"{model_path}/config/decoder.yaml\", \n",
    "    decoder_ckpt_path=f\"{model_path}/asset/Decoder.pt\", \n",
    "    tokenizer_path=f\"{model_path}/asset/tokenizer.pt\", \n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "997ffa14-b2ae-40a6-b5c2-efa65ebc9178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:All initialized.\n",
      " 13%|█████▌                                    | 51/384 [00:02<00:16, 20.69it/s]\n",
      " 23%|█████████▎                              | 479/2048 [00:17<00:58, 26.82it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"卧槽这个玩意儿有点东西啊[uv_break]，不过暂时也开发不出更多玩法了，仅此而已罢了。能给我来点说唱吗[laugh]\", ]\n",
    "wavs = chat.infer(texts, use_decoder=True)\n",
    "scipy.io.wavfile.write(filename=\"./tts3.wav\", rate=24_000, data=wavs[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba85556c-df0b-4cf4-97e3-5931f1cad71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:All initialized.\n",
      "  5%|██▎                                       | 21/384 [00:01<00:19, 18.44it/s]\n",
      "  9%|███▊                                    | 192/2048 [00:06<01:02, 29.55it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"呦呦切克闹，爱你的猫抛瓦\", ]\n",
    "wavs = chat.infer(texts, use_decoder=True)\n",
    "scipy.io.wavfile.write(filename=\"./inmp.wav\", rate=24_000, data=wavs[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd7fe56-5d02-463c-b834-d5d185439f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:All initialized.\n",
      "  1%|▍                                          | 4/384 [00:00<00:49,  7.66it/s]\n",
      "  9%|███▋                                    | 191/2048 [00:06<01:05, 28.43it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"[laugh]\", ]\n",
    "wavs = chat.infer(texts, use_decoder=True)\n",
    "scipy.io.wavfile.write(filename=\"./laugh.wav\", rate=24_000, data=wavs[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c463c-2d20-4e5d-b689-51c96fe75f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc9c9c1-faf0-44b4-a5bb-0a00c72bf92a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 第二种跑法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ca8876-9cae-4407-a76a-8122ac02a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:Load from local: /Users/minkexiu/.cache/modelscope/hub/pzc163/chatTTS/\n",
      "WARNING:ChatTTS.utils.gpu_utils:No GPU found, use CPU instead\n",
      "INFO:ChatTTS.core:use cpu\n",
      "INFO:ChatTTS.core:vocos loaded.\n",
      "INFO:ChatTTS.core:dvae loaded.\n",
      "INFO:ChatTTS.core:gpt loaded.\n",
      "INFO:ChatTTS.core:decoder loaded.\n",
      "INFO:ChatTTS.core:tokenizer loaded.\n",
      "INFO:ChatTTS.core:All initialized.\n",
      "INFO:ChatTTS.core:All initialized.\n",
      "WARNING:ChatTTS.core:Package nemo_text_processing not found!                         Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'Normalizer' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSo we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Perform inference and play the generated audio\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m wavs \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m Audio(wavs[\u001b[38;5;241m0\u001b[39m], rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24_000\u001b[39m, autoplay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Save the generated audio \u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ML_Tryout/LLM/20240604_ChatTTS/ChatTTS/core.py:146\u001b[0m, in \u001b[0;36mChat.infer\u001b[0;34m(self, text, skip_refine_text, refine_text_only, params_refine_text, params_infer_code, use_decoder, do_text_normalization, lang)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text):\n\u001b[1;32m    145\u001b[0m     _lang \u001b[38;5;241m=\u001b[39m detect_language(t) \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lang\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_normalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     text[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer[_lang](t)\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/GitHub/ML_Tryout/LLM/20240604_ChatTTS/ChatTTS/core.py:199\u001b[0m, in \u001b[0;36mChat.init_normalizer\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(logging\u001b[38;5;241m.\u001b[39mWARNING, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPackage nemo_text_processing not found! \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124m        Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer[lang] \u001b[38;5;241m=\u001b[39m partial(\u001b[43mNormalizer\u001b[49m(input_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcased\u001b[39m\u001b[38;5;124m'\u001b[39m, lang\u001b[38;5;241m=\u001b[39mlang)\u001b[38;5;241m.\u001b[39mnormalize, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, punct_post_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'Normalizer' referenced before assignment"
     ]
    }
   ],
   "source": [
    "## 把ChatTTS库下载到本地解压，然后在ChatTTS文件夹外设置一个代码，删掉pip装的ChatTTS库，\n",
    "## 跑下一个代码，可能也行。\n",
    "\n",
    "# Import necessary libraries and configure settings\n",
    "import torch\n",
    "import torchaudio\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import ChatTTS\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Initialize and load the model: \n",
    "chat = ChatTTS.Chat()\n",
    "chat.load_models(\n",
    "    # source='local', \n",
    "    # local_path=\"/Users/minkexiu/.cache/modelscope/hub/pzc163/chatTTS\"\n",
    ") # Set to True for better performance\n",
    "\n",
    "# Define the text input for inference (Support Batching)\n",
    "texts = [\n",
    "\"So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.\",\n",
    "]\n",
    "\n",
    "# Perform inference and play the generated audio\n",
    "wavs = chat.infer(texts)\n",
    "Audio(wavs[0], rate=24_000, autoplay=True)\n",
    "\n",
    "# Save the generated audio \n",
    "torchaudio.save(\"output.wav\", torch.from_numpy(wavs[0]), 24000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e615e8-875b-4964-864a-373689c8c24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a1b57-0dae-47d8-938f-4b90f415fab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c6842-a792-4432-b27b-da9296232f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
