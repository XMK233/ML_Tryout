{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5182c5f7-564a-4b86-a730-619c040fb4d2",
   "metadata": {},
   "source": [
    "https://github.com/2noise/ChatTTS/blob/main/README_CN.md\n",
    "\n",
    "https://github.com/2noise/ChatTTS/issues/216\n",
    "\n",
    "要用chattts环境来跑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dda8a4-9565-4787-9146-5ef32ba1dc67",
   "metadata": {},
   "source": [
    "# 第一种跑法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3de19-7ebb-496b-b55a-75394b0b7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 如果pip装chattts库的话，可以用modelscope先把模型下载下来，然后再跑下面代码：\n",
    "import ChatTTS\n",
    "import scipy\n",
    "model_path = '/Users/minkexiu/.cache/modelscope/hub/pzc163/chatTTS' ## 下载下来的模型的路径。\n",
    "chat = ChatTTS.Chat()\n",
    "chat.load_models(\n",
    "    vocos_config_path=f\"{model_path}/config/vocos.yaml\",\n",
    "    vocos_ckpt_path=f\"{model_path}/asset/Vocos.pt\",\n",
    "    dvae_config_path=f\"{model_path}/config/dvae.yaml\",\n",
    "    dvae_ckpt_path=f\"{model_path}/asset/DVAE.pt\",\n",
    "    gpt_config_path=f\"{model_path}/config/gpt.yaml\",\n",
    "    gpt_ckpt_path=f\"{model_path}/asset/GPT.pt\",\n",
    "    decoder_config_path=f\"{model_path}/config/decoder.yaml\", \n",
    "    decoder_ckpt_path=f\"{model_path}/asset/Decoder.pt\", \n",
    "    tokenizer_path=f\"{model_path}/asset/tokenizer.pt\", \n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "997ffa14-b2ae-40a6-b5c2-efa65ebc9178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:All initialized.\n",
      " 13%|█████▌                                    | 51/384 [00:02<00:16, 20.69it/s]\n",
      " 23%|█████████▎                              | 479/2048 [00:17<00:58, 26.82it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"卧槽这个玩意儿有点东西啊[uv_break]，不过暂时也开发不出更多玩法了，仅此而已罢了。能给我来点说唱吗[laugh]\", ]\n",
    "wavs = chat.infer(texts, use_decoder=True)\n",
    "scipy.io.wavfile.write(filename=\"./tts3.wav\", rate=24_000, data=wavs[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba85556c-df0b-4cf4-97e3-5931f1cad71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:All initialized.\n",
      "  5%|██▎                                       | 21/384 [00:01<00:19, 18.44it/s]\n",
      "  9%|███▊                                    | 192/2048 [00:06<01:02, 29.55it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"呦呦切克闹，爱你的猫抛瓦\", ]\n",
    "wavs = chat.infer(texts, use_decoder=True)\n",
    "scipy.io.wavfile.write(filename=\"./inmp.wav\", rate=24_000, data=wavs[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd7fe56-5d02-463c-b834-d5d185439f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:All initialized.\n",
      "  1%|▍                                          | 4/384 [00:00<00:49,  7.66it/s]\n",
      "  9%|███▋                                    | 191/2048 [00:06<01:05, 28.43it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"[laugh]\", ]\n",
    "wavs = chat.infer(texts, use_decoder=True)\n",
    "scipy.io.wavfile.write(filename=\"./laugh.wav\", rate=24_000, data=wavs[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352d9d5-df7f-47d1-aa17-c19a7dc5805a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc66c0e-d9bd-4e2f-8b7f-266ef2de5cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fb4bb-d258-448a-9bd0-070b44c4b6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc9c9c1-faf0-44b4-a5bb-0a00c72bf92a",
   "metadata": {},
   "source": [
    "# 第二种跑法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ca8876-9cae-4407-a76a-8122ac02a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:Load from local: /Users/minkexiu/.cache/modelscope/hub/pzc163/chatTTS/\n",
      "WARNING:ChatTTS.utils.gpu_utils:No GPU found, use CPU instead\n",
      "INFO:ChatTTS.core:use cpu\n",
      "INFO:ChatTTS.core:vocos loaded.\n",
      "INFO:ChatTTS.core:dvae loaded.\n",
      "INFO:ChatTTS.core:gpt loaded.\n",
      "INFO:ChatTTS.core:decoder loaded.\n",
      "INFO:ChatTTS.core:tokenizer loaded.\n",
      "INFO:ChatTTS.core:All initialized.\n",
      "INFO:ChatTTS.core:All initialized.\n",
      "WARNING:ChatTTS.core:Package nemo_text_processing not found!                         Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'Normalizer' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSo we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Perform inference and play the generated audio\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m wavs \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m Audio(wavs[\u001b[38;5;241m0\u001b[39m], rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24_000\u001b[39m, autoplay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Save the generated audio \u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ML_Tryout/LLM/20240604_ChatTTS/ChatTTS/core.py:146\u001b[0m, in \u001b[0;36mChat.infer\u001b[0;34m(self, text, skip_refine_text, refine_text_only, params_refine_text, params_infer_code, use_decoder, do_text_normalization, lang)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text):\n\u001b[1;32m    145\u001b[0m     _lang \u001b[38;5;241m=\u001b[39m detect_language(t) \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lang\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_normalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     text[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer[_lang](t)\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/GitHub/ML_Tryout/LLM/20240604_ChatTTS/ChatTTS/core.py:199\u001b[0m, in \u001b[0;36mChat.init_normalizer\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(logging\u001b[38;5;241m.\u001b[39mWARNING, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPackage nemo_text_processing not found! \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124m        Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer[lang] \u001b[38;5;241m=\u001b[39m partial(\u001b[43mNormalizer\u001b[49m(input_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcased\u001b[39m\u001b[38;5;124m'\u001b[39m, lang\u001b[38;5;241m=\u001b[39mlang)\u001b[38;5;241m.\u001b[39mnormalize, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, punct_post_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'Normalizer' referenced before assignment"
     ]
    }
   ],
   "source": [
    "## 把ChatTTS库下载到本地解压，然后在ChatTTS文件夹外设置一个代码，删掉pip装的ChatTTS库，\n",
    "## 跑下一个代码，可能也行。\n",
    "\n",
    "# Import necessary libraries and configure settings\n",
    "import torch\n",
    "import torchaudio\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import ChatTTS\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Initialize and load the model: \n",
    "chat = ChatTTS.Chat()\n",
    "chat.load_models(\n",
    "    # source='local', \n",
    "    # local_path=\"/Users/minkexiu/.cache/modelscope/hub/pzc163/chatTTS\"\n",
    ") # Set to True for better performance\n",
    "\n",
    "# Define the text input for inference (Support Batching)\n",
    "texts = [\n",
    "\"So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.\",\n",
    "]\n",
    "\n",
    "# Perform inference and play the generated audio\n",
    "wavs = chat.infer(texts)\n",
    "Audio(wavs[0], rate=24_000, autoplay=True)\n",
    "\n",
    "# Save the generated audio \n",
    "torchaudio.save(\"output.wav\", torch.from_numpy(wavs[0]), 24000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e615e8-875b-4964-864a-373689c8c24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a1b57-0dae-47d8-938f-4b90f415fab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c6842-a792-4432-b27b-da9296232f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
