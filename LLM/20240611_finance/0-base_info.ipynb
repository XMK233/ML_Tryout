{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423f52d4-bab0-4094-bfa1-f531a652d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /Users/minkexiu/Downloads/GitHub/ML_Tryout/LLM/20240611_finance\n",
      "code dir: /Users/minkexiu/Documents/GitHub/ML_Tryout/LLM/20240611_finance\n"
     ]
    }
   ],
   "source": [
    "import random, os, tqdm, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../../\")\n",
    "\n",
    "random.seed(618)\n",
    "np.random.seed(907)\n",
    "\n",
    "new_base_path = os.path.join(\n",
    "    \"/Users/minkexiu/Downloads/\",\n",
    "    \"/\".join(\n",
    "        os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "    ),\n",
    ")\n",
    "print(\"storage dir:\", new_base_path)\n",
    "print(\"code dir:\", os.getcwd())\n",
    "\n",
    "## 创建文件夹。\n",
    "if not os.path.exists(new_base_path):\n",
    "    os.makedirs(\n",
    "        new_base_path\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"preprocessedData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"preprocessedData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"originalData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"originalData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"trained_models\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"trained_models\")\n",
    "    )\n",
    "\n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "\n",
    "def millisec2datetime(timestamp):\n",
    "    time_local = time.localtime(timestamp/1000)\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time_local)\n",
    "    \n",
    "def run_finish():\n",
    "    # 假设你的字体文件是 'myfont.ttf' 并且位于当前目录下  \n",
    "    font = FontProperties(fname=\"/Users/minkexiu/Documents/GitHub/ML_Tryout/SimHei.ttf\", size=24)  \n",
    "    # 创建一个空白的图形  \n",
    "    fig, ax = plt.subplots()  \n",
    "    ax.imshow(\n",
    "        plt.imread(\"/Users/minkexiu/Downloads/wallhaven-dgxpyg.jpg\")\n",
    "    )\n",
    "    # 在图形中添加文字  \n",
    "    ax.text(\n",
    "        ax.get_xlim()[1] * 0.5, \n",
    "        ax.get_ylim()[0] * 0.5, \n",
    "        f\"程序于这个点跑完：\\n{millisec2datetime(time.time()*1000)}\", fontproperties=font, ha=\"center\", va=\"center\", color=\"red\"\n",
    "    )  \n",
    "    # 设置图形的布局  \n",
    "    # ax.set_xlim(0, 1)  \n",
    "    # ax.set_ylim(0, 1)  \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_yticks([])  \n",
    "    ax.patch.set_color(\"blue\")\n",
    "    # 显示图形  \n",
    "    plt.show()\n",
    "        \n",
    "tqdm.tqdm.pandas() ## 引入这个，就可以在apply的时候用progress_apply了。\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "    \n",
    "def simply_show_data(df1):\n",
    "    print(df1.shape)\n",
    "    display(df1.head())\n",
    "    \n",
    "def wait_flag(saved_flag_path, time_interval_sec=10):\n",
    "    print(\"waiting for\", saved_flag_path)\n",
    "    time_count = 0\n",
    "    while True:\n",
    "        if os.path.exists(saved_flag_path):\n",
    "            break\n",
    "        time.sleep(time_interval_sec)\n",
    "        time_count+=time_interval_sec\n",
    "        print(time_count, end=\" \")\n",
    "    print(\"finish!!\")\n",
    "\n",
    "class TimerContext:  \n",
    "    def __enter__(self):  \n",
    "        self.start_time = str(datetime.now())\n",
    "        print(\"start time:\", self.start_time)\n",
    "        return self  \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  \n",
    "        print(\"start time:\", self.start_time)\n",
    "        print(\"end time\", str(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03f10c-178a-49cf-8993-5d0f32ca27e1",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/competition/entrance/532198/information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11716ca5-87d5-4575-9176-71293758e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from add_prompt import PromptCreate\n",
    "from llm_api.my_model import MyModelAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17a1fe9-c2a9-464e-9d4d-d79b5719f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"xx_A_result_14B.jsonl\"\n",
    "data_path = create_originalData_path(\"初赛/初赛/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "322a3c3f-dbc8-432f-bf00-004c94dd513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "model = MyModelAPI(logger)\n",
    "pc = PromptCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1936cdda-dcf2-4eda-bf54-6216bce11c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model_api, in_file, out_file):\n",
    "    questions = []\n",
    "    with open(in_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            questions.append(json.loads(line))\n",
    "    fd = open(\"error.txt\", \"w\", encoding=\"utf-8\")\n",
    "    with open(out_file, \"a+\", encoding=\"utf-8\") as fw:\n",
    "        for question in tqdm(questions):\n",
    "            # 添加prompt模板\n",
    "            try:\n",
    "                prompt = pc.create(key=question[\"task\"], **question)\n",
    "                # print(prompt)\n",
    "                content = model_api.chat_generate(prompt)\n",
    "                question[\"answer\"] = content\n",
    "                logger.info(question)\n",
    "                fw.writelines(json.dumps(question, ensure_ascii=False))\n",
    "                fw.writelines(\"\\n\")\n",
    "                fw.flush()\n",
    "                # break\n",
    "            except:\n",
    "                fd.write(\"{}, {}\".format(in_file, question[\"task\"]))\n",
    "                continue\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e030c97-7f62-4a59-98ef-757b8339b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "start = time.time()\n",
    "for src, dirs, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            in_path = os.path.join(src, file)\n",
    "            run(model, in_path, output_file)\n",
    "print(f\"总计耗时：{(time.time()-start)/3600} h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75a7c979-cd16-4367-8cc1-8dddea6f026e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heheda 123123 12312313'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''heheda {a} {b}'''.format(\n",
    "    **{\n",
    "        \"a\": \"123123\",\n",
    "        \"b\": \"12312313\",\n",
    "        \"c\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8f56a-7ab5-4920-9125-7ffcca1f065b",
   "metadata": {},
   "source": [
    "https://www.bilibili.com/video/BV1mT421v7dE/?spm_id_from=333.337.search-card.all.click&vd_source=f4fd2b4a0ac31736d77c1fc161a66c7d\n",
    "\n",
    "https://huggingface.co/THUDM/glm-4-9b/tree/main\n",
    "\n",
    "https://modelscope.cn/models/ZhipuAI/glm-4-9b/files\n",
    "\n",
    "https://blog.stoeng.site/20240608.html 这个很好玩，可以把chattts和glm合并起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7612c1e2-fe91-4adb-8d93-04f87023f99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:40:58,308 - modelscope - INFO - PyTorch version 2.2.2 Found.\n",
      "2024-06-12 00:40:58,309 - modelscope - INFO - Loading ast index from /Users/minkexiu/.cache/modelscope/ast_indexer\n",
      "2024-06-12 00:40:58,368 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 17686ef0d47f348c51ddcc1bbcd38b7c and a total number of 980 components indexed\n"
     ]
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('ZhipuAI/glm-4-9b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9dc84b-d1aa-4e32-96ab-4a4fc69e63e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████| 1.32k/1.32k [00:00<00:00, 6.16kB/s]\n",
      "Downloading: 100%|█████████████████████████| 2.21k/2.21k [00:00<00:00, 9.20kB/s]\n",
      "Downloading: 100%|█████████████████████████| 6.34k/6.34k [00:00<00:00, 28.3kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.81G/1.81G [02:23<00:00, 13.5MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.69G/1.69G [01:26<00:00, 20.9MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.83G/1.83G [01:32<00:00, 21.3MB/s]\n",
      "Downloading:  30%|███████▋                  | 545M/1.80G [00:27<01:03, 21.3MB/s]2024-06-12 00:48:00,230 - modelscope - WARNING - Downloading: /Users/minkexiu/Downloads/GitHub/ML_Tryout/LLM/20240611_finance/trained_models/._____temp/ZhipuAI/glm-4-9b/model-00004-of-00010.safetensors failed, reason: ('Connection broken: IncompleteRead(1658484 bytes read, 166113676 more expected)', IncompleteRead(1658484 bytes read, 166113676 more expected)) will retry\n",
      "Downloading: 1.80GB [01:31, 21.0MB/s]                                           \n",
      "Downloading: 100%|█████████████████████████| 1.69G/1.69G [01:23<00:00, 21.6MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.83G/1.83G [01:33<00:00, 21.0MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.80G/1.80G [01:37<00:00, 19.8MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.69G/1.69G [01:29<00:00, 20.3MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.83G/1.83G [01:33<00:00, 21.0MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.54G/1.54G [01:29<00:00, 18.5MB/s]\n",
      "Downloading: 100%|██████████████████████████| 28.4k/28.4k [00:00<00:00, 127kB/s]\n",
      "Downloading: 100%|██████████████████████████| 51.0k/51.0k [00:00<00:00, 175kB/s]\n",
      "Downloading: 100%|█████████████████████████| 2.47k/2.47k [00:00<00:00, 12.3kB/s]\n",
      "Downloading: 100%|█████████████████████████| 15.3k/15.3k [00:00<00:00, 39.6kB/s]\n",
      "Downloading: 100%|█████████████████████████| 2.50M/2.50M [00:00<00:00, 4.48MB/s]\n",
      "Downloading: 100%|█████████████████████████| 3.12k/3.12k [00:00<00:00, 13.9kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/minkexiu/Downloads/GitHub/ML_Tryout/LLM/20240611_finance/trained_models/ZhipuAI/glm-4-9b'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\n",
    "    'ZhipuAI/glm-4-9b',\n",
    "    cache_dir=create_trained_models_path(\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b46fd-1507-4b1e-8069-fe7412cbb240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad2e79-5591-42fe-a84d-0c50817ca3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b2edc-0c5e-4685-82eb-277e19c21647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43cf41-4980-43bd-8f1f-a7ccc792db35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792b65f-aefe-41c5-b530-25af2bc732db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624c3b7e-3e7d-4a1b-9f76-341492cef86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 00:09:27,671 - modelscope - INFO - PyTorch version 2.2.2 Found.\n",
      "2024-06-06 00:09:27,672 - modelscope - INFO - Loading ast index from /Users/minkexiu/.cache/modelscope/ast_indexer\n",
      "2024-06-06 00:09:27,757 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 17686ef0d47f348c51ddcc1bbcd38b7c and a total number of 980 components indexed\n"
     ]
    }
   ],
   "source": [
    "from modelscope.models import Model\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbc43e9-a415-4778-84e2-fb269b4f9efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 00:09:29,655 - modelscope - INFO - initialize model from nlp_gte_sentence-embedding_chinese-small\n",
      "2024-06-06 00:09:29,984 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-06-06 00:09:29,985 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-06-06 00:09:29,985 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'nlp_gte_sentence-embedding_chinese-small'}. trying to build by task and model information.\n",
      "2024-06-06 00:09:29,997 - modelscope - INFO - cuda is not available, using cpu instead.\n",
      "2024-06-06 00:09:29,998 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-06-06 00:09:29,998 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-06-06 00:09:29,999 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'nlp_gte_sentence-embedding_chinese-small', 'sequence_length': 512}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "pipeline_se = pipeline(\n",
    "    Tasks.sentence_embedding,\n",
    "    model=Model.from_pretrained(\"nlp_gte_sentence-embedding_chinese-small\"),\n",
    "    sequence_length=512\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa748ad-0600-41db-b3dc-baa807ed0dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_embedding': array([[-0.04563808, -0.0622078 , -0.03775001, ...,  0.01267121,\n",
      "        -0.01111767, -0.03390382],\n",
      "       [-0.02073099, -0.04639562, -0.04818704, ..., -0.00754703,\n",
      "        -0.00731624, -0.02740853],\n",
      "       [-0.00037598, -0.05922903, -0.04592752, ..., -0.00697823,\n",
      "        -0.02154765, -0.0295116 ],\n",
      "       [-0.00491672, -0.02552056, -0.03427779, ..., -0.00760837,\n",
      "        -0.00404085, -0.0509829 ]], dtype=float32), 'scores': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/modeling_utils.py:1051: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # 当输入包含“soure_sentence”与“sentences_to_compare”时，会输出source_sentence中首个句子与sentences_to_compare中每个句子的向量表示，以及source_sentence中首个句子与sentences_to_compare中每个句子的相似度。\n",
    "# inputs = {\n",
    "#         \"source_sentence\": [\"吃完海鲜可以喝牛奶吗?\"],\n",
    "#         \"sentences_to_compare\": [\n",
    "#             \"不可以，早晨喝牛奶不科学\",\n",
    "#             \"吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害\",\n",
    "#             \"吃海鲜是不能同时喝牛奶吃水果，这个至少间隔6小时以上才可以。\",\n",
    "#             \"吃海鲜是不可以吃柠檬的因为其中的维生素C会和海鲜中的矿物质形成砷\"\n",
    "#         ]\n",
    "#     }\n",
    "# result = pipeline_se(input=inputs)\n",
    "# print (result)\n",
    "# '''\n",
    "# {'text_embedding': array([[ 1.6415151e-04,  2.2334497e-02, -2.4202393e-02, ...,\n",
    "#          2.7710509e-02,  2.5980933e-02, -3.1285528e-02],\n",
    "#        [-9.9107623e-03,  1.3627578e-03, -2.1072682e-02, ...,\n",
    "#          2.6786461e-02,  3.5029035e-03, -1.5877936e-02],\n",
    "#        [ 1.9877627e-03,  2.2191243e-02, -2.7656069e-02, ...,\n",
    "#          2.2540951e-02,  2.1780970e-02, -3.0861111e-02],\n",
    "#        [ 3.8688166e-05,  1.3409532e-02, -2.9691193e-02, ...,\n",
    "#          2.9900728e-02,  2.1570563e-02, -2.0719109e-02],\n",
    "#        [ 1.4484422e-03,  8.5943500e-03, -1.6661938e-02, ...,\n",
    "#          2.0832840e-02,  2.3828523e-02, -1.1581291e-02]], dtype=float32), 'scores': [0.8859604597091675, 0.9830712080001831, 0.966042160987854, 0.891857922077179]}\n",
    "# '''\n",
    "# 当输入仅含有soure_sentence时，会输出source_sentence中每个句子的向量表示。\n",
    "inputs2 = {\n",
    "        \"source_sentence\": [\n",
    "            \"不可以，早晨喝牛奶不科学\",\n",
    "            \"吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害\",\n",
    "            \"吃海鲜是不能同时喝牛奶吃水果，这个至少间隔6小时以上才可以。\",\n",
    "            \"吃海鲜是不可以吃柠檬的因为其中的维生素C会和海鲜中的矿物质形成砷\"\n",
    "        ]\n",
    "}\n",
    "result = pipeline_se(input=inputs2)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb59c243-9148-446f-9380-791c2b59fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需在GPU环境运行\n",
    "# 加载数据集过程可能由于网络原因失败，请尝试重新运行代码\n",
    "from modelscope.metainfo import Trainers                                                                                                                                                              \n",
    "from modelscope.msdatasets import MsDataset\n",
    "from modelscope.trainers import build_trainer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5056ffc4-4a29-4231-9da0-2f270ef2627a",
   "metadata": {},
   "source": [
    "import tempfile\n",
    "import os\n",
    "tmp_dir = tempfile.TemporaryDirectory().name\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)\n",
    "print(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0009fa-4150-48d1-b351-022baf9f091b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mMsDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataset_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnamespace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'modelscope'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mversion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'master'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhub\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodelscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHubs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mHubs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'modelscope'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msubset_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodelscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadMode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mDownloadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'reuse_dataset_if_exists'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPosixPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/minkexiu/.cache/modelscope/hub/datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_streaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstream_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcustom_cfg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodelscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataset_info_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MsDataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNativeIterableDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load a MsDataset from the ModelScope Hub, Hugging Face Hub, urls, or a local dataset.\n",
       "\n",
       "Args:\n",
       "    dataset_name (str): Path or name of the dataset.\n",
       "        The form of `namespace/dataset_name` is also supported.\n",
       "    namespace(str, optional): Namespace of the dataset. It should not be None if you load a remote dataset\n",
       "        from Hubs.modelscope,\n",
       "    namespace (str, optional):\n",
       "        Namespace of the dataset. It should not be None if you load a remote dataset\n",
       "        from Hubs.modelscope,\n",
       "    target (str, optional): Name of the column to output.\n",
       "    version (str, optional): Version of the dataset script to load:\n",
       "    subset_name (str, optional): Defining the subset_name of the dataset.\n",
       "    data_dir (str, optional): Defining the data_dir of the dataset configuration. I\n",
       "    data_files (str or Sequence or Mapping, optional): Path(s) to source data file(s).\n",
       "    split (str, optional): Which split of the data to load.\n",
       "    hub (Hubs or str, optional): When loading from a remote hub, where it is from. default Hubs.modelscope\n",
       "    download_mode (DownloadMode or str, optional): How to treat existing datasets. default\n",
       "                                                   DownloadMode.REUSE_DATASET_IF_EXISTS\n",
       "    cache_dir (str, Optional): User-define local cache directory.\n",
       "    use_streaming (bool, Optional): If set to True, no need to download all data files.\n",
       "                                    Instead, it streams the data progressively, and returns\n",
       "                                    NativeIterableDataset or a dict of NativeIterableDataset.\n",
       "    stream_batch_size (int, Optional): The batch size of the streaming data.\n",
       "    custom_cfg (str, Optional): Model configuration, this can be used for custom datasets.\n",
       "                               see https://modelscope.cn/docs/Configuration%E8%AF%A6%E8%A7%A3\n",
       "    token (str, Optional): SDK token of ModelScope.\n",
       "    dataset_info_only (bool, Optional): If set to True, only return the dataset config and info (dict).\n",
       "    **config_kwargs (additional keyword arguments): Keyword arguments to be passed\n",
       "\n",
       "Returns:\n",
       "    MsDataset (MsDataset): MsDataset object for a certain dataset.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/anaconda3/envs/ml12/lib/python3.12/site-packages/modelscope/msdatasets/ms_dataset.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MsDataset.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58deb10f-337d-4cf7-8757-aefc7e6a506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/datasets/load.py:2524: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/datasets/load.py:926: FutureWarning: The repository for dureader-retrieval-ranking contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /Users/minkexiu/Downloads/GitHub/ML_Tryout/LLM/20240605_zhongyi/pipline/originalData/dureader-retrieval-ranking/zyznull/dureader-retrieval-ranking/master/meta/dureader-retrieval-ranking.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "ds = MsDataset.load(\n",
    "    'dureader-retrieval-ranking', \n",
    "    'zyznull',\n",
    "    cache_dir = create_originalData_path('dureader-retrieval-ranking')\n",
    ")\n",
    "train_ds = ds['train'].to_hf_dataset()\n",
    "dev_ds = ds['dev'].to_hf_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27dcd9eb-abb6-428d-ba98-df73aa556bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <modelscope.msdatasets.ms_dataset.MsDataset at 0x2a399edb0>,\n",
       " 'dev': <modelscope.msdatasets.ms_dataset.MsDataset at 0x2a43d8bc0>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de472c1-8c87-4602-bc78-20d0947bdfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
       "    num_rows: 86395\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7edea-02be-45da-876a-cc522a9820b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9dd50-5437-496e-bf77-6cf34a500cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = 'damo/nlp_gte_sentence-embedding_chinese-base'\n",
    "def cfg_modify_fn(cfg):\n",
    "    cfg.task = 'sentence-embedding'\n",
    "    cfg['preprocessor'] = {'type': 'sentence-embedding','max_length': 256}\n",
    "    cfg['dataset'] = {\n",
    "        'train': {\n",
    "            'type': 'bert',\n",
    "            'query_sequence': 'query',\n",
    "            'pos_sequence': 'positive_passages',\n",
    "            'neg_sequence': 'negative_passages',\n",
    "            'text_fileds': ['text'],\n",
    "            'qid_field': 'query_id'\n",
    "        },\n",
    "        'val': {\n",
    "            'type': 'bert',\n",
    "            'query_sequence': 'query',\n",
    "            'pos_sequence': 'positive_passages',\n",
    "            'neg_sequence': 'negative_passages',\n",
    "            'text_fileds': ['text'],\n",
    "            'qid_field': 'query_id'\n",
    "        },\n",
    "    }\n",
    "    cfg['train']['neg_samples'] = 4\n",
    "    cfg['evaluation']['dataloader']['batch_size_per_gpu'] = 30\n",
    "    cfg.train.max_epochs = 1\n",
    "    cfg.train.train_batch_size = 4\n",
    "    return cfg \n",
    "kwargs = dict(\n",
    "    model=model_id,\n",
    "    train_dataset=train_ds,\n",
    "    work_dir=tmp_dir,\n",
    "    eval_dataset=dev_ds,\n",
    "    cfg_modify_fn=cfg_modify_fn)\n",
    "trainer = build_trainer(name=Trainers.nlp_sentence_embedding_trainer, default_args=kwargs)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f1b64-3da3-4f9a-b2b4-d61867b10ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd491fc-9b07-4b84-8b6f-0e747907bec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebb73462-0993-4034-9baf-5a8e7e8df4ab",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17a71cd-8212-406c-93d7-55f5a2c9bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer\n",
    "from transformers.training_args import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2304712-4410-4baa-89d0-208939f98ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17421ec93d1423b8852b0c0e9e418a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minkexiu/anaconda3/envs/chattts/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2418ba5452d04cee946803734188a380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b1210e4bc44108887e36cb6e4c8c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e61e281818e4609b4165f9ad1ec0a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7871f2e2dd514502bbb28e077716702a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"nlp_gte_sentence-embedding_chinese-small\")\n",
    "# model = AutoModel.from_pretrained(\"nlp_gte_sentence-embedding_chinese-small\", trust_remote_code=True)\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir = \"./\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97cc7176-354c-41bd-abad-adb24cb15bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ebf150-51fb-40f2-88af-10436322e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=create_trained_models_path(\"try_ft/\"), # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\", # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1, # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8, # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=10, # evaluation batch size\n",
    "    logging_steps=1000, # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=100,\n",
    "    # load_best_model_at_end=True, # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3, # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6f24c-a7c4-45d6-95ec-a136f5e3c02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9b8aa7-bff4-4516-84bf-a97620af715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = [\n",
    "    \"不可以，早晨喝牛奶不科学\",\n",
    "    \"吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害\",\n",
    "    \"吃海鲜是不能同时喝牛奶吃水果，这个至少间隔6小时以上才可以。\",\n",
    "    \"吃海鲜是不可以吃柠檬的因为其中的维生素C会和海鲜中的矿物质形成砷\"\n",
    "] * 10000\n",
    "test_txt = [\n",
    "    \"东北教育资源应该对全世界开放，让南方人自己卷。\",\n",
    "    \"如果穿一套这个肯定就不是奇装异服了。\",\n",
    "] * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3201f036-0861-4540-88e5-6a770af3f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a55bb95-31a2-4b03-a701-1039455b9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(tokenizer(train_txt))#.to(\"mps\")\n",
    "test_dataset = Dataset.from_dict(tokenizer(test_txt))#.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fc0d55-3cbd-491f-a8dc-548b2ab4bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type=\"torch\")\n",
    "test_dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7e439a-842c-4691-8aaf-fe611494b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b160b03-1822-424b-ab3d-17c7b43d73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/500 00:08 < 08:08, 1.00 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\u001b[38;5;66;03m#.select(range(10000)),\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,#.select(range(10000)),\n",
    "    eval_dataset=test_dataset,#.select(range(10000)),\n",
    ")\n",
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700a48a2-d196-4c76-9281-972b1d532063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    create_trained_models_path(\"try_ft/checkpoint-200\")\n",
    ")\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3deeda7-fc72-45df-8542-e9bc294d3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害，吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害，吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be388bee-c998-4483-bf3b-e03006c41062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /Users/minkexiu/Downloads/GitHub/ML_Tryout/LLM/20240605_zhongyi/pipline/trained_models/try_ft/checkpoint-200 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 151, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "# tokenizer = BertTokenizer.from_pretrained(\n",
    "#     create_trained_models_path(\"try_ft/checkpoint-200\")\n",
    "# )\n",
    "model = BertModel.from_pretrained(\n",
    "    create_trained_models_path(\"try_ft/checkpoint-200\")\n",
    ")\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "embeddings = output.last_hidden_state#[:, 0]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a974a-45ea-4cbb-a1c0-4f4ddd170b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b3f52-3969-4d32-8950-9187cd5f95a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eec539a6-2bdd-489d-8e3b-fabe41da11a0",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8b8d3-a821-4233-a2b0-5da50c25a5c1",
   "metadata": {},
   "source": [
    "## ~~GTE~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "137f97d6-cdb3-43bf-89ac-8637679bcdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "input_texts = [\n",
    "    \"不可以，早晨喝牛奶不科学\",\n",
    "    \"吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害\",\n",
    "    \"吃海鲜是不能同时喝牛奶吃水果，这个至少间隔6小时以上才可以。\",\n",
    "    \"吃海鲜是不可以吃柠檬的因为其中的维生素C会和海鲜中的矿物质形成砷\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlp_gte_sentence-embedding_chinese-small\")\n",
    "model = AutoModel.from_pretrained(\"nlp_gte_sentence-embedding_chinese-small\", trust_remote_code=True)\n",
    "\n",
    "batch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d92ae54-3437-45d1-9d5f-4f67cc0be62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 51, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59f6f8-c91e-4362-b49f-14610aba92d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cb146-4a43-477e-9288-bc3222535d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "889cce67-efc4-4dd7-872a-e283ef06459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = outputs.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95292b72-de88-4aa1-98b4-9ff841b76a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d84888-1dbc-4e3f-8510-180cd134517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = [\n",
    "    '零售木制品、门窗、仪器仪表、实验室专用设备、通讯器材、电子产品、五金交电、制冷专用设备、消防器材、工艺品（不含文物）、文化用品、针纺织品、化工产品（不含危险化学品）、金属材料、体育用品（不含弩）、塑料制品、计算机软件及辅助设备、日用品、服装鞋帽、皮革制品、玩具、化妆品、摄影器材、音响设备、珠宝首饰、宠物用品、建筑材料、玻璃制品、家具；日用电器修理（不符合家用电子电器维修业服务经营规范不得开展经营活动）；技术推广；企业策划；承办展览展示；会议服务；企业管理咨询；翻译服务；组织文化艺术交流活动；电脑图文设计；设计、制作、代理、发布广告；租赁建筑工程机械、建筑工程设备；出租办公用房、商业用房（不得作为有形市场经营用房）；教育咨询（不含培训）；家庭服务（不符合家政服务通用要求不得开展经营活动）；婚庆服务；医学研究（不含诊疗活动）；健康咨询、健康管理（须经审批的诊疗活动除外）；健身服务；修脚服务（不含诊疗活动）；机动车公共停车场经营管理；专业承包；施工总承包；城市园林绿化；物业管理；销售家用电器、机械设备及配件；经济贸易咨询；组装超声波加湿器；汽车租赁（不含九座以上客车）；道路货运代理；零售食品；工程勘察；工程设计。（市场主体依法自主选择经营项目，开展经营活动；依法须经批准的项目，经相关部门批准后依批准的内容开展经营活动；不得从事国家和本区产业政策禁止和限制类项目的经营活动。）',\n",
    "    \"带钢、钢管、型钢、钢材的制造、加工、销售。（依法须经批准的项目，经相关部门批准后方可开展经营活动）\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab83157-1b9c-4607-967a-e4c6ea70783a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919d5f2-6cba-46fc-9f00-875c0f2dffb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
