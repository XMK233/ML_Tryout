{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d92502",
   "metadata": {},
   "source": [
    "https://work.datafountain.cn/forum?id=223&type=2&source=1\n",
    "\n",
    "看了一下，基本上物品的特征就是id，用户倒是有一些标签。\n",
    "\n",
    "然后呢，这版实现，双塔的层数是一样的。可不可以不一样？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d2f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.init import constant_, normal_, xavier_normal_\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4dcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoder(object):\n",
    "\n",
    "    def __init__(self, x, y, batch_size=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.pr = 0 ## 我估计是present的意思。\n",
    "        self.batch_size = batch_size\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pr < len(self.x):\n",
    "            start = self.pr\n",
    "            end = self.pr + self.batch_size\n",
    "            if end > len(self.x):\n",
    "                end = len(self.x)\n",
    "            self.pr = end\n",
    "            return self.x[start:end], self.y[start:end]\n",
    "        else:\n",
    "            self.pr = 0\n",
    "            raise StopIteration\n",
    "\n",
    "class Logger(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "\n",
    "        self.logger = logging.getLogger(filename)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        formatter = logging.Formatter('%(asctime)s.%(msecs)03d: %(message)s',\n",
    "                                      datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # write into file\n",
    "        fh = logging.FileHandler(filename)\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        fh.setFormatter(formatter)\n",
    "\n",
    "        # show on console\n",
    "        ch = logging.StreamHandler(sys.stdout)\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "\n",
    "        # add to Handler\n",
    "        self.logger.addHandler(fh)\n",
    "        self.logger.addHandler(ch)\n",
    "\n",
    "\n",
    "    def _flush(self):\n",
    "        for handler in self.logger.handlers:\n",
    "            handler.flush()\n",
    "\n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "        self._flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00792d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.lr = 0.001\n",
    "        self.dropout = 0.3\n",
    "        self.batch_size = 1024\n",
    "        self.epochs = 50\n",
    "        self.embedding_size = 10\n",
    "        self.mlp_hidden_size = [256, 256, 256]\n",
    "        self.user_feature_num = 5  # ['user_id', 'age', 'gender', 'occupation', 'zip']\n",
    "        self.item_feature_num = 1  # ['movie_id']\n",
    "\n",
    "args = Config()\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    args.epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd21c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id\tuser_id\tgender\tage\toccupation\tzip\ttimestamp\trating\r",
      "\r\n",
      "61\t1\tM\t24\ttechnician\t85711\t878542420\t4\r",
      "\r\n",
      "189\t1\tM\t24\ttechnician\t85711\t888732928\t3\r",
      "\r\n",
      "33\t1\tM\t24\ttechnician\t85711\t878542699\t4\r",
      "\r\n",
      "160\t1\tM\t24\ttechnician\t85711\t875072547\t4\r",
      "\r\n",
      "20\t1\tM\t24\ttechnician\t85711\t887431883\t4\r",
      "\r\n",
      "202\t1\tM\t24\ttechnician\t85711\t875072442\t5\r",
      "\r\n",
      "171\t1\tM\t24\ttechnician\t85711\t889751711\t5\r",
      "\r\n",
      "265\t1\tM\t24\ttechnician\t85711\t878542441\t4\r",
      "\r\n",
      "155\t1\tM\t24\ttechnician\t85711\t878542201\t2\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head ../originalDataset/ml-100k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8a73d",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c75bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义用户侧，商品侧特征，label域\n",
    "user_features = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
    "item_features = ['movie_id']\n",
    "label_features = 'label'\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_table(\"../originalDataset/ml-100k.txt\") # pd.read_csv(\"../originalDataset/ml-100k.txt\", sep='\\t')\n",
    "data['label'] = (data['rating'] > 3).astype(int) ## label就是3分以上与否，说白了就是打高分。\n",
    "\n",
    "# 为类别属性进行id转换, 让其公用一套id\n",
    "## 所有的类别型特征，为什么要公用同一套id呢？有什么道理吗？\n",
    "## 或许看到后面，你会理解的。\n",
    "shift = 0  # 记录当前已经编码到的id \n",
    "for feature in user_features + item_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + shift\n",
    "    shift += data[feature].nunique()\n",
    "\n",
    "# 打乱数据\n",
    "## 我在别的地方看到一个数据划分方式，先按时间排序，再八二开。\n",
    "## 我觉得这有一定道理，因为后来的数据就是OOT了。\n",
    "## 今天就先不理会这部分了。以后慢慢理会。\n",
    "data = data.sample(frac=1).reset_index()\n",
    "\n",
    "# 训练集验证集划分，比例为8：2\n",
    "split = int(len(data) * 0.8)\n",
    "train_data = data.loc[:split]\n",
    "test_data = data.loc[split:]\n",
    "\n",
    "# 定义训练集和验证集\n",
    "X_train, y_train = train_data[user_features +\n",
    "                              item_features], train_data[label_features]\n",
    "X_test, y_test = test_data[user_features +\n",
    "                           item_features], test_data[label_features]\n",
    "\n",
    "\n",
    "# 构造Dataloader，用于对数据进行分batch\n",
    "train_batchs = DataLoder(X_train, y_train, batch_size=args.batch_size)\n",
    "test_batchs = DataLoder(X_test, y_test, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0e907",
   "metadata": {},
   "source": [
    "# MLP层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2635a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP层\n",
    "class MLPLayers(nn.Module):\n",
    "    def __init__(self, layers, dropout=0, activation='Tanh', bn=False, init_method=None):\n",
    "        super(MLPLayers, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.use_bn = bn\n",
    "        self.init_method = init_method\n",
    "\n",
    "        mlp_modules = []\n",
    "        for _, (input_size, output_size) in enumerate(zip(self.layers[:-1], self.layers[1:])): \n",
    "            ## 这些个layers都是input, output size\n",
    "            mlp_modules.append(nn.Dropout(p=self.dropout))\n",
    "            mlp_modules.append(nn.Linear(input_size, output_size))\n",
    "            if self.use_bn:\n",
    "                mlp_modules.append(nn.BatchNorm1d(num_features=output_size))\n",
    "            mlp_modules.append(nn.Tanh())\n",
    "            \n",
    "        self.mlp_layers = nn.Sequential(*mlp_modules) ## 前面弄完，这个就是真正的网络的层了。\n",
    "        if self.init_method is not None:\n",
    "            self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if self.init_method == 'norm':\n",
    "                normal_(module.weight.data, 0, 0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, input_feature):\n",
    "        return self.mlp_layers(input_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5d5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSSM 模型\n",
    "class DSSM(nn.Module):\n",
    "    def __init__(self, user_features=None, item_features=None, mlp_hidden_size=[256, 256],\n",
    "                 embedding_size=10, num_feature=None):\n",
    "        super().__init__()\n",
    "        # load parameters info\n",
    "        self.mlp_hidden_size = mlp_hidden_size ## [256, 256]\n",
    "        self.dropout_prob = args.dropout\n",
    "\n",
    "        self.user_feature = user_features\n",
    "        self.item_feature = item_features\n",
    "\n",
    "        self.user_feature_num = len(user_features) ## 5\n",
    "        self.item_feature_num = len(item_features) ## 1\n",
    "\n",
    "        self.embedding_size = embedding_size ## 10\n",
    "        self.num_feature = num_feature ## 这个值为多少呢？说白了就是shift变量，就是整套数据里面所有类别型变量的总类别数。\n",
    "\n",
    "        self.embed = nn.Embedding(num_feature, self.embedding_size) ## ?->10的一个算是映射吧\n",
    "\n",
    "        user_size_list = [self.embedding_size *\n",
    "                          self.user_feature_num] + self.mlp_hidden_size ## [10*5, 256, 256]\n",
    "        item_size_list = [self.embedding_size *\n",
    "                          self.item_feature_num] + self.mlp_hidden_size ## [10*1, 256, 256]\n",
    "        \n",
    "        # define layers and loss\n",
    "        self.user_mlp_layers = MLPLayers(\n",
    "            user_size_list, self.dropout_prob, bn=True)\n",
    "        self.item_mlp_layers = MLPLayers(\n",
    "            item_size_list, self.dropout_prob, bn=True)\n",
    "\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "\n",
    "        # parameters initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            xavier_normal_(module.weight.data)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            xavier_normal_(module.weight.data)\n",
    "            if module.bias is not None:\n",
    "                constant_(module.bias.data, 0)\n",
    "                \n",
    "    def embed_user_input(self, x):\n",
    "        results = []\n",
    "        user_x = x[self.user_feature]\n",
    "        for col in user_x.columns:\n",
    "            idx = torch.LongTensor(user_x[col].tolist())\n",
    "            results.append(self.embed(idx).unsqueeze(1))\n",
    "        return torch.cat(results, dim=1)\n",
    "\n",
    "    def embed_item_input(self, x):\n",
    "        results = []\n",
    "        item_x = x[self.item_feature]\n",
    "        for col in item_x.columns:\n",
    "            idx = torch.LongTensor(item_x[col].tolist())\n",
    "            results.append(self.embed(idx).unsqueeze(1))\n",
    "        return torch.cat(results, dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_user = self.embed_user_input(x)\n",
    "        embed_item = self.embed_item_input(x)\n",
    "\n",
    "        batch_size = embed_item.shape[0]\n",
    "        user_dnn_out = self.user_mlp_layers(embed_user.view(batch_size, -1))\n",
    "        item_dnn_out = self.item_mlp_layers(embed_item.view(batch_size, -1))\n",
    "        score = torch.cosine_similarity(user_dnn_out, item_dnn_out, dim=1)\n",
    "\n",
    "        sig_score = self.sigmod(score)\n",
    "        return sig_score.squeeze()\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        label = torch.FloatTensor(y.tolist())\n",
    "        output = self.forward(x)\n",
    "        return self.loss(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78b75f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([[1]]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6ef4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-14 14:38:33.674: Epoch 0 Loss 54.800395 AUC 0.518149 LogLoss 0.693298\n",
      "2022-05-14 14:38:38.363: Epoch 1 Loss 54.567612 AUC 0.556224 LogLoss 0.687731\n",
      "2022-05-14 14:38:43.027: Epoch 2 Loss 53.433128 AUC 0.623684 LogLoss 0.667887\n",
      "2022-05-14 14:38:47.819: Epoch 3 Loss 51.341466 AUC 0.680786 LogLoss 0.643096\n",
      "2022-05-14 14:38:52.577: Epoch 4 Loss 49.465626 AUC 0.712640 LogLoss 0.625642\n",
      "2022-05-14 14:38:57.385: Epoch 5 Loss 47.994871 AUC 0.738300 LogLoss 0.609770\n",
      "2022-05-14 14:39:02.089: Epoch 6 Loss 46.918527 AUC 0.753753 LogLoss 0.599737\n",
      "2022-05-14 14:39:06.794: Epoch 7 Loss 46.268345 AUC 0.764355 LogLoss 0.592893\n",
      "2022-05-14 14:39:11.511: Epoch 8 Loss 45.905574 AUC 0.770129 LogLoss 0.589347\n",
      "2022-05-14 14:39:16.265: Epoch 9 Loss 45.719941 AUC 0.772257 LogLoss 0.587734\n",
      "2022-05-14 14:39:20.978: Epoch 10 Loss 45.567423 AUC 0.773771 LogLoss 0.586722\n",
      "2022-05-14 14:39:25.701: Epoch 11 Loss 45.408663 AUC 0.774698 LogLoss 0.586207\n",
      "2022-05-14 14:39:30.406: Epoch 12 Loss 45.358069 AUC 0.774842 LogLoss 0.585972\n",
      "2022-05-14 14:39:35.471: Epoch 13 Loss 45.267716 AUC 0.776031 LogLoss 0.585469\n",
      "2022-05-14 14:39:40.221: Epoch 14 Loss 45.279023 AUC 0.776716 LogLoss 0.585285\n",
      "2022-05-14 14:39:44.961: Epoch 15 Loss 45.208290 AUC 0.776280 LogLoss 0.585344\n",
      "2022-05-14 14:39:49.722: Epoch 16 Loss 45.171147 AUC 0.776873 LogLoss 0.585249\n",
      "2022-05-14 14:39:54.565: Epoch 17 Loss 45.154046 AUC 0.777190 LogLoss 0.585052\n",
      "2022-05-14 14:39:59.436: Epoch 18 Loss 45.108624 AUC 0.777652 LogLoss 0.584777\n",
      "2022-05-14 14:40:04.585: Epoch 19 Loss 45.058449 AUC 0.777411 LogLoss 0.584841\n",
      "2022-05-14 14:40:09.836: Epoch 20 Loss 45.040429 AUC 0.777810 LogLoss 0.584777\n",
      "2022-05-14 14:40:14.779: Epoch 21 Loss 45.056013 AUC 0.777213 LogLoss 0.584884\n",
      "2022-05-14 14:40:19.590: Epoch 22 Loss 45.030584 AUC 0.777852 LogLoss 0.584733\n",
      "2022-05-14 14:40:24.473: Epoch 23 Loss 44.992543 AUC 0.777565 LogLoss 0.584925\n",
      "2022-05-14 14:40:29.353: Epoch 24 Loss 44.965491 AUC 0.777660 LogLoss 0.585184\n",
      "2022-05-14 14:40:34.227: Epoch 25 Loss 44.961342 AUC 0.777809 LogLoss 0.584943\n",
      "2022-05-14 14:40:39.006: Epoch 26 Loss 44.908760 AUC 0.778119 LogLoss 0.584896\n",
      "2022-05-14 14:40:44.038: Epoch 27 Loss 44.889897 AUC 0.778404 LogLoss 0.584698\n",
      "2022-05-14 14:40:49.009: Epoch 28 Loss 44.920881 AUC 0.778551 LogLoss 0.584558\n",
      "2022-05-14 14:40:53.797: Epoch 29 Loss 44.871001 AUC 0.778854 LogLoss 0.584485\n",
      "2022-05-14 14:40:58.627: Epoch 30 Loss 44.914321 AUC 0.778339 LogLoss 0.584742\n",
      "2022-05-14 14:41:03.414: Epoch 31 Loss 44.846739 AUC 0.778190 LogLoss 0.584824\n",
      "2022-05-14 14:41:08.301: Epoch 32 Loss 44.885542 AUC 0.778127 LogLoss 0.584862\n",
      "2022-05-14 14:41:13.243: Epoch 33 Loss 44.850842 AUC 0.778539 LogLoss 0.584768\n",
      "2022-05-14 14:41:18.003: Epoch 34 Loss 44.846207 AUC 0.778425 LogLoss 0.584829\n",
      "2022-05-14 14:41:22.848: Epoch 35 Loss 44.812684 AUC 0.778332 LogLoss 0.585032\n",
      "2022-05-14 14:41:27.634: Epoch 36 Loss 44.835206 AUC 0.778836 LogLoss 0.584834\n",
      "2022-05-14 14:41:32.519: Epoch 37 Loss 44.863856 AUC 0.777740 LogLoss 0.585105\n",
      "2022-05-14 14:41:37.492: Epoch 38 Loss 44.824769 AUC 0.777985 LogLoss 0.585238\n",
      "2022-05-14 14:41:42.262: Epoch 39 Loss 44.802775 AUC 0.778259 LogLoss 0.585079\n",
      "2022-05-14 14:41:47.162: Epoch 40 Loss 44.776115 AUC 0.778052 LogLoss 0.585070\n",
      "2022-05-14 14:41:52.308: Epoch 41 Loss 44.775610 AUC 0.778173 LogLoss 0.584974\n",
      "2022-05-14 14:41:57.126: Epoch 42 Loss 44.773082 AUC 0.777988 LogLoss 0.584966\n",
      "2022-05-14 14:42:01.945: Epoch 43 Loss 44.799997 AUC 0.778194 LogLoss 0.585008\n",
      "2022-05-14 14:42:06.711: Epoch 44 Loss 44.742788 AUC 0.778425 LogLoss 0.585086\n",
      "2022-05-14 14:42:11.422: Epoch 45 Loss 44.760637 AUC 0.778458 LogLoss 0.584940\n",
      "2022-05-14 14:42:16.161: Epoch 46 Loss 44.759746 AUC 0.778459 LogLoss 0.584866\n",
      "2022-05-14 14:42:21.007: Epoch 47 Loss 44.757467 AUC 0.778754 LogLoss 0.584780\n",
      "2022-05-14 14:42:25.862: Epoch 48 Loss 44.719453 AUC 0.778571 LogLoss 0.584934\n",
      "2022-05-14 14:42:30.785: Epoch 49 Loss 44.774709 AUC 0.778457 LogLoss 0.584963\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = DSSM(user_features=user_features, item_features=item_features, mlp_hidden_size=args.mlp_hidden_size,\n",
    "             embedding_size=args.embedding_size, num_feature=shift)\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = Adam(params=model.parameters(), lr=args.lr)\n",
    "\n",
    "# 创建日志\n",
    "logger = Logger('log.txt') \n",
    "\n",
    "# 训练\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    total_loss = None\n",
    "    model.train()\n",
    "    for idx, (x, y) in enumerate(train_batchs):\n",
    "        optimizer.zero_grad()\n",
    "        losses = model.calculate_loss(x, y)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        total_loss = losses.item() if total_loss is None else total_loss + losses.item()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad(): ## torch.no_grad() 是一个上下文管理器，被该语句 wrap 起来的部分将不会track 梯度。\n",
    "        for idx, (x, y) in enumerate(test_batchs):\n",
    "            pred = model(x)\n",
    "            preds.extend(pred.numpy())\n",
    "            trues.extend(y.tolist())\n",
    "\n",
    "    auc = roc_auc_score(trues, preds)\n",
    "    loss = log_loss(trues, preds)\n",
    "    logger.info(\"Epoch {:d} Loss {:4f} AUC {:4f} LogLoss {:4f}\".format(epoch, total_loss, auc, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
