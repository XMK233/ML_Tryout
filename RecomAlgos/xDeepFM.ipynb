{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1ba0ca",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/57162373\n",
    "\n",
    "https://github.com/jc-LeeHub/Recommend-System-tf2.0/tree/3741b742e81588b6d9259410ed51c964c4778e71/xDeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6440723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.out_layer = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        output = self.out_layer(inputs)\n",
    "        return output\n",
    "\n",
    "class Dense_layer(Layer):\n",
    "    def __init__(self, hidden_units, out_dim=1, activation='relu', dropout=0.0):\n",
    "        '''\n",
    "        hidden_units = [256, 128, 64]\n",
    "        '''\n",
    "        super(Dense_layer, self).__init__()\n",
    "        self.hidden_layers = [Dense(i, activation=activation) for i in hidden_units]\n",
    "        self.out_layer = Dense(out_dim, activation=None)\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # inputs: [None, n*k]\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        output = self.out_layer(x)\n",
    "        return output\n",
    "\n",
    "class CIN(Layer):\n",
    "    '''\n",
    "    我明白了，CIN的意义何在。\n",
    "    CIN部分最初的输入，是n个原始特征，每个特征都被embedding到特定维度emb_dim.\n",
    "    CIN每一层的cin_size意思就是：第一层就是n个特征，到了第二层假如cin_size是128_1，就是说，第二层计算完后“特征”数量从n变成了128_1。\n",
    "        个中的计算，从数学原理上说，是使用了外积，以及一些矩阵相乘。\n",
    "        不过这份代码在实现的时候，使用了tf中的1维卷积把这部分给做了。卷积怎么就能实现这个？我尚存疑。\n",
    "    如果还有后续的层，就是128_1个特征又变成了若干个特征。\n",
    "    经过了所有层，总共有(n + 128_1 + ......)个特征。把最开始的n个原始特征去掉不要，剩下的特征简单concat起来就好了。\n",
    "    最后的（128_1 + ....）个特征，每一个都是原来的n个特征的embedding向量各种乱七八糟地交叉乘啊，blabla，得到的。\n",
    "    所以xDeepFM就达成了特征向量级（vector_wise）交叉的目的。这就是有别于DCN的bit_wise交叉的地方。\n",
    "    \n",
    "    https://zhuanlan.zhihu.com/p/57162373 里面的“为啥取名CIN”这一部分介绍的黄色圆点的计算啊，简单来说，可以理解成[H, 1]矩阵乘上[1, m]矩阵得到一个[H, m]的矩阵。\n",
    "    这种乘，就是所谓的外积，参考：https://www.zhihu.com/question/419909144 \n",
    "    xDeepFM原文介绍黄色圆点的计算的时候，说的也是外积。所以没错了。\n",
    "    \n",
    "    我把原理整个撸了一遍之后，也没看出来Hadamard积在哪里有用到。难道被作者虚晃一枪？\n",
    "    '''\n",
    "    def __init__(self, cin_size):\n",
    "        '''\n",
    "        cin_size = [128, 128]\n",
    "        '''\n",
    "        super(CIN, self).__init__()\n",
    "        self.cin_size = cin_size  # 每层的矩阵个数\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [None, n, k]\n",
    "        self.field_num = [input_shape[1]] + self.cin_size # 每层的矩阵个数(包括第0层)；[n【就是特征数】, 128_1, 128_2]\n",
    "\n",
    "        self.cin_W = [self.add_weight(\n",
    "                         name='w'+str(i),\n",
    "                         shape=(1, self.field_num[0]*self.field_num[i], self.field_num[i+1]), ## 第一个是(1, n * n, 128_1), 第二个是(1, n*128_1, 128_2)\n",
    "                         initializer=tf.initializers.glorot_uniform(),\n",
    "                         regularizer=tf.keras.regularizers.l1_l2(1e-5),\n",
    "                         trainable=True)\n",
    "                      for i in range(len(self.field_num)-1)] ## [self.add_weight( i blabla ) for i in range(2)]\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # inputs: [None, n, k] 这里的k就是emb_dim，n是特征数。\n",
    "        k = inputs.shape[-1] ## k\n",
    "        res_list = [inputs] ## [inputs(None, n, k)]\n",
    "        X0 = tf.split(inputs, k, axis=-1) ## 难道是 k * [?, n, 1]          # 最后维切成k份，list: k * [None, field_num[0], 1] ## \n",
    "        for i, size in enumerate(self.field_num[1:]): ## [128_1, 128_2]\n",
    "            Xi = tf.split(res_list[-1], k, axis=-1) # list: k * [None, field_num[i], 1]\n",
    "            \n",
    "            ## 下面那一行，transpose_b=True的意思是，tf.matmul(a, b)里面的b进行转置。这样一来，。。。\n",
    "            ## 就是k * [?,n,1]的变量乘以k * [?, 1, field_num[i]]，乘完后, 自然得到k * [None, field_num[0], field_num[i]]\n",
    "            x = tf.matmul(X0, Xi, transpose_b=True) # list: k * [None, field_num[0], field_num[i]]\n",
    "            \n",
    "            x = tf.reshape(x, shape=[k, -1, self.field_num[0]*self.field_num[i]])\n",
    "                                                    # [k, None, field_num[0]*field_num[i]]\n",
    "            x = tf.transpose(x, [1, 0, 2])          # [None, k, field_num[0]*field_num[i]] ## [?, emb_dim, n*n]\n",
    "            \n",
    "            # print(self.cin_W[i].shape) ## 打印出来之后啊，我发现我前面总结的self.cin_W的维度是正确的。\n",
    "            \n",
    "            \n",
    "            x = tf.nn.conv1d(input=x, filters=self.cin_W[i], stride=1, padding='VALID') ## self.cin_W[i]：(1, n*n, 128_1), 意思难道是：（扫1层得到一个值 * emb_dim层）* 128_1次？\n",
    "                                                    # (None, k, field_num[i+1]) ## 输出的x的形状：(?, emb_dim, 128_1)。总的来说不是完全理解里面的原理，姑且接受这个现实吧。\n",
    "            x = tf.transpose(x, [0, 2, 1])          # (None, field_num[i+1], k) ## (?, 128_1, emb_dim)\n",
    "            res_list.append(x)\n",
    "\n",
    "        res_list = res_list[1:]   # 去掉X0\n",
    "        res = tf.concat(res_list, axis=1)  # (None, field_num[1]+...+field_num[n], k) ## (?, 128_1 + 128_2, emb_dim)\n",
    "        output = tf.reduce_sum(res, axis=-1)  # (None, field_num[1]+...+field_num[n]) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61515ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "\n",
    "class xDeepFM(Model):\n",
    "    def __init__(self, feature_columns, cin_size, hidden_units, out_dim=1, activation='relu', dropout=0.0):\n",
    "        '''\n",
    "        hidden_units = [256, 128, 64]\n",
    "        cin_size = [128, 128]\n",
    "        '''\n",
    "        super(xDeepFM, self).__init__()\n",
    "        self.dense_feature_columns, self.sparse_feature_columns = feature_columns\n",
    "        self.embed_layers = [Embedding(feat['feat_onehot_dim'], feat['embed_dim'])\n",
    "                                    for feat in self.sparse_feature_columns]\n",
    "        self.linear = Linear()\n",
    "        self.dense_layer = Dense_layer(hidden_units, out_dim, activation, dropout)\n",
    "        self.cin_layer = CIN(cin_size)\n",
    "        self.out_layer = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        dense_inputs, sparse_inputs = inputs[:, :13], inputs[:, 13:]\n",
    "\n",
    "        # linear\n",
    "        linear_out = self.linear(inputs)\n",
    "        \n",
    "        ## 这里n是特征的数量，k是embedding向量的维度。\n",
    "        emb = [self.embed_layers[i](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])] # [n, None, k]\n",
    "        emb = tf.transpose(tf.convert_to_tensor(emb), [1, 0, 2]) # [None, n, k]\n",
    "\n",
    "        # CIN\n",
    "        cin_out = self.cin_layer(emb)\n",
    "\n",
    "        # Dense\n",
    "        emb = tf.reshape(emb, shape=(-1, emb.shape[1]*emb.shape[2])) ## [None, n*k] 稀疏矩阵\n",
    "        emb = tf.concat([dense_inputs, emb], axis=1) ## 稠密矩阵和稀疏矩阵concat一下。\n",
    "        dense_out = self.dense_layer(emb)\n",
    "\n",
    "        output = self.out_layer(linear_out + cin_out + dense_out)\n",
    "        return tf.nn.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8496c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sparseFeature(feat, feat_onehot_dim, embed_dim):\n",
    "    return {'feat': feat, 'feat_onehot_dim': feat_onehot_dim, 'embed_dim': embed_dim}\n",
    "\n",
    "def denseFeature(feat):\n",
    "    return {'feat': feat}\n",
    "\n",
    "def create_criteo_dataset(file_path, embed_dim=8, test_size=0.2):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)] ## 一系列I开头的特征，是稠密特征。\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)] ## 一系列C开头的特征，是稀疏特征。\n",
    "\n",
    "    #缺失值填充\n",
    "    data[dense_features] = data[dense_features].fillna(0)\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1')\n",
    "\n",
    "    #归一化\n",
    "    data[dense_features] = MinMaxScaler().fit_transform(data[dense_features])\n",
    "    #LabelEncoding编码\n",
    "    for col in sparse_features:\n",
    "        data[col] = LabelEncoder().fit_transform(data[col]).astype(int)\n",
    "\n",
    "    feature_columns = [[denseFeature(feat) for feat in dense_features]] + \\\n",
    "           [[sparseFeature(feat, data[feat].nunique(), embed_dim) for feat in sparse_features]]\n",
    "\n",
    "    X = data.drop(['label'], axis=1).values\n",
    "    y = data['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    return feature_columns, (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4233e5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 0 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 0 batch: 20 loss: 4.338266849517822\n",
      "epoch: 0 batch: 30 loss: 4.820296287536621\n",
      "epoch: 0 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 1 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 1 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 1 batch: 20 loss: 4.338266849517822\n",
      "epoch: 1 batch: 30 loss: 4.820296287536621\n",
      "epoch: 1 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 2 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 2 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 2 batch: 20 loss: 4.338266849517822\n",
      "epoch: 2 batch: 30 loss: 4.820296287536621\n",
      "epoch: 2 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 3 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 3 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 3 batch: 20 loss: 4.338266849517822\n",
      "epoch: 3 batch: 30 loss: 4.820296287536621\n",
      "epoch: 3 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 4 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 4 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 4 batch: 20 loss: 4.338266849517822\n",
      "epoch: 4 batch: 30 loss: 4.820296287536621\n",
      "epoch: 4 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 5 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 5 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 5 batch: 20 loss: 4.338266849517822\n",
      "epoch: 5 batch: 30 loss: 4.820296287536621\n",
      "epoch: 5 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 6 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 6 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 6 batch: 20 loss: 4.338266849517822\n",
      "epoch: 6 batch: 30 loss: 4.820296287536621\n",
      "epoch: 6 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 7 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 7 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 7 batch: 20 loss: 4.338266849517822\n",
      "epoch: 7 batch: 30 loss: 4.820296287536621\n",
      "epoch: 7 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 8 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 8 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 8 batch: 20 loss: 4.338266849517822\n",
      "epoch: 8 batch: 30 loss: 4.820296287536621\n",
      "epoch: 8 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 9 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 9 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 9 batch: 20 loss: 4.338266849517822\n",
      "epoch: 9 batch: 30 loss: 4.820296287536621\n",
      "epoch: 9 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 10 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 10 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 10 batch: 20 loss: 4.338266849517822\n",
      "epoch: 10 batch: 30 loss: 4.820296287536621\n",
      "epoch: 10 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 11 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 11 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 11 batch: 20 loss: 4.338266849517822\n",
      "epoch: 11 batch: 30 loss: 4.820296287536621\n",
      "epoch: 11 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 12 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 12 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 12 batch: 20 loss: 4.338266849517822\n",
      "epoch: 12 batch: 30 loss: 4.820296287536621\n",
      "epoch: 12 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 13 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 13 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 13 batch: 20 loss: 4.338266849517822\n",
      "epoch: 13 batch: 30 loss: 4.820296287536621\n",
      "epoch: 13 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 14 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 14 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 14 batch: 20 loss: 4.338266849517822\n",
      "epoch: 14 batch: 30 loss: 4.820296287536621\n",
      "epoch: 14 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 15 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 15 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 15 batch: 20 loss: 4.338266849517822\n",
      "epoch: 15 batch: 30 loss: 4.820296287536621\n",
      "epoch: 15 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 16 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 16 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 16 batch: 20 loss: 4.338266849517822\n",
      "epoch: 16 batch: 30 loss: 4.820296287536621\n",
      "epoch: 16 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 17 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 17 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 17 batch: 20 loss: 4.338266849517822\n",
      "epoch: 17 batch: 30 loss: 4.820296287536621\n",
      "epoch: 17 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 18 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 18 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 18 batch: 20 loss: 4.338266849517822\n",
      "epoch: 18 batch: 30 loss: 4.820296287536621\n",
      "epoch: 18 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 19 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 19 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 19 batch: 20 loss: 4.338266849517822\n",
      "epoch: 19 batch: 30 loss: 4.820296287536621\n",
      "epoch: 19 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 20 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 20 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 20 batch: 20 loss: 4.338266849517822\n",
      "epoch: 20 batch: 30 loss: 4.820296287536621\n",
      "epoch: 20 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 21 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 21 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 21 batch: 20 loss: 4.338266849517822\n",
      "epoch: 21 batch: 30 loss: 4.820296287536621\n",
      "epoch: 21 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 22 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 22 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 22 batch: 20 loss: 4.338266849517822\n",
      "epoch: 22 batch: 30 loss: 4.820296287536621\n",
      "epoch: 22 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 23 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 23 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 23 batch: 20 loss: 4.338266849517822\n",
      "epoch: 23 batch: 30 loss: 4.820296287536621\n",
      "epoch: 23 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 24 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 24 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 24 batch: 20 loss: 4.338266849517822\n",
      "epoch: 24 batch: 30 loss: 4.820296287536621\n",
      "epoch: 24 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 25 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 25 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 25 batch: 20 loss: 4.338266849517822\n",
      "epoch: 25 batch: 30 loss: 4.820296287536621\n",
      "epoch: 25 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 26 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 26 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 26 batch: 20 loss: 4.338266849517822\n",
      "epoch: 26 batch: 30 loss: 4.820296287536621\n",
      "epoch: 26 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 27 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 27 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 27 batch: 20 loss: 4.338266849517822\n",
      "epoch: 27 batch: 30 loss: 4.820296287536621\n",
      "epoch: 27 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 28 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 28 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 28 batch: 20 loss: 4.338266849517822\n",
      "epoch: 28 batch: 30 loss: 4.820296287536621\n",
      "epoch: 28 batch: 40 loss: 3.8562369346618652\n",
      "epoch: 29 batch: 0 loss: 2.8921778202056885\n",
      "epoch: 29 batch: 10 loss: 3.8562371730804443\n",
      "epoch: 29 batch: 20 loss: 4.338266849517822\n",
      "epoch: 29 batch: 30 loss: 4.820296287536621\n",
      "epoch: 29 batch: 40 loss: 3.8562369346618652\n",
      "Accuracy:  0.7975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file = '../originalDataset/Criteo.txt'\n",
    "    test_size = 0.2\n",
    "    hidden_units = [256, 128, 64]\n",
    "    dropout = 0.3\n",
    "    cin_size = [128, 128]\n",
    "    \n",
    "    ## \n",
    "    feature_columns, (X_train, y_train), (X_test, y_test) = create_criteo_dataset(file, test_size=test_size) \n",
    "\n",
    "    #########################################################################\n",
    "    model = xDeepFM(feature_columns, cin_size, hidden_units, dropout=dropout)\n",
    "    #########################################################################\n",
    "    \n",
    "    optimizer = optimizers.SGD(0.01)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    summary_writer = tf.summary.create_file_writer('./tensorboard')\n",
    "    for epoch in range(30):\n",
    "        loss_summary = []\n",
    "        for batch, data_batch in enumerate(train_dataset):\n",
    "            X_train, y_train = data_batch[0], data_batch[1]\n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                ##############################\n",
    "                y_pre = model(X_train)\n",
    "                ##############################\n",
    "                \n",
    "                loss = tf.reduce_mean(losses.binary_crossentropy(y_true=y_train, y_pred=y_pre))\n",
    "                grad = tape.gradient(loss, model.variables)\n",
    "                optimizer.apply_gradients(grads_and_vars=zip(grad, model.variables))\n",
    "            if batch%10==0:\n",
    "                print('epoch: {} batch: {} loss: {}'.format(epoch, batch, loss.numpy()))\n",
    "            loss_summary.append(loss.numpy())\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"loss\", np.mean(loss_summary), step=epoch)\n",
    "\n",
    "    pre = model(X_test)\n",
    "    pre = [1 if x>0.5 else 0 for x in pre]\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f3f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
