{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Networks(Metrics Based Method): Omniglot Dataset\n",
    "\n",
    "In this tutorial, we will learn how to create Matching Networks, and train it on Omniglot Dataset.\n",
    "\n",
    "###### To begin, Let's first understand what Omniglot dataset is:\n",
    "\n",
    "The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds. For more details, please refer : https://github.com/brendenlake/omniglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching Networks Architecture\n",
    "\n",
    "Matching networks, in general, proposes a framework which learns a network that maps a small training dataset and test unlabeled example to same embeddings space. Matching networks aim to learn the proper embeddings representation of small training dataset and use differentiable k-NN with cosine similarity measure to ensure whether a test data point is something ever seen or not.\n",
    "\n",
    "Matching networks are designed to be two-fold:\n",
    "\n",
    "Modeling level: At Modeling level, they proposed Matching nets, which uses advances made in attention and memory that enable fast and efficient learning.\n",
    "\n",
    "Training procedure: At Training Level, they have one condition that distribution of training and test set must be the same. For example: show a few examples per class, switching the task from minibatch to minibatch, similar to how it will be tested when presented with a few examples of a new task.\n",
    "\n",
    "\n",
    "![Screen%20Shot%202019-05-15%20at%2011.10.18%20PM.png](Images/MatchingNetworks.png)\n",
    "\n",
    "\n",
    "As we have learned, Matching Networks Architecture implementation consists of following 5 important parts:\n",
    "\n",
    "1. Embeddings Extractor, g.\n",
    "2. Full Context Embeddings, Bi-directional LSTM, f\n",
    "3. Cosine Similarity Distance Function, c\n",
    "4. Attention Model: Softmax(c)\n",
    "5. Loss Function: Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Import all necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: We will load omniglot dataset, tranformed in .npy format using helper script.  \n",
    "In helper script, we are just loading data in size format: [total_number,character,28,28].                             \n",
    "for more details, go through helper.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # setting batch_size\n",
    "classes_per_set = 20 # Number of classes per set\n",
    "samples_per_class = 1 # as we are choosing it to be one shot learning, so we have 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1623, 20, 28, 28)\n",
      "<class 'numpy.ndarray'>\n",
      "(1200, 20, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.load('../../originalDataset/omniglot-master/python/dataset.npy') # Load Data\n",
    "print (x.shape)\n",
    "print (type(x))\n",
    "n_classes = x.shape[0] # total number of classes\n",
    "x = np.reshape(x, newshape=(x.shape[0], x.shape[1], 28, 28, 1)) # expand dimension from (x.shape[0],x.shape[1],28,28)\n",
    "np.random.shuffle(x) # shuffle dataset\n",
    "x_train, x_val, x_test = x[:1200], x[1200:1411], x[1411:] # divide dataset in to train, val,ctest\n",
    "\n",
    "print (x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess Images: Here we have use normalization method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processes_batch(data, mu, sigma):\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "# Normalize Dataset\n",
    "x_train = processes_batch(x_train, np.mean(x_train), np.std(x_train))\n",
    "x_val = processes_batch(x_val, np.mean(x_val), np.std(x_val))\n",
    "x_test = processes_batch(x_test, np.mean(x_test), np.std(x_test))\n",
    "\n",
    "# Defining dictionary of dataset\n",
    "datatset = {\"train\": x_train, \"val\": x_val, \"test\": x_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Visualize example of one character written by 20 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = x_train[5,:,:,:,:]  \n",
    "# for i in range(0,20):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(temp[i,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 3: Training Data Processing\n",
    "To Load Omniglot dataset, and prepare it for Matching Networks Architecture, we need to create:\n",
    "1. Label Set: Variable choose_label \n",
    "2. Support Set: support_set_x, support_set_y\n",
    "3. Batch from Suppport Set Examples\n",
    "\n",
    "Let's first create a batch which can give a support set, and target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(data):\n",
    "    \"\"\"\n",
    "    Generates sample batch \n",
    "    :param : data - one of(train,test,val) our current dataset shape [total_classes,20,28,28,1]\n",
    "    :return: [support_set_x,support_set_y,target_x,target_y] for Matching Networks\n",
    "\n",
    "    每一个batch，都重复做batch_size次这样的操作：\n",
    "    1. 从一千多个class里面选择20个，是support_set_x。每个类都只有1个图片，每个图片是28*28*1。\n",
    "    2. 这20个class对应的support_set_y就是0～19.\n",
    "    3. 从这20个class里面选择出1个作为target_x, target_y。\n",
    "    嗯差不多就是这样。\n",
    "    \"\"\"\n",
    "    support_set_x = np.zeros((batch_size, classes_per_set, samples_per_class, data.shape[2],\n",
    "                              data.shape[3], data.shape[4]), np.float32)\n",
    "    support_set_y = np.zeros((batch_size, classes_per_set, samples_per_class), np.int32)\n",
    "\n",
    "    target_x = np.zeros((batch_size, data.shape[2], data.shape[3], data.shape[4]), np.float32)\n",
    "    target_y = np.zeros((batch_size, 1), np.int32)\n",
    "    for i in range(batch_size):\n",
    "        choose_classes = np.random.choice(data.shape[0], size=classes_per_set, replace=False) # choosing random classes\n",
    "        choose_label = np.random.choice(classes_per_set, size=1) # label set\n",
    "        choose_samples = np.random.choice(data.shape[1], size=samples_per_class + 1, replace=False)\n",
    "        x_temp = data[choose_classes] # choosing classes\n",
    "        x_temp = x_temp[:, choose_samples] # choosing sample batch from classes chosen outputs (20X2X28X28X1)\n",
    "        y_temp = np.arange(classes_per_set) # will return [0,1,2,3,...,19]\n",
    "        support_set_x[i] = x_temp[:, :-1] ## (20X前面那1个X28*28*1)\n",
    "        support_set_y[i] = np.expand_dims(y_temp[:], axis=1) # expand dimension ## shape will be (20,1)\n",
    "        target_x[i] = x_temp[choose_label, -1] ## (1X后面那1个X28X28X1)\n",
    "        target_y[i] = y_temp[choose_label] ## shape = (1,)\n",
    "    return support_set_x, support_set_y, target_x, target_y # returns support of [batch_size, 20 classes per set, 1 sample, 28, 28,1]\n",
    "    \n",
    "def get_batch(dataset_name):\n",
    "    \"\"\"\n",
    "    gen batch while training\n",
    "    :param dataset_name: The name of dataset(one of \"train\",\"val\",\"test\")\n",
    "    :return: a batch images\n",
    "\n",
    "    实际上本质上还是sample_batch操作，只不过多了一步，就是把某一个形状是1的维度消去罢了。\n",
    "    \"\"\"\n",
    "    support_set_x, support_set_y, target_x, target_y = sample_batch(datatset[dataset_name])\n",
    "    support_set_x = support_set_x.reshape((support_set_x.shape[0], support_set_x.shape[1] * support_set_x.shape[2],\n",
    "                                           support_set_x.shape[3], support_set_x.shape[4], support_set_x.shape[5]))\n",
    "    support_set_y = support_set_y.reshape(support_set_y.shape[0], support_set_y.shape[1] * support_set_y.shape[2])\n",
    "    return support_set_x, support_set_y, target_x, target_y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall, Matching Networks architecture, you will remember that there are mainly 4 parts of network:\n",
    "1. Embeddings extractor(g)\n",
    "2. Full-Context Embeddings(f)\n",
    "2. Attention Model(a)\n",
    "3. Distance Function(c)\n",
    "\n",
    "\n",
    "\n",
    "so, In this section first we will create a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Create an Embeddings extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convLayer(in_channels, out_channels, dropout_prob=0.0):\n",
    "    \"\"\"\n",
    "    :param dataset_name: The name of dataset(one of \"train\",\"val\",\"test\")\n",
    "    :return: a batch images\n",
    "    \"\"\"\n",
    "    cnn_seq = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Dropout(dropout_prob)\n",
    "    )\n",
    "    return cnn_seq\n",
    "\n",
    "class Embeddings_extractor(nn.Module):\n",
    "    def __init__(self, layer_size=64, num_channels=1, dropout_prob=0.5, image_size=28):\n",
    "        super(Embeddings_extractor, self).__init__()\n",
    "        \"\"\"\n",
    "        Build a CNN to produce embeddings\n",
    "        :param layer_size:64(default)\n",
    "        :param num_channels:\n",
    "        :param keep_prob:\n",
    "        :param image_size:\n",
    "        \"\"\"\n",
    "        self.layer1 = convLayer(num_channels, layer_size, dropout_prob)\n",
    "        self.layer2 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "        self.layer3 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "        self.layer4 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "\n",
    "        finalSize = int(math.floor(image_size / (2 * 2 * 2 * 2)))\n",
    "        self.outSize = finalSize * finalSize * layer_size\n",
    "\n",
    "    def forward(self, image_input):\n",
    "        \"\"\"\n",
    "        :param: Image\n",
    "        :return: embeddings\n",
    "        \"\"\"\n",
    "        x = self.layer1(image_input)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 5: Create an Attention model after classifier.\n",
    "a(x,x^)= softmax of cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionalClassify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionalClassify, self).__init__()\n",
    "\n",
    "    def forward(self, similarities, support_set_y):\n",
    "        \"\"\"\n",
    "        Products pdfs over the support set classes for the target set image.\n",
    "        :param similarities: A tensor with cosine similarites of size[batch_size,sequence_length]\n",
    "        :param support_set_y:[batch_size,sequence_length,classes_num]\n",
    "        :return: Softmax pdf shape[batch_size,classes_num]\n",
    "        \"\"\"\n",
    "        ## 这个的输入的形状是啥呢？\n",
    "        ## similarities的形状(16,20)，表示的是各个batch里面，目标图片的向量跟support的各个图片的相似程度。 \n",
    "        ## (16,20,20)  support_set_y则表示了，support图片的label的onehot形式。本质上这个张量，每个batch里面都是一个20维的对角矩阵。\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        softmax_similarities = softmax(similarities)\n",
    "        preds = softmax_similarities.unsqueeze(1).bmm(support_set_y).squeeze()\n",
    "        ## unsqueeze之后变成（16，1，20），跟 (16,20,20) 相乘，得到的应该是(16,1,20)，然后再squeeze一下得到（16，20）\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Create a Distance Network, which will take output from Test Image, and training embeddings, to calculate the Distance.\n",
    "\n",
    "find cosine similarities between support set and input_test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This model calculates the cosine distance between each of the support set embeddings and \n",
    "    the target image embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistanceNetwork, self).__init__()\n",
    "\n",
    "    def forward(self, support_set, input_image):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        :param support_set:the embeddings of the support set images.shape[sequence_length,batch_size,64]\n",
    "        :param input_image: the embedding of the target image,shape[batch_size,64]\n",
    "        :return:shape[batch_size,sequence_length]\n",
    "        \"\"\"\n",
    "        ## 输入的形状，我们分别设定为 (20, 16, 64) (16, 64)\n",
    "        eps = 1e-10\n",
    "        similarities = []\n",
    "        for support_image in support_set: ## support_image的形状(16,64)\n",
    "            sum_support = torch.sum(torch.pow(support_image, 2), 1) ## (16,)\n",
    "            support_manitude = sum_support.clamp(eps, float(\"inf\")).rsqrt() ## rsqrt，就是（开方然后一个倒数操作）\n",
    "            dot_product = input_image.unsqueeze(1).bmm(support_image.unsqueeze(2)).squeeze()\n",
    "            ## input_image unsqueeze之后是（16，1，64），support_image.unsqueeze(2)之后是(16,64,1),\n",
    "            ## bmm是batch matmul的意思，所以得到(16, 1)，再squeeze一下就是（16，）了\n",
    "            cosine_similarity = dot_product * support_manitude ## 这一步乘法，就得到了最后的余弦相似度。形状(16,)\n",
    "            ## 我横竖看了几遍，觉得这里余弦相似度的求法，似乎不对，上面这个cosine_similarity应该再\n",
    "            ## 除以一个input_image的向量的模长。\n",
    "            ## 但后来我意识到，如果similarity的结果是用来做比较的，那么大可不必再除以一个常数了，因为这不影响比较的结果。\n",
    "            ## 不过similarity值本身就很重要，那还是要把这个数除上去的。\n",
    "            similarities.append(cosine_similarity)\n",
    "        similarities = torch.stack(similarities) ## (20, 16)\n",
    "        return similarities.t() ## (16, 20) ## 这个similarity就代表了，input_image跟20个class的样本之间的相似程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7:  Create a Bi-directional LSTM, which is taking input and output from Test-image, and put them in same embeddings space.  \n",
    "If we wish to use full-context embeddings, Matching Networks introduced Bi-directional LSTM for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, layer_size, batch_size, vector_dim):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        \"\"\"\n",
    "        Initial a muti-layer Bidirectional LSTM\n",
    "        :param layer_size: a list of each layer'size\n",
    "        :param batch_size: \n",
    "        :param vector_dim: \n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = layer_size[0]\n",
    "        self.vector_dim = vector_dim\n",
    "        self.num_layer = len(layer_size)\n",
    "        self.lstm = nn.LSTM(input_size=self.vector_dim, num_layers=self.num_layer, hidden_size=self.hidden_size,\n",
    "                            bidirectional=True)\n",
    "        self.hidden = (Variable(torch.zeros(self.lstm.num_layers * 2, self.batch_size, self.lstm.hidden_size),requires_grad=False),\n",
    "                Variable(torch.zeros(self.lstm.num_layers * 2, self.batch_size, self.lstm.hidden_size),requires_grad=False))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, \n",
    "        to detach them from their history.\"\"\"\n",
    "        if type(h) == torch.Tensor:\n",
    "            return Variable(h.data)\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.hidden = self.repackage_hidden(self.hidden)\n",
    "        output, self.hidden = self.lstm(inputs, self.hidden)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 8: Let's club all small modules we made, and create a matching network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingNetwork(nn.Module):\n",
    "    def __init__(self, keep_prob, batch_size=32, num_channels=1, learning_rate=1e-3, fce=False, num_classes_per_set=20, \\\n",
    "                 num_samples_per_class=1, image_size=28):\n",
    "        \"\"\"\n",
    "        Matching Network\n",
    "        :param keep_prob: dropout rate\n",
    "        :param batch_size:\n",
    "        :param num_channels:\n",
    "        :param learning_rate:\n",
    "        :param fce: Flag indicating whether to use full context embeddings(i.e. apply an LSTM on the CNN embeddings)\n",
    "        :param num_classes_per_set:\n",
    "        :param num_samples_per_class:\n",
    "        :param image_size:\n",
    "        \"\"\"\n",
    "        super(MatchingNetwork, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.num_channels = num_channels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes_per_set = num_classes_per_set\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.image_size = image_size\n",
    "        # Let's set all peices of Matching Networks Architecture\n",
    "        self.g = Embeddings_extractor(layer_size=64, num_channels=num_channels, dropout_prob=keep_prob, image_size=image_size)\n",
    "        self.f = fce # if we are considering full-context embeddings\n",
    "        self.c = DistanceNetwork() # cosine distance among embeddings\n",
    "        self.a = AttentionalClassify() # softmax of cosine distance of embeddings\n",
    "        if self.f:\n",
    "            self.lstm = BidirectionalLSTM(layer_size=[32], batch_size=self.batch_size, vector_dim=self.g.outSize)\n",
    "\n",
    "    def forward(self, support_set_images, support_set_y_one_hot, target_image, target_y):\n",
    "        \"\"\"\n",
    "        Main process of the network\n",
    "        :param support_set_images: shape[batch_size,sequence_length,num_channels,image_size,image_size]\n",
    "        :param support_set_y_one_hot: shape[batch_size,sequence_length,num_classes_per_set]\n",
    "        :param target_image: shape[batch_size,num_channels,image_size,image_size]\n",
    "        :param target_y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # produce embeddings for support set images\n",
    "        encoded_images = []\n",
    "        for i in np.arange(support_set_images.size(1)):\n",
    "            gen_encode = self.g(support_set_images[:, i, :, :]) ## self.g的输入和输出的维度分别是([16, 1, 28, 28]) ([16, 64])\n",
    "            encoded_images.append(gen_encode)\n",
    "        ## 循环结束后，理论上encoded_images的维度应该是(20, 16, 64)\n",
    "        \n",
    "        # produce embeddings for target images\n",
    "        gen_encode = self.g(target_image)## (16, 64)\n",
    "        encoded_images.append(gen_encode) ## 维度变成了(21, 16, 64)\n",
    "        output = torch.stack(encoded_images,dim=0) ## 把encoded_images合并成一个向量，形状(21, 16, 64)\n",
    "        \n",
    "        # if we are considering full-context embeddings\n",
    "        if self.f:\n",
    "            output = self.lstm(output)\n",
    "            \n",
    "        # get similarities between support set embeddings and target\n",
    "        similarites = self.c(\n",
    "            support_set=output[:-1], ## shape是(20,16,64)\n",
    "            input_image=output[-1] ## (16,64)\n",
    "        ) ## (16, 20)，这个similarities就代表了，input_image跟20个class的样本之间的相似程度。\n",
    "        \n",
    "        # produce predictions for target probabilities\n",
    "        preds = self.a(\n",
    "            similarites, ## (16, 20)\n",
    "            support_set_y=support_set_y_one_hot ## torch.Size([16, 20, 20])\n",
    "        ) ## （16，20）；这个preds相当于是给出一个判断，就是各个batch里面，input_image跟20个class里面的哪个是最像的。\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        values, indices = preds.max(1) ## 意思是，在axis=1上做max，然后values就是最大值，indices就是最大值对应的列号。\n",
    "        accuracy = torch.mean((indices.squeeze() == target_y).float()) ## 这个squeeze看样子没啥用啊。\n",
    "        ## 得到的accuracy意思就是一个batch里面，预测最像预测对了的比例。\n",
    "        crossentropy_loss = F.cross_entropy(preds, target_y.long())## 这个就是计算损失了。\n",
    "\n",
    "        return accuracy, crossentropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Create a Dataset Loader.\n",
    "For our case, as we are using Omniglot Dataset,it will create a Omnligloat builder which calls Matching Network, and run its epochs for training, testing, and validation purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(total_train_batches, name='train'):\n",
    "    \"\"\"\n",
    "    Run the training epoch\n",
    "    :param total_train_batches: Number of batches to train on\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    total_c_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    for i in range(int(total_train_batches)):\n",
    "        x_support_set, y_support_set, x_target, y_target = get_batch(name)\n",
    "        '''\n",
    "        (16, 20, 28, 28, 1)\n",
    "        (16, 20)\n",
    "        (16, 28, 28, 1)\n",
    "        (16, 1)\n",
    "        '''\n",
    "        x_support_set = Variable(torch.from_numpy(x_support_set)).float()\n",
    "        y_support_set = Variable(torch.from_numpy(y_support_set), requires_grad=False).long()\n",
    "        x_target = Variable(torch.from_numpy(x_target)).float()\n",
    "        y_target = Variable(torch.from_numpy(y_target), requires_grad=False).squeeze().long()\n",
    "        '''\n",
    "        (16, 20, 28, 28, 1)\n",
    "        (16, 20)\n",
    "        (16, 28, 28, 1)\n",
    "        (16, )\n",
    "        '''\n",
    "        # convert to one hot encoding\n",
    "        y_support_set = y_support_set.unsqueeze(2) ## (16, 20, 1)\n",
    "        sequence_length = y_support_set.size()[1] ## 20\n",
    "        batch_size = y_support_set.size()[0] ## 16\n",
    "        y_support_set_one_hot = Variable(\n",
    "            torch.zeros(batch_size, sequence_length, \n",
    "            classes_per_set).scatter_(2,y_support_set.data,1), requires_grad=False\n",
    "        ) ## torch.Size([16, 20, 20]) 这一步是转onehot。\n",
    "\n",
    "        # reshape channels and change order\n",
    "        size = x_support_set.size() ## (16, 20, 28, 28, 1)\n",
    "        x_support_set = x_support_set.permute(0, 1, 4, 2, 3) ## (16, 20, 1, 28, 28)\n",
    "        x_target = x_target.permute(0, 3, 1, 2) ## (16, 1, 28, 28, )\n",
    "        acc, c_loss = matchNet(x_support_set, y_support_set_one_hot, x_target, y_target)\n",
    "        if name == 'train':\n",
    "            # optimize process\n",
    "            optimizer.zero_grad()\n",
    "            c_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        iter_out = \"tr_loss: {}, tr_accuracy: {}\".format(c_loss, acc)\n",
    "        total_c_loss += c_loss\n",
    "        total_accuracy += acc\n",
    "\n",
    "    total_c_loss = total_c_loss / total_train_batches\n",
    "    total_accuracy = total_accuracy / total_train_batches\n",
    "    return total_c_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set-up Experiments variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "num_channels=1\n",
    "lr=1e-3\n",
    "image_size=28\n",
    "classes_per_set=20\n",
    "samples_per_class=1\n",
    "keep_prob=0.0\n",
    "fce=True\n",
    "optim=\"adam\"\n",
    "wd=0\n",
    "matchNet = MatchingNetwork(keep_prob, batch_size, num_channels, lr, fce, classes_per_set,\n",
    "                                samples_per_class, image_size)\n",
    "total_iter = 0\n",
    "total_train_iter = 0\n",
    "optimizer = torch.optim.Adam(matchNet.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',verbose=True)\n",
    "# Training setup\n",
    "total_epochs = 20\n",
    "total_train_batches = 100\n",
    "total_val_batches = 10\n",
    "total_test_batches = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's Run Experiments !!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss:2.89 train_accuracy:0.40 valid_loss:2.82 valid_accuracy:0.53\n",
      "Epoch 1: train_loss:2.77 train_accuracy:0.57 valid_loss:2.76 valid_accuracy:0.59\n",
      "Epoch 2: train_loss:2.71 train_accuracy:0.67 valid_loss:2.68 valid_accuracy:0.68\n",
      "Epoch 3: train_loss:2.68 train_accuracy:0.72 valid_loss:2.69 valid_accuracy:0.66\n",
      "Epoch 4: train_loss:2.65 train_accuracy:0.73 valid_loss:2.66 valid_accuracy:0.70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rc/5v8dff5j52303vp04pm0xz6c0000gn/T/ipykernel_40174/156365976.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m############################### Training Step ##########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtotal_c_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_train_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_c_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rc/5v8dff5j52303vp04pm0xz6c0000gn/T/ipykernel_40174/526324948.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(total_train_batches, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx_support_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_support_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## (16, 20, 1, 28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## (16, 1, 28, 28, )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatchNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_support_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_support_set_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# optimize process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rc/5v8dff5j52303vp04pm0xz6c0000gn/T/ipykernel_40174/4042056711.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, support_set_images, support_set_y_one_hot, target_image, target_y)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mencoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_set_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mgen_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_set_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mencoded_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_encode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rc/5v8dff5j52303vp04pm0xz6c0000gn/T/ipykernel_40174/3283668012.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_input)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 443\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss,train_accuracy=[],[]\n",
    "val_loss,val_accuracy=[],[]\n",
    "test_loss,test_accuracy=[],[]\n",
    "\n",
    "\n",
    "for e in range(total_epochs):\n",
    "    ############################### Training Step ##########################################\n",
    "    total_c_loss, total_accuracy = run_epoch(total_train_batches,'train')\n",
    "    train_loss.append(total_c_loss)\n",
    "    train_accuracy.append(total_accuracy)\n",
    "    \n",
    "    ################################# Validation Step #######################################\n",
    "    total_val_c_loss, total_val_accuracy = run_epoch(total_val_batches, 'val')\n",
    "    val_loss.append(total_val_c_loss)\n",
    "    val_accuracy.append(total_val_accuracy)\n",
    "    print(\"Epoch {}: train_loss:{:.2f} train_accuracy:{:.2f} valid_loss:{:.2f} valid_accuracy:{:.2f}\".\n",
    "          format(e, total_c_loss, total_accuracy, total_val_c_loss, total_val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's obtain our test accuracy by running the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_c_loss, total_test_accuracy = run_epoch(total_test_batches,'test')\n",
    "print(\"test_accuracy:{}%\".format(total_test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: Let's visualize our results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train,val,name1=\"train_loss\",name2=\"val_loss\",title=\"\"):\n",
    "    plt.title(title)\n",
    "    plt.plot(train, label=name1)\n",
    "    plt.plot(val, label=name2)\n",
    "    plt.legend()\n",
    "\n",
    "plot_loss(train_loss,val_loss,\"train_loss\",\"val_loss\",\"Loss Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_accuracy,val_accuracy,\"train_accuracy\",\"val_accuracy\",\"Accuracy Graph\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
