{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c38299",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 预处理数据\n",
    "\n",
    "原来参考的代码，里面的scipy.misc版本太旧，所以这里我查了一下，相当于是放弃了scipy.misc库，改用别的库来弄：\n",
    "https://blog.csdn.net/huang1024rui/article/details/119668502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9203f144",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "import os\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "\n",
    "dataset = []\n",
    "examples = []\n",
    "# images_background\n",
    "data_root = \"../../originalDataset/omniglot-master/python/\"\n",
    "alphabets = os.listdir(data_root + \"images_background\")\n",
    "for alphabet in alphabets:\n",
    "    characters = os.listdir(os.path.join(data_root, \"images_background\", alphabet))\n",
    "    for character in characters:\n",
    "        files = os.listdir(os.path.join(data_root, \"images_background\", alphabet, character))\n",
    "        examples = []\n",
    "        for img_file in files:\n",
    "            img = resize(\n",
    "                imageio.imread(os.path.join(data_root, \"images_background\", alphabet, character, img_file)), [28, 28]\n",
    "            )\n",
    "            examples.append(img) # example level append, making dim as 20X28X28\n",
    "        dataset.append(examples) # character level append, making dim as (20*30)X20X28X28\n",
    "\n",
    "# images_evaluation\n",
    "data_root = \"../../originalDataset/omniglot-master/python/\"\n",
    "alphabets = os.listdir(data_root + \"images_evaluation\")\n",
    "for alphabet in alphabets:\n",
    "    characters = os.listdir(os.path.join(data_root, \"images_evaluation\", alphabet))\n",
    "    for character in characters:\n",
    "        files = os.listdir(os.path.join(data_root, \"images_evaluation\", alphabet, character))\n",
    "        examples = []\n",
    "        for img_file in files:\n",
    "            img = resize(\n",
    "                imageio.imread(os.path.join(data_root, \"images_evaluation\", alphabet, character, img_file)), [28, 28]\n",
    "            )\n",
    "            examples.append(img)\n",
    "        dataset.append(examples) # character level append, making dim as (n)X20X28X28\n",
    "\n",
    "np.save(data_root + \"dataset.npy\", np.asarray(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc2160",
   "metadata": {},
   "source": [
    "# 跑一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654bae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98824695",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # setting batch_size\n",
    "classes_per_set = 20 # Number of classes per set\n",
    "samples_per_class = 1 # as we are choosing it to be one shot learning, so we have 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a51a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1623, 20, 28, 28)\n",
      "<class 'numpy.ndarray'>\n",
      "(1200, 20, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.load('../../originalDataset/omniglot-master/python/dataset.npy') # Load Data\n",
    "print (x.shape)\n",
    "print (type(x))\n",
    "n_classes = x.shape[0] # total number of classes\n",
    "x = np.reshape(x, newshape=(x.shape[0], x.shape[1], 28, 28, 1)) # expand dimension from (x.shape[0],x.shape[1],28,28)\n",
    "np.random.shuffle(x) # shuffle dataset\n",
    "x_train, x_val, x_test = x[:1200], x[1200:1411], x[1411:] # divide dataset in to train, val,ctest\n",
    "\n",
    "print (x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413ab5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processes_batch(data, mu, sigma):\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "# Normalize Dataset\n",
    "x_train = processes_batch(x_train, np.mean(x_train), np.std(x_train))\n",
    "x_val = processes_batch(x_val, np.mean(x_val), np.std(x_val))\n",
    "x_test = processes_batch(x_test, np.mean(x_test), np.std(x_test))\n",
    "\n",
    "# Defining dictionary of dataset\n",
    "datatset = {\"train\": x_train, \"val\": x_val, \"test\": x_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7050957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(data):\n",
    "    \"\"\"\n",
    "    Generates sample batch \n",
    "    :param : data - one of(train,test,val) our current dataset shape [total_classes,20,28,28,1]\n",
    "    :return: [support_set_x,support_set_y,target_x,target_y] for Matching Networks\n",
    "\n",
    "    每一个batch，都重复做batch_size次这样的操作：\n",
    "    1. 从一千多个class里面选择20个，是support_set_x。每个类都只有1个图片，每个图片是28*28*1。\n",
    "    2. 这20个class对应的support_set_y就是0～19.\n",
    "    3. 从这20个class里面选择出1个作为target_x, target_y。\n",
    "    嗯差不多就是这样。\n",
    "    \"\"\"\n",
    "    support_set_x = np.zeros((batch_size, classes_per_set, samples_per_class, data.shape[2],\n",
    "                              data.shape[3], data.shape[4]), np.float32)\n",
    "    support_set_y = np.zeros((batch_size, classes_per_set, samples_per_class), np.int32)\n",
    "\n",
    "    target_x = np.zeros((batch_size, data.shape[2], data.shape[3], data.shape[4]), np.float32)\n",
    "    target_y = np.zeros((batch_size, 1), np.int32)\n",
    "    for i in range(batch_size):\n",
    "        choose_classes = np.random.choice(data.shape[0], size=classes_per_set, replace=False) # choosing random classes\n",
    "        choose_label = np.random.choice(classes_per_set, size=1) # label set\n",
    "        choose_samples = np.random.choice(data.shape[1], size=samples_per_class + 1, replace=False)\n",
    "        x_temp = data[choose_classes] # choosing classes\n",
    "        x_temp = x_temp[:, choose_samples] # choosing sample batch from classes chosen outputs (20X2X28X28X1)\n",
    "        y_temp = np.arange(classes_per_set) # will return [0,1,2,3,...,19]\n",
    "        support_set_x[i] = x_temp[:, :-1] ## (20X前面那1个X28*28*1)\n",
    "        support_set_y[i] = np.expand_dims(y_temp[:], axis=1) # expand dimension ## shape will be (20,1)\n",
    "        target_x[i] = x_temp[choose_label, -1] ## (1X后面那1个X28X28X1)\n",
    "        target_y[i] = y_temp[choose_label] ## shape = (1,)\n",
    "    return support_set_x, support_set_y, target_x, target_y # returns support of [batch_size, 20 classes per set, 1 sample, 28, 28,1]\n",
    "    \n",
    "def get_batch(dataset_name):\n",
    "    \"\"\"\n",
    "    gen batch while training\n",
    "    :param dataset_name: The name of dataset(one of \"train\",\"val\",\"test\")\n",
    "    :return: a batch images\n",
    "\n",
    "    实际上本质上还是sample_batch操作，只不过多了一步，就是把某一个形状是1的维度消去罢了。\n",
    "    \"\"\"\n",
    "    support_set_x, support_set_y, target_x, target_y = sample_batch(datatset[dataset_name])\n",
    "    support_set_x = support_set_x.reshape((support_set_x.shape[0], support_set_x.shape[1] * support_set_x.shape[2],\n",
    "                                           support_set_x.shape[3], support_set_x.shape[4], support_set_x.shape[5]))\n",
    "    support_set_y = support_set_y.reshape(support_set_y.shape[0], support_set_y.shape[1] * support_set_y.shape[2])\n",
    "    return support_set_x, support_set_y, target_x, target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b105dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 20, 28, 28, 1)\n",
      "(16, 20)\n",
      "(16, 28, 28, 1)\n",
      "(16, 1)\n"
     ]
    }
   ],
   "source": [
    "test_batch = get_batch(\"train\")\n",
    "for i in range(4):\n",
    "    print(test_batch[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edeab558",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, y_support_set, x_target, y_target = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0fcf307",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set = Variable(torch.from_numpy(x_support_set)).float()\n",
    "y_support_set = Variable(torch.from_numpy(y_support_set), requires_grad=False).long()\n",
    "x_target = Variable(torch.from_numpy(x_target)).float()\n",
    "y_target = Variable(torch.from_numpy(y_target), requires_grad=False).squeeze().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12c054b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target.shape ## 看来，y_target相比于原来的y_target，被squeeze了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd8bff9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set = y_support_set.unsqueeze(2)\n",
    "y_support_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82bad44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 20])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = y_support_set.size()[1] ## 20\n",
    "batch_size = y_support_set.size()[0] ## 16\n",
    "y_support_set_one_hot = Variable(\n",
    "    torch.zeros(batch_size, sequence_length, \n",
    "                classes_per_set).scatter_(2,y_support_set.data,1), requires_grad=False)\n",
    "y_support_set_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28592db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 20])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "342b8452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f747b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "num_channels=1\n",
    "lr=1e-3\n",
    "image_size=28\n",
    "classes_per_set=20\n",
    "samples_per_class=1\n",
    "keep_prob=0.0\n",
    "fce=True\n",
    "optim=\"adam\"\n",
    "wd=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc3dd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convLayer(in_channels, out_channels, dropout_prob=0.0):\n",
    "    \"\"\"\n",
    "    :param dataset_name: The name of dataset(one of \"train\",\"val\",\"test\")\n",
    "    :return: a batch images\n",
    "    \"\"\"\n",
    "    cnn_seq = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Dropout(dropout_prob)\n",
    "    )\n",
    "    return cnn_seq\n",
    "\n",
    "class Embeddings_extractor(nn.Module):\n",
    "    def __init__(self, layer_size=64, num_channels=1, dropout_prob=0.5, image_size=28):\n",
    "        super(Embeddings_extractor, self).__init__()\n",
    "        \"\"\"\n",
    "        Build a CNN to produce embeddings\n",
    "        :param layer_size:64(default)\n",
    "        :param num_channels:\n",
    "        :param keep_prob:\n",
    "        :param image_size:\n",
    "        \"\"\"\n",
    "        self.layer1 = convLayer(num_channels, layer_size, dropout_prob)\n",
    "        self.layer2 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "        self.layer3 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "        self.layer4 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "\n",
    "        finalSize = int(math.floor(image_size / (2 * 2 * 2 * 2)))\n",
    "        self.outSize = finalSize * finalSize * layer_size\n",
    "\n",
    "    def forward(self, image_input):\n",
    "        \"\"\"\n",
    "        :param: Image\n",
    "        :return: embeddings\n",
    "        \"\"\"\n",
    "        x = self.layer1(image_input)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97ef05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ee = Embeddings_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2112c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 28, 28, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_support_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a37b2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set = x_support_set.permute(0, 1, 4, 2, 3) ## (16, 20, 1, 28, 28)\n",
    "x_target = x_target.permute(0, 3, 1, 2) ## (16, 1, 28, 28, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a480fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n",
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 64])\n"
     ]
    }
   ],
   "source": [
    "encoded_images = []\n",
    "for i in np.arange(x_support_set.size(1)):\n",
    "    input_img = x_support_set[:, i, :, :]\n",
    "    gen_encode = test_ee(input_img)\n",
    "    print(input_img.shape, gen_encode.shape)\n",
    "    encoded_images.append(gen_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ca6f492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 16, 64])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# produce embeddings for target images\n",
    "gen_encode = test_ee(x_target)## (16, 64)\n",
    "encoded_images.append(gen_encode) ## 维度变成了(21, 16, 64)\n",
    "output = torch.stack(encoded_images,dim=0) ## 把encoded_images合并成一个向量，形状(21, 16, 64)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f26b1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This model calculates the cosine distance between each of the support set embeddings and \n",
    "    the target image embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistanceNetwork, self).__init__()\n",
    "\n",
    "    def forward(self, support_set, input_image):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        :param support_set:the embeddings of the support set images.shape[sequence_length,batch_size,64]\n",
    "        :param input_image: the embedding of the target image,shape[batch_size,64]\n",
    "        :return:shape[batch_size,sequence_length]\n",
    "        \"\"\"\n",
    "        ## 输入的形状，我们分别设定为 (20, 16, 64) (16, 64)\n",
    "        eps = 1e-10\n",
    "        similarities = []\n",
    "        for support_image in support_set: ## support_image的形状(16,64)\n",
    "            sum_support = torch.sum(torch.pow(support_image, 2), 1)\n",
    "            print(sum_support.shape) ## (16,1)\n",
    "            support_manitude = sum_support.clamp(eps, float(\"inf\")).rsqrt()\n",
    "            dot_product = input_image.unsqueeze(1).bmm(support_image.unsqueeze(2)).squeeze()\n",
    "            cosine_similarity = dot_product * support_manitude\n",
    "            similarities.append(cosine_similarity)\n",
    "        similarities = torch.stack(similarities)\n",
    "        return similarities.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81086edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "self_c = DistanceNetwork()\n",
    "similarites = self_c(\n",
    "    support_set=output[:-1], ## shape是(20,16,64)\n",
    "    input_image=output[-1] ## (16,64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be92f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = similarites.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cec988e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 10, 18,  3, 10, 19,  5,  3, 18, 12,  8, 15,  9, 15,  2, 13])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e51e0c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 10, 18,  3, 10, 19,  5,  3, 18, 12,  8, 15,  9, 15,  2, 13])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2286034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
