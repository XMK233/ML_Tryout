{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ad2d9d",
   "metadata": {},
   "source": [
    "两个实现应该来讲可以交叉验证\n",
    "* https://github.com/ssw-nlp-study-group/nlp_study/blob/main/nlp/word2vec_negative_sampling.py\n",
    "* https://blog.csdn.net/qq_24668285/article/details/121754529\n",
    "\n",
    "通过交叉验证发现，forward部分的实现原理是一样的。但是到底为什么会是酱紫的，可以再看看原理。\n",
    "\n",
    "https://blog.csdn.net/csdn_xmj/article/details/118702420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028c3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- ecoding: utf-8 -*-\n",
    "# @Author: anyang\n",
    "# @Time: 2021/1/29 2:13 下午\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defef318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于word2vec的skip gram方法在单词数很大的情况下需要训练费用无法承受\n",
    "# 因此使用负样本采样的方法代替，节省训练时间，并且结果上也是近似 将一个多分类问题转换为多个二分类问题\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('../runs/word2vec_skip_gram_negative_sampling')\n",
    "\n",
    "# 判断是否有GPU\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('../runs/word2vec_skip_gram_negative_sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd737943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(data_file):\n",
    "    lines = []\n",
    "    with open(data_file, encoding=\"UTF-8\") as file:\n",
    "        for line in file:\n",
    "            lines.append(line.strip())\n",
    "\n",
    "    sentence = \" \".join(lines) ## 把原文档拼成一行。\n",
    "    words = sentence.split(\" \") ## 把原文里面所有的单词得到。感觉这种方法不太高明啊。\n",
    "    words_set = list(set(words)) \n",
    "\n",
    "    words_freq_num = Counter(words).most_common(len(words_set))\n",
    "    # print(words_freq_num)\n",
    "    words_freq_p = np.array([freq for word, freq in words_freq_num])\n",
    "    # print(words_freq_p)\n",
    "    words_freq_p = words_freq_p ** (3. / 4.) ## 这个3/4次方是论文里提到的方法。\n",
    "    # print(words_freq_p)\n",
    "    words_freq_p = words_freq_p / np.sum(words_freq_p)\n",
    "    # print(words_freq_p)\n",
    "\n",
    "    words2id = {w: i for i, w in enumerate(words_set)}\n",
    "    id2words = {i: w for i, w in enumerate(words_set)}\n",
    "\n",
    "    return lines, words2id, id2words, words_set, words, np.array(words_freq_num), np.array(words_freq_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46804ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines, words2id, id2words, words_set, words, words_freq_num, words_freq_p =\\\n",
    "get_words(data_file=f\"../originalDataset/negative_sampling_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bc5403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['橘子', '1'],\n",
       "       ['香蕉', '1'],\n",
       "       ['苹果', '1'],\n",
       "       ['梨子', '1'],\n",
       "       ['葡萄', '1'],\n",
       "       ['柠檬', '1'],\n",
       "       ['猪', '1'],\n",
       "       ['狗', '1'],\n",
       "       ['羊', '1'],\n",
       "       ['牛', '1'],\n",
       "       ['兔子', '1'],\n",
       "       ['老鼠', '1'],\n",
       "       ['马', '1'],\n",
       "       ['龙', '1'],\n",
       "       ['鸡', '1'],\n",
       "       ['电脑', '1'],\n",
       "       ['洗衣机', '1'],\n",
       "       ['冰箱', '1'],\n",
       "       ['音响', '1'],\n",
       "       ['手机', '1'],\n",
       "       ['收音机', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eab26f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.04761905])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b635b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skip_pairs(lines, context_size):\n",
    "    '''\n",
    "    说白了，这个函数就是要获得每一个单词的上下文单词。\n",
    "    '''\n",
    "    skip_grams = []\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        for i, w in enumerate(words):\n",
    "            context_words = words[max(i - context_size, 0):max(i, 0)] + words[i + 1:i + context_size + 1]\n",
    "            skip_grams.append((w, context_words))\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae528e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['橘子 香蕉 苹果 梨子 葡萄 柠檬', '猪 狗 羊 牛 兔子 老鼠 马 龙 鸡', '电脑 洗衣机 冰箱 音响 手机 收音机']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584e431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('橘子', ['香蕉', '苹果']),\n",
       " ('香蕉', ['橘子', '苹果', '梨子']),\n",
       " ('苹果', ['橘子', '香蕉', '梨子', '葡萄']),\n",
       " ('梨子', ['香蕉', '苹果', '葡萄', '柠檬']),\n",
       " ('葡萄', ['苹果', '梨子', '柠檬']),\n",
       " ('柠檬', ['梨子', '葡萄']),\n",
       " ('猪', ['狗', '羊']),\n",
       " ('狗', ['猪', '羊', '牛']),\n",
       " ('羊', ['猪', '狗', '牛', '兔子']),\n",
       " ('牛', ['狗', '羊', '兔子', '老鼠']),\n",
       " ('兔子', ['羊', '牛', '老鼠', '马']),\n",
       " ('老鼠', ['牛', '兔子', '马', '龙']),\n",
       " ('马', ['兔子', '老鼠', '龙', '鸡']),\n",
       " ('龙', ['老鼠', '马', '鸡']),\n",
       " ('鸡', ['马', '龙']),\n",
       " ('电脑', ['洗衣机', '冰箱']),\n",
       " ('洗衣机', ['电脑', '冰箱', '音响']),\n",
       " ('冰箱', ['电脑', '洗衣机', '音响', '手机']),\n",
       " ('音响', ['洗衣机', '冰箱', '手机', '收音机']),\n",
       " ('手机', ['冰箱', '音响', '收音机']),\n",
       " ('收音机', ['音响', '手机'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_skip_pairs(lines, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1120c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_embedding_dataset(tud.Dataset):\n",
    "    def __init__(self, skip_grams):\n",
    "        super(word_embedding_dataset, self).__init__()\n",
    "        self.skip_grams = skip_grams\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skip_grams)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.skip_grams[idx]\n",
    "\n",
    "\n",
    "def find_nearest_k(word, k):\n",
    "    '''\n",
    "    这个就很显然了。寻找跟word最像的k个单词。\n",
    "    '''\n",
    "    wid = words2id[word]\n",
    "    w_vec = wordvec[wid]\n",
    "\n",
    "    similarity = wordvec @ w_vec.T\n",
    "    sort = np.sort(similarity)[::-1]\n",
    "    sort_arg = np.argsort(similarity)[::-1]\n",
    "\n",
    "    result = []\n",
    "    for i in sort_arg:\n",
    "        result.append(id2words[i])\n",
    "\n",
    "    print(\"与 %s 相似度排序\" % word, result)\n",
    "\n",
    "    return result[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd398f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding_model(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(embedding_model, self).__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        init_range = 0.5 / self.emb_size\n",
    "        self.in_embed = nn.Embedding(num_embeddings=self.voc_size, embedding_dim=emb_size)\n",
    "        self.in_embed.weight.data.uniform_(-init_range, init_range) ## 我怀疑这个的意思是按照某种分布（uniform），生成某范围内的数值来初始化参数\n",
    "        self.out_embed = nn.Embedding(num_embeddings=self.voc_size, embedding_dim=emb_size)\n",
    "        self.out_embed.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        输入参数名字里面的label我总觉得有一些误导性，实际上应该是index吧。\n",
    "        '''\n",
    "        # shape (1,embedding_size)\n",
    "        input_embedding = self.in_embed(input_labels)\n",
    "        # shape (context_size,embedding_size)\n",
    "        pos_embedding = self.out_embed(pos_labels)\n",
    "        neg_embedding = self.out_embed(neg_labels)\n",
    "\n",
    "        input_embedding = input_embedding.unsqueeze(2) ## 变成(1, embedding_size, 1)\n",
    "        # print(\"input_embedding: \", input_embedding.size())\n",
    "        pos_embedding = pos_embedding.unsqueeze(0) ## 变成(1, context_size, embedding_size)\n",
    "        # print(\"pos_embedding: \", pos_embedding.size())\n",
    "        neg_embedding = neg_embedding.unsqueeze(0)\n",
    "\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding) ## (1, context_size, 1)\n",
    "        # print(\"pos_dot: \", pos_dot.size())\n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding)\n",
    "\n",
    "        # 正样本\n",
    "        log_pos = torch.sigmoid(pos_dot).sum(1) ## (1, 1)\n",
    "        # print(\"torch.sigmoid(pos_dot).sum(1)\", torch.sigmoid(pos_dot).sum(1))\n",
    "        # 负样本\n",
    "        log_neg = torch.sigmoid(neg_dot).sum(1)\n",
    "\n",
    "        loss = (-log_pos - log_neg).squeeze() ## 把两个部分的logit加起来，取相反数。我怎么觉得这里不太对劲，难道不应该是-log_pos + log_neg吗？\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0bb2a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.502588987350464\n",
      "-3.999926805496216\n",
      "-3.506493091583252\n",
      "-4.571134090423584\n",
      "-4.626928329467773\n",
      "-4.6111159324646\n",
      "-3.9755728244781494\n",
      "-4.919957637786865\n",
      "-4.0158538818359375\n",
      "-6.339545249938965\n",
      "-6.094932556152344\n",
      "-8.249215126037598\n",
      "-6.048399448394775\n",
      "-6.589468479156494\n",
      "-4.862904071807861\n",
      "-5.799762725830078\n",
      "-6.878992080688477\n",
      "in_embed.weight Parameter containing:\n",
      "tensor([[-0.5439,  1.7315,  1.1067,  0.3626,  1.0088, -0.3547, -0.4186,  1.0471],\n",
      "        [ 0.8736, -0.6747, -1.9054, -1.0974,  0.4093, -1.2663,  0.7302, -0.4865],\n",
      "        [-0.0171,  1.2406,  1.2650,  0.7522,  0.5946, -0.8517, -0.5177,  1.0918],\n",
      "        [ 1.1086,  0.2772, -1.4757, -1.1954,  0.3241, -0.7477,  0.9878, -0.8893],\n",
      "        [ 0.3647,  1.0288,  1.4437,  0.5225,  0.4326, -1.0316, -0.4966,  0.2638],\n",
      "        [ 0.2258,  1.2009,  1.6470,  0.5341,  0.9298, -0.9575, -0.6354,  0.6581],\n",
      "        [ 0.1586,  1.2078,  1.5422,  0.6504,  0.8917, -0.8086, -0.6243,  1.0452],\n",
      "        [-0.7561, -0.5491,  0.6392,  0.2798, -1.5445,  1.5659, -0.2726, -1.6793],\n",
      "        [-0.6552, -0.5599,  0.7384,  0.1214, -1.5437,  1.2871, -0.2389, -1.5783],\n",
      "        [ 1.0648, -0.0721, -1.3757, -1.0541,  0.2776, -1.0047,  0.5643, -0.8733],\n",
      "        [ 0.7966,  0.0668, -1.7559, -1.1073,  0.3176, -0.8225,  1.4427, -0.6169],\n",
      "        [-0.3584, -0.5521,  0.5017,  0.5365, -1.6107,  1.7417, -0.2180, -1.3927],\n",
      "        [-0.2874, -0.6825,  0.7244,  0.3715, -1.4992,  1.4702, -0.2357, -1.4452],\n",
      "        [-0.5043, -0.3243,  0.5898,  0.2739, -1.2821,  1.3355, -0.2383, -1.6687],\n",
      "        [ 0.9829, -0.3935, -1.5477, -1.2506,  0.3819, -1.0791,  0.7180, -0.6673],\n",
      "        [ 0.4373,  0.0565, -1.1704, -1.0399,  0.2928, -0.7812,  1.4245, -0.6807],\n",
      "        [-0.3928, -0.3340,  0.5696,  0.3019, -1.5561,  1.2808, -0.0289, -1.3421],\n",
      "        [-0.2695,  1.5734,  1.0649,  0.3121,  0.8642, -0.3425, -0.4027,  0.6733],\n",
      "        [-0.2859,  1.4863,  1.2637,  0.7959,  0.7659, -0.8095, -0.2428,  1.0950],\n",
      "        [-0.2190,  1.3814,  1.4045,  0.2784,  1.3851, -0.3375, -0.7452,  0.9778],\n",
      "        [ 0.1691,  1.0639,  1.5128,  0.4125,  1.0419, -0.6262, -0.6674,  1.0178]],\n",
      "       requires_grad=True)\n",
      "out_embed.weight Parameter containing:\n",
      "tensor([[ 8.0002e-02,  6.5314e-01,  1.4067e+00,  5.9238e-01,  1.1955e+00,\n",
      "         -5.7828e-01, -7.9770e-01,  1.4610e+00],\n",
      "        [ 6.0794e-01, -4.7636e-01, -1.9426e+00, -1.2245e+00,  4.6243e-01,\n",
      "         -6.1108e-01,  1.3319e+00, -4.6695e-02],\n",
      "        [ 6.2087e-03,  7.8009e-01,  1.2853e+00,  8.4637e-01,  7.4919e-01,\n",
      "         -9.2424e-01, -5.7557e-01,  1.2756e+00],\n",
      "        [ 7.3583e-01, -8.0583e-01, -2.0868e+00, -9.2456e-01,  4.2376e-01,\n",
      "         -8.3886e-01,  7.0827e-01,  1.0364e-01],\n",
      "        [-3.9142e-01,  4.3384e-01,  4.9028e-01,  8.1725e-01,  4.5928e-01,\n",
      "         -4.0373e-01, -3.1355e-01,  1.6340e+00],\n",
      "        [-4.0523e-01,  1.0219e+00,  9.1418e-01,  7.6264e-01,  8.3882e-01,\n",
      "         -5.2896e-01, -4.7651e-01,  1.6874e+00],\n",
      "        [-5.1362e-01,  1.1791e+00,  9.8906e-01,  5.3023e-01,  1.1732e+00,\n",
      "         -3.7019e-01, -5.4110e-01,  1.4224e+00],\n",
      "        [-6.2537e-01, -1.1323e+00,  3.0404e-01,  4.8474e-01, -1.6590e+00,\n",
      "          1.7260e+00, -2.8168e-01, -7.2060e-01],\n",
      "        [-7.2621e-01, -1.0416e+00,  1.3377e-01,  5.2221e-01, -1.5266e+00,\n",
      "          1.6764e+00, -2.3299e-01, -6.3591e-01],\n",
      "        [ 6.9083e-01, -3.8002e-01, -1.9264e+00, -8.5207e-01,  3.4742e-01,\n",
      "         -4.2766e-01,  9.9464e-01,  1.2776e-01],\n",
      "        [ 1.0365e+00, -8.8325e-01, -2.0773e+00, -1.1315e+00,  4.5248e-01,\n",
      "         -9.4968e-01,  4.3599e-01, -9.6957e-02],\n",
      "        [-8.6485e-01, -1.1575e+00,  4.5492e-01,  3.4800e-01, -1.4935e+00,\n",
      "          1.6943e+00, -3.8870e-01, -1.0892e+00],\n",
      "        [-7.8093e-01, -9.5709e-01,  9.9576e-02,  5.5228e-01, -1.3409e+00,\n",
      "          1.8136e+00, -3.6466e-01, -8.9321e-01],\n",
      "        [-4.1123e-01, -1.2072e+00,  1.0512e-01,  6.1686e-01, -1.2373e+00,\n",
      "          1.6862e+00, -3.3434e-01, -4.0849e-01],\n",
      "        [ 2.0114e-01, -8.6865e-01, -1.9381e+00, -8.0969e-01,  3.6708e-01,\n",
      "         -6.8412e-01,  1.2269e+00,  2.8602e-01],\n",
      "        [ 4.7012e-01, -1.2866e+00, -1.8387e+00, -7.7112e-01,  3.8297e-01,\n",
      "         -7.4571e-01,  3.1065e-01,  3.4973e-01],\n",
      "        [-9.4417e-01, -1.1339e+00,  2.5630e-01,  3.0984e-01, -1.3386e+00,\n",
      "          1.5125e+00, -3.8443e-01, -7.1792e-01],\n",
      "        [-1.8278e-01,  3.7172e-01,  8.8288e-01,  3.8875e-01,  1.0752e+00,\n",
      "         -1.6433e-01, -7.4867e-01,  1.5072e+00],\n",
      "        [ 1.5397e-01,  7.4729e-01,  1.4374e+00,  8.0455e-01,  7.8098e-01,\n",
      "         -9.3358e-01, -8.9022e-01,  1.4089e+00],\n",
      "        [-3.2055e-01,  1.1185e+00,  1.0670e+00,  6.4764e-01,  1.0154e+00,\n",
      "         -3.8963e-01, -6.0511e-01,  1.6846e+00],\n",
      "        [-6.8659e-01,  1.1474e+00,  7.3407e-01,  3.3651e-01,  1.1743e+00,\n",
      "          1.0261e-03, -6.3883e-01,  1.5076e+00]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 22855 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 20598 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 29275 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 33529 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 26524 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 40857 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 33889 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 33796 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 40481 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 32769 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 40736 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 20820 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 23376 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 38899 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 21709 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 25163 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 26426 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 26592 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 27308 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 26792 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 20912 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 31665 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 27927 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 34915 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 30005 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 33041 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 39321 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 34121 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 27224 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 25910 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 29482 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 39532 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 32650 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 29399 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 22855 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 20598 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 29275 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 33529 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 26524 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 40857 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 33889 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 33796 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 40481 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 32769 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 40736 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 20820 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 23376 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 38899 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 21709 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 25163 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 26426 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 26592 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 27308 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 26792 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 20912 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 31665 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 27927 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 34915 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 30005 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 33041 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 39321 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 34121 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 27224 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 25910 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 29482 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 39532 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 32650 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/minkexiu/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 29399 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与 猪 相似度排序 ['羊', '牛', '马', '兔子', '老鼠', '狗', '龙', '猪', '鸡', '橘子', '电脑', '葡萄', '收音机', '柠檬', '手机', '梨子', '音响', '洗衣机', '冰箱', '香蕉', '苹果']\n",
      "['羊', '牛', '马', '兔子']\n"
     ]
    }
   ],
   "source": [
    "def words2id_func(words):\n",
    "    return np.array([words2id[w] for w in words])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_file = \"../originalDataset/negative_sampling_data.txt\"\n",
    "    model_file = \"../trainedModel/word2vec_negative_sampling.pkl\"\n",
    "    # 设定超参数（hyper parameters）\n",
    "    # 负采样个数k\n",
    "    k = 5\n",
    "    embedding_size = 8\n",
    "    context_size = 2\n",
    "    lr = 1e-2\n",
    "    num_epoch = 800\n",
    "\n",
    "    # 数据处理\n",
    "    lines, words2id, id2words, words_set, words, words_freq_num, words_freq_p = \\\n",
    "        get_words(data_file=data_file)\n",
    "\n",
    "    # 不重复单词个数\n",
    "    voc_size = len(words_set)\n",
    "\n",
    "    # 根据文本获取 训练数据 pairs [(input,output),...]\n",
    "    skip_grams = get_skip_pairs(lines, context_size=2)\n",
    "    # print(skip_grams)\n",
    "\n",
    "    model = embedding_model(voc_size=voc_size, emb_size=embedding_size)\n",
    "\n",
    "    dataset = word_embedding_dataset(skip_grams=skip_grams)\n",
    "\n",
    "    # indexs = torch.multinomial(torch.Tensor(words_freq_p), k, replacement=True) ## 应该是按照多项式分布，有放回的，采样k个。\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    word_freq_indexs = {} ## 把前面得到的words_freq_num这个list里面的word的排列顺序给它搞出来。为的是后续能够从word_freq_p里面拿到对应的p\n",
    "    for i, (word, freq) in enumerate(words_freq_num): ## \n",
    "        word_freq_indexs[word] = i\n",
    "\n",
    "    index = 0\n",
    "    # 词向量训练部分\n",
    "    for j in range(num_epoch):\n",
    "        for i, skip_gram in enumerate(dataset): ## 这里就可以看作是一行一行地扫描原始数据了。\n",
    "            center_word = words2id[skip_gram[0]] ## 中心词的id\n",
    "            context_words_id = [words2id[word] for word in skip_gram[1]] ## 上下文词的id\n",
    "            context_words = skip_gram[1] ## 上下文词\n",
    "            p = words_freq_p.copy()\n",
    "\n",
    "            for context in context_words:\n",
    "                p[word_freq_indexs[context]] = 0 ## 把上下文词对应的freq_p替换为0。我猜是为了不取这些数？\n",
    "\n",
    "            # 对负样本-即非 target的部分按照频率归一化后的概率进行采样\n",
    "            # 正样本部分保持不变\n",
    "            # 取样个数取决于k值\n",
    "            neg_words_sample = torch.multinomial(torch.Tensor(p), k, replacement=True)\n",
    "#             print(neg_words_sample, \"neg_words_sample\")\n",
    "            neg_words = words2id_func(words_freq_num[neg_words_sample.numpy()][:, 0])\n",
    "#             print(\n",
    "#                 words_freq_num[neg_words_sample.numpy()][:, 0], \"words_freq_num\"\n",
    "#             )\n",
    "#             print(neg_words, \"neg_words\")\n",
    "            ##### 上面这几行是做什么的呢？是为了采样一些单词，这些单词不是context words，所以算是负样本。\n",
    "            ##### 采样出来的neg_words就是若干个非context_words的id。\n",
    "            \n",
    "            context_words = words2id_func(context_words)\n",
    "            ## 这行，就是得到了context words的id。\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             print(torch.LongTensor([center_word]), torch.LongTensor(context_words), torch.LongTensor(neg_words))\n",
    "            loss = model(torch.LongTensor([center_word]), torch.LongTensor(context_words), torch.LongTensor(neg_words))\n",
    "            if index % 1000 == 0:\n",
    "                print(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"loss\", loss.item(), index)\n",
    "            index += 1\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model, model_file)\n",
    "\n",
    "    model = torch.load(model_file)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param)\n",
    "        # 获取in_embed的参数作为词向量 out_embed可舍弃\n",
    "        if name == \"in_embed.weight\":\n",
    "            wordvec = param.data.numpy()\n",
    "\n",
    "    # 利用pca对词向量进行降维 方便画图展示\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(wordvec)\n",
    "    plt.scatter(result[:, 0], result[:, 1])\n",
    "#     print(id2words)\n",
    "#     print(words_set)\n",
    "\n",
    "    for i in range(voc_size):\n",
    "        plt.annotate(id2words[i], xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "    plt.xlabel('奇数')\n",
    "    plt.ylabel('偶数')\n",
    "    plt.show()\n",
    "\n",
    "    # 获取最相近的k个词向量\n",
    "    result = find_nearest_k('猪', k=4)\n",
    "    print(result) ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a6b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
