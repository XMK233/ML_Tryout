{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161b3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import os, time\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "import tqdm, re, pickle, gc, os, time, random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50410b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_trained_model_v2(model_name, feas_file_path, oot_append, predict_data_batch = 5): \n",
    "    '''\n",
    "    model_name: str of the model's name. \n",
    "    oot_append: dataframe.\n",
    "    predict_data_batch: how many batches the oot_append will be splitted. \n",
    "    '''\n",
    "    model = xgb.Booster({'nthread': 4}) ## ns: not strict labeled\n",
    "    model.load_model(f'trained_models/{model_name}')\n",
    "    \n",
    "    with open(feas_file_path, \"r\") as f:\n",
    "        feas_cols_v2 = [i.strip() for i in f.readlines()]\n",
    "    \n",
    "    print(f\"How many features are used? \", len(feas_cols_v2))\n",
    "    \n",
    "    n_batches = predict_data_batch\n",
    "    nrow_interval = oot_append.shape[0] // n_batches - 5 ## 把原来的数据分成5份，然后逐份来做预测。否则实在太久。\n",
    "\n",
    "    ns_prob_col = None\n",
    "\n",
    "    for i in tqdm.tqdm(range(n_batches + 1)):\n",
    "        if i == n_batches:\n",
    "            partial_data_to_be_predict = oot_append.loc[i*nrow_interval:, feas_cols_v2]\n",
    "        else:\n",
    "            partial_data_to_be_predict = oot_append.loc[i*nrow_interval: (i+1)*nrow_interval - 1, feas_cols_v2] ## 用loc来切片，“行”这一维度，竟然是上下限都保留！！\n",
    "        ##################### \n",
    "        if ns_prob_col is None:\n",
    "            ns_prob_col = model.predict(xgb.DMatrix(partial_data_to_be_predict[feas_cols_v2].values))\n",
    "        else:\n",
    "            ns_prob_col = np.hstack(\n",
    "                [\n",
    "                    ns_prob_col, \n",
    "                    model.predict(xgb.DMatrix(partial_data_to_be_predict[feas_cols_v2].values))\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "    return ns_prob_col\n",
    "\n",
    "def loadTheData_doThePrediction_v2(model_name, feas_file_path, df, score_df_path, predict_data_batch = 5, score_col_name = \"score\", id_col_name = \"trace_id\"):\n",
    "    scores = predict_with_trained_model_v2(\n",
    "        model_name, \n",
    "        feas_file_path, \n",
    "        df, \n",
    "        predict_data_batch\n",
    "    )\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            id_col_name: df[id_col_name],\n",
    "            score_col_name: scores,\n",
    "        }\n",
    "    ).to_csv(score_df_path, index=False)\n",
    "    print(f\"{model_name} prediction done!\")\n",
    "    \n",
    "def get_all_feas(\n",
    "    feas_file_name,\n",
    "    feas_data_fname,\n",
    "    feas_file_dir = f\"/data/private/public_data/cpu1/minkexiu/thirdParty_feas_list/\",\n",
    "    feas_data_dir = f\"/data/private/public_data/cpu1/minkexiu/parquet_data/\",\n",
    "    left_on = \"trace_id\", \n",
    "    right_on = \"traceid\",\n",
    "):\n",
    "    '''\n",
    "    把前面的代码合集一下，放到这里。\n",
    "    '''\n",
    "    feas_file_path = os.path.join(feas_file_dir, feas_file_name)\n",
    "    with open(feas_file_path, \"r\") as f:\n",
    "        feas_cols_v2_upper = [i.strip() for i in f.readlines()]\n",
    "    feas_cols_v2 = [_.lower() for _ in feas_cols_v2_upper]\n",
    "    feas_data_path = os.path.join(feas_data_dir, feas_data_fname)\n",
    "    df = pd.read_parquet(feas_data_path)\n",
    "    \n",
    "    print(\"here i am\")\n",
    "    label_data = pd.read_csv(\n",
    "        # os.path.join(feas_data_dir, f\"trace_ids-target_1d.csv\")\n",
    "        f\"/data/private/public_data/cpu1/minkexiu/trace_id-info-202201_202204.csv\"\n",
    "    )\n",
    "    print(\"merging the data...\")\n",
    "    all_feas = label_data.merge(df, how='left', left_on=left_on, right_on = right_on)\n",
    "    \n",
    "    del df, label_data\n",
    "    gc.collect()\n",
    "    \n",
    "    return all_feas, feas_cols_v2_upper, feas_cols_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37228074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tmpList(nm, actual_nFeas, itv, n_iter=11):\n",
    "    l = [nm.rsplit(\"-\", 1)[-1] + \"-origin\"]\n",
    "    for j in range(n_iter):\n",
    "        i = actual_nFeas-j*itv\n",
    "        l.append(i)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c097a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = \"pre1\"\n",
    "target_label = \"isDefault\"\n",
    "userid_col = \"id\" ## loan_account_id\n",
    "traceid_col = \"issueDate\" ## trace_id\n",
    "n_fold = 5\n",
    "NEED_TO_PREPARE_DATA = False ## 用这个标记一下，是否需要重新准备数据\n",
    "NEED_TO_TRAIN = False ## 用这个标记一下，是否需要重新准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c90c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"preprocessedDataset/{nm}.txt\", \"r\") as f:\n",
    "    feas_cols_v2 = [i.strip() for i in f.readlines()]\n",
    "len(feas_cols_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac8f74",
   "metadata": {},
   "source": [
    "# 得到评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea8fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** getting prediction stores id residual is 0......\n",
      "How many features are used?  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 21/21 [00:00<00:00, 157.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre1-IDResd_0 prediction done!\n",
      "****************************** getting prediction stores id residual is 1......\n",
      "How many features are used?  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 21/21 [00:00<00:00, 143.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre1-IDResd_1 prediction done!\n",
      "****************************** getting prediction stores id residual is 2......\n",
      "How many features are used?  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 21/21 [00:00<00:00, 155.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre1-IDResd_2 prediction done!\n",
      "****************************** getting prediction stores id residual is 3......\n",
      "How many features are used?  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 21/21 [00:00<00:00, 186.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre1-IDResd_3 prediction done!\n",
      "****************************** getting prediction stores id residual is 4......\n",
      "How many features are used?  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 21/21 [00:00<00:00, 221.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre1-IDResd_4 prediction done!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"pred_scores/\"):\n",
    "    os.makedirs(\"pred_scores/\")\n",
    "\n",
    "for i in range(n_fold):\n",
    "    print(\"*\"*30, f\"getting prediction stores id residual is {i}......\")\n",
    "    all_feas_pred = pq.read_table(\n",
    "        f\"preprocessedDataset/{nm}-IDResd_{i}.parquet\",\n",
    "        columns=feas_cols_v2 + [traceid_col]\n",
    "    ).to_pandas()\n",
    "    \n",
    "    tmp_name = f\"{nm}-IDResd_{i}\"\n",
    "    loadTheData_doThePrediction_v2(\n",
    "        model_name = f\"{tmp_name}\",\n",
    "        feas_file_path = f\"preprocessedDataset/{nm}.txt\",\n",
    "        df = all_feas_pred, \n",
    "        score_df_path = f\"pred_scores/{tmp_name}.csv\", \n",
    "        predict_data_batch = 20,\n",
    "        score_col_name=f\"{nm}--score\",\n",
    "        id_col_name = traceid_col,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836085e2",
   "metadata": {},
   "source": [
    "# 把评分拼起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e8713ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre1-IDResd_0\n",
      "pre1-IDResd_1\n",
      "pre1-IDResd_2\n",
      "pre1-IDResd_3\n",
      "pre1-IDResd_4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = None\n",
    "for i in range(n_fold):\n",
    "    tmp_name = f\"{nm}-IDResd_{i}\"\n",
    "    print(tmp_name)\n",
    "    if scores is None:\n",
    "        scores = pd.read_csv(f\"pred_scores/{tmp_name}.csv\")\n",
    "    else:\n",
    "        scores = pd.concat([scores, pd.read_csv(f\"pred_scores/{tmp_name}.csv\")])\n",
    "scores.to_csv(f\"pred_scores/{nm}-maidianScore.csv\", index=False)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87eb491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
