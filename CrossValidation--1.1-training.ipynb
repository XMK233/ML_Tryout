{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d95465",
   "metadata": {},
   "source": [
    "还有一个问题：用户id是用来划分数据的，但是trace_id才是一一对应到原来的训练数据的诶。\n",
    "\n",
    "划分完之后的数据，其实没必要再带上用户id了，还是得带上trace_id更重要一些。\n",
    "\n",
    "以及，之前我从来都是丢掉用户id的。所以在处理实际上的原始数据的时候，我还得去哪里把用户id找回来才行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cba075a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1390\n",
      "1190\n",
      "990\n",
      "790\n",
      "590\n",
      "390\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "itv = 200\n",
    "for i in range(7):\n",
    "    print(1390 - i*itv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df17da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** 1390\n",
      "****************************** 1190\n",
      "****************************** 990\n",
      "****************************** 790\n",
      "****************************** 590\n",
      "****************************** 390\n",
      "****************************** 190\n"
     ]
    }
   ],
   "source": [
    "sorted_features = [i for i in range(1390)]\n",
    "itv = 200\n",
    "for i in range(7):\n",
    "    if i == 0:\n",
    "        test_feas = sorted_features\n",
    "    else:\n",
    "        test_feas = sorted_features[:-i*itv]\n",
    "    \n",
    "    ###########  \n",
    "    if os.path.exists(\"trained_models/\"+nm + f\"-{len(test_feas)}\"):\n",
    "        continue\n",
    "    ############\n",
    "    \n",
    "    print(\"*\" * 30, len(test_feas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafef82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac6ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import os, time\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "import tqdm, re, pickle, gc, os, time, random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613000cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance_fixed1(\n",
    "    xgb_model_path,\n",
    "    feas_cols_v2\n",
    "):\n",
    "    xgb_model = xgb.Booster()\n",
    "    xgb_model.load_model(xgb_model_path)\n",
    "    \n",
    "    weights = xgb_model.get_score(importance_type=\"weight\")\n",
    "    gains = xgb_model.get_score(importance_type=\"gain\")\n",
    "    total_gains = {}\n",
    "    for fea_avt in weights:\n",
    "        total_gains[fea_avt] = weights[fea_avt]*gains[fea_avt]\n",
    "        \n",
    "    iptc_scores = total_gains ## xgb_model.get_score(importance_type=\"total_gain\")\n",
    "    importances = []\n",
    "    for i in range(len(feas_cols_v2)):\n",
    "        fea_avt = f\"f{i}\"\n",
    "        if fea_avt not in iptc_scores:\n",
    "            continue\n",
    "        importances.append((feas_cols_v2[i], iptc_scores[fea_avt]))\n",
    "    importances.sort(key=lambda x: x[1], reverse=True)\n",
    "    sorted_features = [_[0] for _ in importances]\n",
    "    \n",
    "    return importances, sorted_features\n",
    "\n",
    "def train_model_with_different_label_2(dtrain, dtest):\n",
    "    \n",
    "    params={\n",
    "            'booster':'gbtree',\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'max_depth': 5,\n",
    "            'scale_pos_weight': 3,\n",
    "            'learning_rate': 0.1, \n",
    "            'reg_lambda': 5,\n",
    "            'reg_alpha': 0, \n",
    "            'colsample_bytree': 0.8,\n",
    "#             'tree_method': 'gpu_hist',\n",
    "            \"n_gpus\": -1,\n",
    "        }\n",
    "    # params['nthread'] = 25\n",
    "    train_start_time = time.time()\n",
    "    booster_maidian = xgb.train(params, dtrain, num_boost_round=10, evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "                                                      early_stopping_rounds=50,verbose_eval=100)\n",
    "    train_end_time = time.time()\n",
    "    \n",
    "    print(f\"train time: {train_end_time - train_start_time}\")\n",
    "    \n",
    "    if not os.path.exists(\"trained_models\"):\n",
    "        os.makedirs(\"trained_models\")\n",
    "    return booster_maidian\n",
    "\n",
    "def train_original_model_1(\n",
    "    train, test,\n",
    "    feas_cols_v2, \n",
    "    dt_name,\n",
    "    model_dir,\n",
    "    target_label,\n",
    "):\n",
    "    # print(\"starting...\")\n",
    "    all_train_matrix = xgb.DMatrix(\n",
    "        train[feas_cols_v2].values, \n",
    "        train[target_label].values,\n",
    "        feature_names=feas_cols_v2\n",
    "    )\n",
    "\n",
    "    all_test_matrix = xgb.DMatrix(\n",
    "        test[feas_cols_v2].values, \n",
    "        test[target_label].values,\n",
    "        feature_names=feas_cols_v2\n",
    "    )\n",
    "    # print(\"we are here...\")\n",
    "    model = train_model_with_different_label_2(\n",
    "        all_train_matrix, all_test_matrix\n",
    "    )\n",
    "    del all_train_matrix, all_test_matrix\n",
    "    gc.collect()\n",
    "    # print(\"we are here now\")\n",
    "    model_path = os.path.join(model_dir, dt_name)\n",
    "    \n",
    "    model.save_model(model_path)\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c73462",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = \"pre1\"\n",
    "target_label = \"isDefault\"\n",
    "userid_col = \"id\" ## loan_account_id\n",
    "traceid_col = \"issueDate\" ## trace_id\n",
    "n_fold = 5\n",
    "NEED_TO_PREPARE_DATA = True ## 用这个标记一下，是否需要重新准备数据\n",
    "NEED_TO_TRAIN = True ## 用这个标记一下，是否需要重新准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8715928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"preprocessedDataset/{nm}.txt\", \"r\") as f:\n",
    "    feas_cols_v2 = [i.strip() for i in f.readlines()]\n",
    "len(feas_cols_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952328a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 这一步userid的获取要独立进行。\n",
    "## 为的是好在实际环境中修改。\n",
    "all_userid = pq.read_table(\n",
    "    f\"preprocessedDataset/{nm}.parquet\",\n",
    "    columns = [userid_col]\n",
    ").to_pandas()\n",
    "all_userid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ffc1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 51)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feas = pq.read_table(\n",
    "    f\"preprocessedDataset/{nm}.parquet\",\n",
    "    columns = feas_cols_v2 + [target_label, traceid_col]\n",
    ").to_pandas()\n",
    "all_feas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd158cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "if NEED_TO_PREPARE_DATA:\n",
    "    for i in tqdm.tqdm(range(n_fold)):\n",
    "        train_lines = all_feas[all_userid[userid_col] % n_fold != i]\n",
    "        train_lines.to_parquet(f\"preprocessedDataset/{nm}-IDResd_Not_{i}.parquet\", index=False)\n",
    "        del train_lines\n",
    "        gc.collect()\n",
    "\n",
    "        test_lines = all_feas[all_userid[userid_col] % n_fold == i]\n",
    "        test_lines.to_parquet(f\"preprocessedDataset/{nm}-IDResd_{i}.parquet\", index=False)\n",
    "        del test_lines\n",
    "        gc.collect()\n",
    "\n",
    "    del all_feas\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7869d930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** preparing model for predicting id residual is 0......\n",
      "splitting data...\n",
      "start training...\n",
      "[18:43:11] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"n_gpus\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.70138\ttest-auc:0.70088\n",
      "[9]\ttrain-auc:0.71308\ttest-auc:0.71168\n",
      "train time: 3.0662708282470703\n",
      "****************************** preparing model for predicting id residual is 1......\n",
      "splitting data...\n",
      "start training...\n",
      "[18:43:15] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"n_gpus\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.70145\ttest-auc:0.69934\n",
      "[9]\ttrain-auc:0.71320\ttest-auc:0.71027\n",
      "train time: 3.27360200881958\n",
      "****************************** preparing model for predicting id residual is 2......\n",
      "splitting data...\n",
      "start training...\n",
      "[18:43:19] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"n_gpus\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.70178\ttest-auc:0.69836\n",
      "[9]\ttrain-auc:0.71393\ttest-auc:0.71015\n",
      "train time: 3.0330822467803955\n",
      "****************************** preparing model for predicting id residual is 3......\n",
      "splitting data...\n",
      "start training...\n",
      "[18:43:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"n_gpus\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.70133\ttest-auc:0.70060\n",
      "[9]\ttrain-auc:0.71294\ttest-auc:0.71185\n",
      "train time: 3.2624337673187256\n",
      "****************************** preparing model for predicting id residual is 4......\n",
      "splitting data...\n",
      "start training...\n",
      "[18:43:27] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"n_gpus\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.70092\ttest-auc:0.70079\n",
      "[9]\ttrain-auc:0.71336\ttest-auc:0.71183\n",
      "train time: 3.2946600914001465\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_fold):\n",
    "    if not NEED_TO_TRAIN:\n",
    "        continue\n",
    "    print(\"*\"*30, f\"preparing model for predicting id residual is {i}......\")\n",
    "    all_feas_trn = pq.read_table(\n",
    "        f\"preprocessedDataset/{nm}-IDResd_Not_{i}.parquet\",\n",
    "        columns=feas_cols_v2 + [target_label]\n",
    "    ).to_pandas()\n",
    "    print(\"splitting data...\")\n",
    "    y = all_feas_trn[target_label]\n",
    "    train, test = train_test_split(all_feas_trn, test_size=0.25, random_state=30, shuffle=True, stratify=y)\n",
    "    \n",
    "    print(\"start training...\")\n",
    "    model_path = train_original_model_1(\n",
    "        train, test, \n",
    "        feas_cols_v2, \n",
    "        f\"{nm}-IDResd_{i}\", ## 模型的名字\n",
    "        \"trained_models\",\n",
    "        target_label,\n",
    "    )\n",
    "    \n",
    "    ## 把特征重要性写到文件里\n",
    "    importances, sorted_features = get_importance_fixed1(model_path, feas_cols_v2)\n",
    "    len(importances), len(sorted_features)\n",
    "    if not os.path.exists(\"sorted_feas_importance/\"):\n",
    "        os.makedirs(\"sorted_feas_importance/\")\n",
    "    with open(f\"sorted_feas_importance/{nm}-IDResd_{i}--hasImportance_{len(importances)}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(sorted_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70658031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_feas_trn, train, test, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc876400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
