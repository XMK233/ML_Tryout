{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a7bdf8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 第一种实现"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "85e249d5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a050a26f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Muti-head Attention 机制的实现\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self,input_dim,dim_k,dim_v):\n",
    "        '''\n",
    "        input_dim: 说白了就是原来单词的embedding长度。\n",
    "        dim_k: 在attention过程中间用的一个参数。如果你只注重输入和输出，这个就当一个超参数设设就好了。\n",
    "        dim_v: 这就是最后输出的attention向量的长度。\n",
    "        '''\n",
    "        super(Self_Attention,self).__init__()\n",
    "        self.q = nn.Linear(input_dim,dim_k) ## 这里\n",
    "        self.k = nn.Linear(input_dim,dim_k)\n",
    "        self.v = nn.Linear(input_dim,dim_v)\n",
    "        self._norm_fact = 1 / sqrt(dim_k) ## 这个东西是一个trick，据称可以保持梯度的稳定。\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        input : batch_size * seq_len * input_dim\n",
    "        q : batch_size * input_dim * dim_k\n",
    "        k : batch_size * input_dim * dim_k\n",
    "        v : batch_size * input_dim * dim_v\n",
    "        '''\n",
    "        Q = self.q(x) # Q: batch_size * seq_len * dim_k\n",
    "        K = self.k(x) # K: batch_size * seq_len * dim_k\n",
    "        V = self.v(x) # V: batch_size * seq_len * dim_v\n",
    "        \n",
    "        atten = nn.Softmax(dim=-1)(\n",
    "            torch.bmm( ## bmm是batch matrix * matrix的简称。说白了就是\n",
    "                ## 俩batch里面，对应位置的matrix相乘，\n",
    "                ## 然后组成新的batch。\n",
    "                Q, # Q: batch_size * seq_len * dim_k\n",
    "                K.permute(0,2,1) ## permute就是张量的转置。我认为应该是这个意思：原本的维度是（0, 1, 2）\n",
    "                ## 要把第三维和第二维调换一下，并且第一维不变，所以就传参为(0, 2, 1)\n",
    "                ## 所以，K.permute(0, 2, 1)的形状就是batch_size * dim_k * seq_len\n",
    "            ) * self._norm_fact\n",
    "        )  # Q * K.T() # batch_size * seq_len * seq_len\n",
    "        \n",
    "        output = torch.bmm(atten,V) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbc52425",
   "metadata": {
    "hidden": true
   },
   "source": [
    "torch.tensor([\n",
    "        [[1, 2],\n",
    "         [3, 4]],\n",
    "        [[5, 6],\n",
    "         [7, 8]],\n",
    "        [[9, 10],\n",
    "         [11, 12]],\n",
    "        [[13, 14],\n",
    "         [15, 16]],\n",
    "    ]).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76430769",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  7,  10],\n",
       "         [ 15,  22]],\n",
       "\n",
       "        [[ 67,  78],\n",
       "         [ 91, 106]],\n",
       "\n",
       "        [[191, 210],\n",
       "         [231, 254]],\n",
       "\n",
       "        [[379, 406],\n",
       "         [435, 466]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(\n",
    "    torch.tensor([\n",
    "        [[1, 2],\n",
    "         [3, 4]],\n",
    "        [[5, 6],\n",
    "         [7, 8]],\n",
    "        [[9, 10],\n",
    "         [11, 12]],\n",
    "        [[13, 14],\n",
    "         [15, 16]],\n",
    "    ]),\n",
    "    \n",
    "    torch.tensor([\n",
    "        [[1, 2],\n",
    "         [3, 4]],\n",
    "        [[5, 6],\n",
    "         [7, 8]],\n",
    "        [[9, 10],\n",
    "         [11, 12]],\n",
    "        [[13, 14],\n",
    "         [15, 16]],\n",
    "    ])\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b0e664",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(5, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08bf03cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "permute() missing 1 required positional arguments: \"dims\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rc/5v8dff5j52303vp04pm0xz6c0000gn/T/ipykernel_50776/3221807239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## 5句话，每句话三个单词，每一个单词长度为2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: permute() missing 1 required positional arguments: \"dims\""
     ]
    }
   ],
   "source": [
    "X.permute() ## 5句话，每句话三个单词，每一个单词长度为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502aed4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sa = Self_Attention(2, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fc955",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sa(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da4855",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Muti-head Attention 机制的实现\n",
    "## 哦，这里其实不是真正的multi-head，\n",
    "## 这里只是概念验证一下self-att怎么变成multi-att.az\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "\n",
    "class Self_Attention_Muti_Head(nn.Module):\n",
    "    # input : batch_size * seq_len * input_dim\n",
    "    # q : batch_size * input_dim * dim_k\n",
    "    # k : batch_size * input_dim * dim_k\n",
    "    # v : batch_size * input_dim * dim_v\n",
    "    def __init__(self,input_dim,dim_k,dim_v,nums_head):\n",
    "        ## 我们假设输入的每一个字的embedding维度为2，\n",
    "        ## dim_k, dim_v为9，6\n",
    "        ## 3个头。\n",
    "        super(Self_Attention_Muti_Head,self).__init__()\n",
    "        assert dim_k % nums_head == 0\n",
    "        assert dim_v % nums_head == 0\n",
    "        self.q = nn.Linear(input_dim,dim_k)\n",
    "        self.k = nn.Linear(input_dim,dim_k)\n",
    "        self.v = nn.Linear(input_dim,dim_v)\n",
    "        \n",
    "        self.nums_head = nums_head\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self._norm_fact = 1 / sqrt(dim_k)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        Q = self.q(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.nums_head) \n",
    "        K = self.k(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.nums_head) \n",
    "        V = self.v(x).reshape(-1, x.shape[0], x.shape[1], self.dim_v // self.nums_head)\n",
    "        print(x.shape) # torch.Size([4, 7, 2])\n",
    "        print(Q.size()) # torch.Size([3, 4, 7, 3])\n",
    "        print(Q)\n",
    "\n",
    "        atten = nn.Softmax(dim=-1)(torch.matmul(Q,K.permute(0,1,3,2))) # Q * K.T() # batch_size * seq_len * seq_len\n",
    "        \n",
    "        output = torch.matmul(atten,V).reshape(x.shape[0],x.shape[1],-1) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
    "        \n",
    "        return output\n",
    "    \n",
    "mh = Self_Attention_Muti_Head(2, 9, 6, 3) \n",
    "## 我大概明白是啥意思了。\n",
    "## dim_k, dim_v其实是多头之后的结果。\n",
    "## 相当于，这里分三个头，那么每一个头就应该是Self_Attention(2, 3, 2)。\n",
    "## 然后，（4，7，2）的输入，就会在每一个头变成（4，7，3）\n",
    "## 然后，每一个头都分开，那就有三个(4, 7, 3)，就变成(3, (4, 7, 3))-->(3, 4, 7, 3)\n",
    "\n",
    "mh(\n",
    "    torch.randn(4, 7, 2) ## 输入假设是一个七言绝句：一个batch 4句诗，一句7个字，一个字被embedding为2维。\n",
    ") ## 输出形状是(4, 9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed695c0",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.tril(torch.ones((5, 5))) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c22e0d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Mutihead_Attention(nn.Module):\n",
    "    def __init__(self,d_model,dim_k,dim_v,n_heads):\n",
    "        super(Mutihead_Attention, self).__init__()\n",
    "        self.dim_v = dim_v\n",
    "        self.dim_k = dim_k\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.q = nn.Linear(d_model,dim_k)\n",
    "        self.k = nn.Linear(d_model,dim_k)\n",
    "        self.v = nn.Linear(d_model,dim_v)\n",
    "\n",
    "        self.o = nn.Linear(dim_v,d_model)\n",
    "        self.norm_fact = 1 / math.sqrt(d_model)\n",
    "\n",
    "    def generate_mask(self,dim):\n",
    "        # 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。\n",
    "        # padding mask 在数据输入模型之前完成。\n",
    "        matirx = np.ones((dim,dim))\n",
    "        mask = torch.Tensor(np.triu(matirx)) ## np.tril是下三角矩阵，下三角都是1；有人说应该用np.triu，不晓得对不对。\n",
    "\n",
    "        return mask==1 ## 转化一下，下三角都是True，其他地方是False。\n",
    "\n",
    "    def forward(self,x,y,requires_mask=False):\n",
    "        assert self.dim_k % self.n_heads == 0 and self.dim_v % self.n_heads == 0\n",
    "        # size of x : [batch_size * seq_len * batch_size]\n",
    "        # 对 x 进行自注意力\n",
    "        Q = self.q(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) \n",
    "        # n_heads * batch_size * seq_len * dim_k\n",
    "        K = self.k(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) \n",
    "        # n_heads * batch_size * seq_len * dim_k\n",
    "        V = self.v(y).reshape(-1,y.shape[0],y.shape[1],self.dim_v // self.n_heads) \n",
    "        # n_heads * batch_size * seq_len * dim_v\n",
    "        \n",
    "        # print(\"Attention V shape : {}\".format(V.shape))\n",
    "        attention_score = torch.matmul(\n",
    "            Q, # n_heads * batch_size * seq_len * dim_k\n",
    "            K.permute(0,1,3,2) # n_heads * batch_size * dim_k * seq_len\n",
    "        ) * self.norm_fact ## n_heads * batch_size * seq_len * seq_len\n",
    "        # print(\"attention score, \", attention_score)\n",
    "        if requires_mask:\n",
    "            mask = self.generate_mask(x.shape[1])\n",
    "            print(\"true的位置的数据会被填充为0。填充的机制没完全搞懂，这里只展示填充效果，对不对不保证哦。\", mask)\n",
    "            print(\"befor: \", attention_score[0][0])\n",
    "            attention_score = attention_score.masked_fill(mask, value=0) #float(\"-inf\")\n",
    "            print(\"after: \", attention_score[0][0])\n",
    "            # print(\"attention scorejj, \", attention_score)\n",
    "            # 注意这里的小Trick，不需要将Q,K,V 分别MASK,只MASKSoftmax之前的结果就好了\n",
    "        output = torch.matmul(attention_score, V).reshape(y.shape[0],y.shape[1],-1)\n",
    "        # print(\"Attention output shape : {}\".format(output.shape))\n",
    "\n",
    "        output = self.o(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05963861",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "ipt = torch.randn((4, 7, 2))\n",
    "# ipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449801c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Mutihead_Attention(2, 9, 6, 3)(torch.Tensor(ipt), torch.Tensor(ipt), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9bf7e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969ac42",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_mask(dim):\n",
    "    # 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。\n",
    "    # padding mask 在数据输入模型之前完成。\n",
    "    matirx = np.ones((dim,dim))\n",
    "    mask = torch.Tensor(np.tril(matirx)) ## np.tril是下三角矩阵，下三角都是1\n",
    "\n",
    "    return mask==1 ## 转化一下，下三角都是True，其他地方是False。\n",
    "generate_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a8cc3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747e6ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4773fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289350c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2607bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.Tensor(np.ones((5, 5))).masked_fill?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952651d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.Tensor(np.ones((5, 5))).masked_fill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b47aa",
   "metadata": {},
   "source": [
    "# 第二种实现\n",
    "\n",
    "https://colab.research.google.com/drive/15yTJSjZpYuIWzL9hSbyThHLer4iaJjBD?usp=sharing#scrollTo=EFsf8gHrgAl1\n",
    "\n",
    "https://blog.csdn.net/qq_37236745/article/details/107352273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22694eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by wmathor\n",
    "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "              https://github.com/JayParks/transformer\n",
    "'''\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = [\n",
    "        # enc_input           dec_input         dec_output\n",
    "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
    "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "src_len = 5 # enc_input max sequence length\n",
    "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
    "\n",
    "# Transformer Parameters\n",
    "d_model = 512  # Embedding Size\n",
    "d_ff = 2048 # FeedForward dimension\n",
    "d_k = 64 # dimension of K(=Q), \n",
    "d_v = 128  # dimension of V\n",
    "n_layers = 6  # number of Encoder of Decoder Layer\n",
    "n_heads = 8  # number of heads in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f39abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = [\n",
    "        # enc_input                dec_input            dec_output\n",
    "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
    "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "src_len = 5 # enc_input max sequence length\n",
    "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
    "\n",
    "def make_data(sentences):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for sentence in sentences:\n",
    "        enc_inputs.append(\n",
    "            [src_vocab[word] for word in sentence[0].split()]\n",
    "        )\n",
    "        dec_inputs.append(\n",
    "            [tgt_vocab[word] for word in sentence[1].split()]\n",
    "        )\n",
    "        dec_outputs.append(\n",
    "            [tgt_vocab[word] for word in sentence[2].split()]\n",
    "        )\n",
    "    # enc_inputs: [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
    "    # dec_inputs: [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
    "    # dec_outputs: [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe4d19",
   "metadata": {},
   "source": [
    "Furthermore, the outputs are scaled by a factor of \n",
    "    \n",
    "    :math:`\\frac{1}{1-p}` \n",
    "\n",
    "during\n",
    "training. This means that during evaluation the module simply computes an\n",
    "identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c50ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0],\n",
       "        [   1],\n",
       "        [   2],\n",
       "        ...,\n",
       "        [4997],\n",
       "        [4998],\n",
       "        [4999]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 5000).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818bf6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.00000000001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(9.210340371976184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa47f160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.017988946039015984"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-math.log(10000.0) / 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342129e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7736e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7736e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(\n",
    "    torch.arange(0, 512, 2).float() * (-math.log(10000.0) / 512)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "956f56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = torch.zeros(7, 2) ## 7 * 2, full of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51cd72b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 7, dtype=torch.float).unsqueeze(1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aef5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = torch.exp(torch.arange(0, 2, 2).float() * (-math.log(10000.0) / 2)) \n",
    "## 上面那个div_term，我自己手推导了一下，结果发现：div_term = 1/(10000**(2i / d_model))\n",
    "## 只不过为什么要这样搞，为什么要把10000转为exp(log10000)呢？\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfb5134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403],\n",
       "        [ 0.9093, -0.4161],\n",
       "        [ 0.1411, -0.9900],\n",
       "        [-0.7568, -0.6536],\n",
       "        [-0.9589,  0.2837],\n",
       "        [-0.2794,  0.9602]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24984f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5119c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eafbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fddd5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) ## 5000 * 512, full of zeros\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        '''\n",
    "        position looks like: \n",
    "            tensor(\n",
    "                [[   0],\n",
    "                [   1],\n",
    "                [   2],\n",
    "                ...,\n",
    "                [4997],\n",
    "                [4998],\n",
    "                [4999]]\n",
    "            )\n",
    "        '''\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) \n",
    "        ## 上面那个div_term，我自己手推导了一下，结果发现：div_term = 1/(10000**(2i / d_model))\n",
    "        ## 只不过为什么要这样搞，为什么要把10000转为exp(log10000)呢？\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    '''\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], False is masked\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n",
    "\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    '''\n",
    "    seq: [batch_size, tgt_len]\n",
    "    '''\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    return subsequence_mask # [batch_size, tgt_len, tgt_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29e7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32a968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ce6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5055423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4b542ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequence_mask = np.triu(np.ones([7, 7]), k=1)\n",
    "torch.from_numpy(subsequence_mask).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d5d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v] \n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
    "        \n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        '''\n",
    "        input_Q: [batch_size, len_q, d_model]\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\n",
    "        ## transpose的功用和其与permute的差别：https://cloud.tencent.com/developer/article/1914024\n",
    "        output = self.fc(context) # [batch_size, len_q, d_model]\n",
    "        # return nn.LayerNorm(d_model).cuda()(output + residual), attn\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs: [batch_size, seq_len, d_model]\n",
    "        '''\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        # return nn.LayerNorm(d_model).cuda()(output + residual) # [batch_size, seq_len, d_model]\n",
    "        return nn.LayerNorm(d_model)(output + residual) # [batch_size, seq_len, d_model]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        '''\n",
    "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
    "        '''\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        '''\n",
    "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]\n",
    "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_intpus: [batch_size, src_len]\n",
    "        enc_outputs: [batsh_size, src_len, d_model]\n",
    "        '''\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]\n",
    "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1)#.cuda() # [batch_size, tgt_len, d_model]\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)#.cuda() # [batch_size, tgt_len, tgt_len]\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs)#.cuda() # [batch_size, tgt_len, tgt_len]\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0)#.cuda() # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) # [batc_size, tgt_len, src_len]\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()#.cuda()\n",
    "        self.decoder = Decoder()#.cuda()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)#.cuda()\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        # tensor to store decoder outputs\n",
    "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        # dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "model = Transformer()#.cuda()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb09cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss = 2.149814\n",
      "Epoch: 0002 loss = 2.045696\n",
      "Epoch: 0003 loss = 1.830476\n",
      "Epoch: 0004 loss = 1.590124\n",
      "Epoch: 0005 loss = 1.367128\n",
      "Epoch: 0006 loss = 1.141821\n",
      "Epoch: 0007 loss = 0.922311\n",
      "Epoch: 0008 loss = 0.701138\n",
      "Epoch: 0009 loss = 0.557514\n",
      "Epoch: 0010 loss = 0.418543\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        dec_outputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        enc_inputs, dec_inputs, dec_outputs = enc_inputs, dec_inputs, dec_outputs\n",
    "        # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "        loss = criterion(outputs, dec_outputs.view(-1))\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2403b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"\n",
    "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
    "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
    "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    :param model: Transformer Model\n",
    "    :param enc_input: The encoder input\n",
    "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
    "    :return: The target input\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
    "    terminal = False\n",
    "    next_symbol = start_symbol\n",
    "    while not terminal:         \n",
    "        dec_input = torch.cat([dec_input.detach(),torch.tensor([[next_symbol]],dtype=enc_input.dtype)],-1)\n",
    "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "        projected = model.projection(dec_outputs)\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        next_word = prob.data[-1]\n",
    "        next_symbol = next_word\n",
    "        if next_symbol == tgt_vocab[\".\"]:\n",
    "            terminal = True\n",
    "        print(next_word)            \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9024e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(5)\n",
      "tensor(8)\n",
      "tensor([1, 2, 3, 5, 0]) -> ['i', 'want', 'a', 'coke', '.']\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(4)\n",
      "tensor(8)\n",
      "tensor([1, 2, 3, 4, 0]) -> ['i', 'want', 'a', 'beer', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "enc_inputs, _, _ = next(iter(loader))\n",
    "enc_inputs = enc_inputs#.cuda()\n",
    "for i in range(len(enc_inputs)):\n",
    "    greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\n",
    "    predict, _, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e2c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
