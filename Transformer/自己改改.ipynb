{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a7bdf8",
   "metadata": {},
   "source": [
    "# 第一种实现"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "85e249d5",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a050a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muti-head Attention 机制的实现\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self,input_dim,dim_k,dim_v):\n",
    "        '''\n",
    "        input_dim: 说白了就是原来单词的embedding长度。\n",
    "        dim_k: 在attention过程中间用的一个参数。如果你只注重输入和输出，这个就当一个超参数设设就好了。\n",
    "        dim_v: 这就是最后输出的attention向量的长度。\n",
    "        '''\n",
    "        super(Self_Attention,self).__init__()\n",
    "        self.q = nn.Linear(input_dim,dim_k) ## 这里\n",
    "        self.k = nn.Linear(input_dim,dim_k)\n",
    "        self.v = nn.Linear(input_dim,dim_v)\n",
    "        self._norm_fact = 1 / sqrt(dim_k) ## 这个东西是一个trick，据称可以保持梯度的稳定。\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        input : batch_size * seq_len * input_dim\n",
    "        q : batch_size * input_dim * dim_k\n",
    "        k : batch_size * input_dim * dim_k\n",
    "        v : batch_size * input_dim * dim_v\n",
    "        '''\n",
    "        Q = self.q(x) # Q: batch_size * seq_len * dim_k\n",
    "        K = self.k(x) # K: batch_size * seq_len * dim_k\n",
    "        V = self.v(x) # V: batch_size * seq_len * dim_v\n",
    "        \n",
    "        atten = nn.Softmax(dim=-1)(\n",
    "            torch.bmm( ## bmm是batch matrix * matrix的简称。说白了就是\n",
    "                ## 俩batch里面，对应位置的matrix相乘，\n",
    "                ## 然后组成新的batch。\n",
    "                Q, # Q: batch_size * seq_len * dim_k\n",
    "                K.permute(0,2,1) ## permute就是张量的转置。我认为应该是这个意思：原本的维度是（0, 1, 2）\n",
    "                ## 要把第三维和第二维调换一下，并且第一维不变，所以就传参为(0, 2, 1)\n",
    "                ## 所以，K.permute(0, 2, 1)的形状就是batch_size * dim_k * seq_len\n",
    "            ) * self._norm_fact\n",
    "        )  # Q * K.T() # batch_size * seq_len * seq_len\n",
    "        \n",
    "        output = torch.bmm(atten,V) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbc52425",
   "metadata": {},
   "source": [
    "torch.tensor([\n",
    "        [[1, 2],\n",
    "         [3, 4]],\n",
    "        [[5, 6],\n",
    "         [7, 8]],\n",
    "        [[9, 10],\n",
    "         [11, 12]],\n",
    "        [[13, 14],\n",
    "         [15, 16]],\n",
    "    ]).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76430769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  7,  10],\n",
       "         [ 15,  22]],\n",
       "\n",
       "        [[ 67,  78],\n",
       "         [ 91, 106]],\n",
       "\n",
       "        [[191, 210],\n",
       "         [231, 254]],\n",
       "\n",
       "        [[379, 406],\n",
       "         [435, 466]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(\n",
    "    torch.tensor([\n",
    "        [[1, 2],\n",
    "         [3, 4]],\n",
    "        [[5, 6],\n",
    "         [7, 8]],\n",
    "        [[9, 10],\n",
    "         [11, 12]],\n",
    "        [[13, 14],\n",
    "         [15, 16]],\n",
    "    ]),\n",
    "    \n",
    "    torch.tensor([\n",
    "        [[1, 2],\n",
    "         [3, 4]],\n",
    "        [[5, 6],\n",
    "         [7, 8]],\n",
    "        [[9, 10],\n",
    "         [11, 12]],\n",
    "        [[13, 14],\n",
    "         [15, 16]],\n",
    "    ])\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75b0e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(5, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08bf03cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1397,  1.5112],\n",
       "         [ 0.6295, -0.4357],\n",
       "         [ 0.5890,  0.4404]],\n",
       "\n",
       "        [[-0.6805,  1.9379],\n",
       "         [ 1.7581, -1.1805],\n",
       "         [ 0.3078,  0.9715]],\n",
       "\n",
       "        [[-2.2677, -0.7401],\n",
       "         [ 0.4469,  0.5488],\n",
       "         [ 1.7480, -1.0478]],\n",
       "\n",
       "        [[ 0.5224,  0.6812],\n",
       "         [ 0.4704, -0.1157],\n",
       "         [ 0.0523, -1.2683]],\n",
       "\n",
       "        [[-1.0887,  1.7661],\n",
       "         [-1.5415,  0.6116],\n",
       "         [ 0.6841, -0.3969]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute() ## 5句话，每句话三个单词，每一个单词长度为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b502aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = Self_Attention(2, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba3fc955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1650,  0.0408, -1.0610, -0.6229],\n",
      "         [ 0.2948, -0.3838, -0.3129,  1.0601],\n",
      "         [ 0.1673, -0.1334, -0.7205,  0.4199]],\n",
      "\n",
      "        [[-0.5803, -0.1009, -0.9438, -1.4562],\n",
      "         [ 0.8860, -0.2362, -0.3979,  2.3171],\n",
      "         [-0.0233, -0.0662, -0.8664, -0.1353]],\n",
      "\n",
      "        [[-0.9451, -1.4295,  0.9766, -0.6142],\n",
      "         [ 0.0910, -0.1476, -0.7169,  0.2515],\n",
      "         [ 0.8649, -0.1996, -0.4581,  2.2175]],\n",
      "\n",
      "        [[ 0.1077, -0.0829, -0.8106,  0.2079],\n",
      "         [ 0.1846, -0.3399, -0.4049,  0.7323],\n",
      "         [ 0.1445, -0.8244,  0.3171,  1.2673]],\n",
      "\n",
      "        [[-0.7388, -0.2870, -0.6998, -1.6017],\n",
      "         [-0.7940, -0.7836,  0.0369, -1.0879],\n",
      "         [ 0.3141, -0.3541, -0.3532,  1.0685]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.1650,  0.2948,  0.1673],\n",
      "         [ 0.0408, -0.3838, -0.1334],\n",
      "         [-1.0610, -0.3129, -0.7205],\n",
      "         [-0.6229,  1.0601,  0.4199]],\n",
      "\n",
      "        [[-0.5803,  0.8860, -0.0233],\n",
      "         [-0.1009, -0.2362, -0.0662],\n",
      "         [-0.9438, -0.3979, -0.8664],\n",
      "         [-1.4562,  2.3171, -0.1353]],\n",
      "\n",
      "        [[-0.9451,  0.0910,  0.8649],\n",
      "         [-1.4295, -0.1476, -0.1996],\n",
      "         [ 0.9766, -0.7169, -0.4581],\n",
      "         [-0.6142,  0.2515,  2.2175]],\n",
      "\n",
      "        [[ 0.1077,  0.1846,  0.1445],\n",
      "         [-0.0829, -0.3399, -0.8244],\n",
      "         [-0.8106, -0.4049,  0.3171],\n",
      "         [ 0.2079,  0.7323,  1.2673]],\n",
      "\n",
      "        [[-0.7388, -0.7940,  0.3141],\n",
      "         [-0.2870, -0.7836, -0.3541],\n",
      "         [-0.6998,  0.0369, -0.3532],\n",
      "         [-1.6017, -1.0879,  1.0685]]], grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3002, -0.0599,  0.2961, -0.2256, -0.3441],\n",
       "         [-0.3773, -0.2764,  0.3004, -0.0882, -0.2022],\n",
       "         [-0.3464, -0.1867,  0.2987, -0.1474, -0.2621]],\n",
       "\n",
       "        [[-0.3608,  0.2283,  0.2956, -0.7527, -0.6980],\n",
       "         [-0.3603, -0.5882,  0.3025,  0.3798,  0.1316],\n",
       "         [-0.3671,  0.0886,  0.2970, -0.5724, -0.5625]],\n",
       "\n",
       "        [[-0.3743,  0.1780,  0.2964, -0.7116, -0.6606],\n",
       "         [-0.1488,  0.1563,  0.2896, -0.2053, -0.4103],\n",
       "         [ 0.3167,  0.3531,  0.2733,  0.5049, -0.1389]],\n",
       "\n",
       "        [[-0.1708,  0.1091,  0.2907, -0.1865, -0.3847],\n",
       "         [-0.2023,  0.0570,  0.2921, -0.1807, -0.3637],\n",
       "         [-0.2280,  0.0141,  0.2933, -0.1757, -0.3462]],\n",
       "\n",
       "        [[-0.2560,  0.0601,  0.2937, -0.2984, -0.4211],\n",
       "         [-0.2538,  0.0575,  0.2937, -0.2901, -0.4163],\n",
       "         [-0.1220, -0.2440,  0.2921,  0.4059,  0.0233]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0da4855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 7, 2])\n",
      "torch.Size([3, 4, 7, 3])\n",
      "tensor([[[[-4.0858e-01,  2.4728e-01, -6.0135e-01],\n",
      "          [-1.3510e+00,  1.6699e+00, -1.2581e+00],\n",
      "          [-9.3690e-01,  8.5264e-01,  1.1535e+00],\n",
      "          [-6.9821e-01,  3.3121e-02, -7.3311e-01],\n",
      "          [-1.0629e+00,  1.0589e+00, -1.9279e+00],\n",
      "          [-5.0314e-01,  1.1458e+00,  9.4173e-01],\n",
      "          [-2.3354e-01,  3.6383e-01,  3.3549e-01]],\n",
      "\n",
      "         [[-4.6762e-01,  6.3938e-01, -1.3910e-01],\n",
      "          [-2.4085e-02,  3.5489e-01,  5.0659e-01],\n",
      "          [-6.8826e-01,  1.5385e-02,  9.4257e-01],\n",
      "          [ 9.8887e-01, -1.6491e+00, -5.1259e-01],\n",
      "          [ 1.7726e+00,  5.1078e-01, -5.6163e-01],\n",
      "          [-7.1630e-01,  7.0723e-03,  1.0239e-01],\n",
      "          [-3.9923e-03, -3.5708e-01, -1.2668e+00]],\n",
      "\n",
      "         [[ 6.8045e-01,  8.4860e-01,  1.6581e-01],\n",
      "          [ 3.2884e-02,  5.6519e-01,  1.6655e-01],\n",
      "          [-1.0906e+00,  1.6752e+00,  2.3532e-01],\n",
      "          [-8.2078e-01,  1.9371e-01,  9.6364e-01],\n",
      "          [-1.3699e-01,  4.2687e-01,  9.3557e-01],\n",
      "          [ 1.2245e-01, -6.5140e-02,  5.4752e-01],\n",
      "          [ 5.9364e-01,  4.9174e-02,  7.4432e-02]],\n",
      "\n",
      "         [[-7.0767e-01,  1.7530e-02, -1.6493e-01],\n",
      "          [-3.4721e-01,  1.0408e-01, -1.4728e+00],\n",
      "          [ 2.9573e-01,  9.4130e-01,  4.1731e-01],\n",
      "          [ 1.2801e-01,  6.3605e-01,  1.7502e-01],\n",
      "          [-1.2282e+00,  1.9328e+00,  4.2632e-01],\n",
      "          [-1.0110e+00,  1.1043e-01,  1.0647e+00],\n",
      "          [-8.0321e-01, -5.5655e-02, -3.9045e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.3257e-02, -3.7400e-01, -1.5527e+00],\n",
      "          [ 6.7093e-01,  9.7468e-01,  1.9439e-01],\n",
      "          [-4.8900e-02,  5.0032e-01,  4.2182e-01],\n",
      "          [-6.4842e-01,  1.0251e+00,  2.8985e-01],\n",
      "          [-2.9741e-01,  1.6711e-01,  6.3947e-01],\n",
      "          [-4.4674e-01,  2.0820e-01,  1.0442e-01],\n",
      "          [-4.2094e-01,  4.0855e-01, -7.4387e-01]],\n",
      "\n",
      "         [[ 1.1144e-01,  6.2084e-01,  4.7191e-01],\n",
      "          [-3.2368e-01,  2.9418e-01,  4.9467e-01],\n",
      "          [-1.3098e-01,  1.2231e-01, -1.8078e-01],\n",
      "          [ 3.8531e-01,  3.7127e-01,  2.5972e-01],\n",
      "          [-4.5301e-01,  1.9995e-01,  3.4209e-01],\n",
      "          [-1.1797e-01,  2.5525e-03, -5.5798e-01],\n",
      "          [ 4.5051e-01,  5.3724e-01,  2.4990e-01]],\n",
      "\n",
      "         [[-7.3833e-01, -5.5455e-03, -1.5212e-01],\n",
      "          [-2.8370e-01, -4.2998e-03, -1.5214e+00],\n",
      "          [ 3.7833e-01,  9.6233e-01,  3.7071e-01],\n",
      "          [-2.7947e-01,  3.4379e-01, -6.1263e-01],\n",
      "          [-1.5658e+00,  2.0566e+00, -1.0178e+00],\n",
      "          [-1.2262e+00,  7.4814e-01,  1.3112e+00],\n",
      "          [-4.9517e-01,  1.7704e-01, -2.2709e-01]],\n",
      "\n",
      "         [[-7.5456e-01,  8.1177e-01, -1.1137e+00],\n",
      "          [-2.4023e-01,  7.8560e-01,  7.1626e-01],\n",
      "          [-3.8881e-01,  2.4263e-01,  6.9057e-01],\n",
      "          [ 2.1203e-01, -3.8337e-01, -1.4351e-01],\n",
      "          [ 7.9197e-01,  3.5285e-01,  8.2394e-03],\n",
      "          [-4.0104e-01,  2.4933e-01, -3.6337e-01],\n",
      "          [-1.0692e+00,  1.3028e+00, -1.0452e+00]]],\n",
      "\n",
      "\n",
      "        [[[-6.2669e-01,  7.5730e-01,  9.4698e-01],\n",
      "          [ 2.2769e-01,  6.9656e-01,  1.0991e+00],\n",
      "          [-2.4327e-01,  7.0810e-01,  1.3889e+00],\n",
      "          [ 4.4209e-02, -3.1908e-01,  3.4321e-01],\n",
      "          [-1.0683e+00, -2.4191e-01, -8.1130e-01],\n",
      "          [-5.8342e-01,  1.3078e-01, -2.7087e+00],\n",
      "          [ 1.7481e-01,  1.4867e+00,  5.8959e-01]],\n",
      "\n",
      "         [[-4.7377e-01,  1.7402e-01,  1.0376e+00],\n",
      "          [ 7.7240e-01, -1.1925e+00, -1.8651e-02],\n",
      "          [ 1.4479e+00,  2.9461e-01, -4.0254e-01],\n",
      "          [-1.2475e-01,  4.2891e-01,  1.4083e+00],\n",
      "          [ 6.8655e-01, -8.0211e-01,  9.6500e-01],\n",
      "          [ 1.2156e+00, -1.3790e-01, -3.3887e-01],\n",
      "          [-5.6562e-01,  1.2418e-01, -2.0785e-01]],\n",
      "\n",
      "         [[-6.2121e-01,  5.7939e-01, -1.2339e+00],\n",
      "          [-6.4430e-02,  8.3773e-01,  6.1839e-01],\n",
      "          [ 2.1064e-01,  6.8491e-01,  1.0277e+00],\n",
      "          [-3.0485e-01,  7.7610e-01,  1.2965e+00],\n",
      "          [-1.7519e-02, -2.7802e-01,  3.8830e-01],\n",
      "          [ 3.9422e-01,  8.1245e-01,  1.6570e+00],\n",
      "          [ 1.8592e-01,  2.7202e-01,  2.1758e+00]],\n",
      "\n",
      "         [[ 4.5572e-01, -6.6797e-01,  2.9098e-02],\n",
      "          [-6.5608e-01,  4.8608e-02,  3.2922e-01],\n",
      "          [ 1.8213e-01, -5.5569e-01, -9.6136e-01],\n",
      "          [ 8.6361e-01,  7.1306e-01,  2.9565e-02],\n",
      "          [-5.0675e-01,  1.7196e-01, -4.6409e-01],\n",
      "          [-1.0289e+00,  1.1657e+00, -1.3335e+00],\n",
      "          [-5.4052e-01,  8.8398e-01,  9.1727e-01]]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.8946e-02, -1.9290e-01,  1.4421e-01, -9.6825e-02,  6.1973e-01,\n",
       "           2.0504e-01],\n",
       "         [-8.4352e-02, -1.8786e-01, -1.1483e-01, -2.2165e-01,  5.7221e-01,\n",
       "           1.4039e-01],\n",
       "         [ 2.1672e-01, -7.7053e-02,  2.5861e-01,  4.2332e-01,  2.3794e-01,\n",
       "           4.5298e-01],\n",
       "         [ 3.9231e-01,  5.3636e-01,  1.0507e-01,  1.2187e-01,  5.5238e-02,\n",
       "           7.5112e-02],\n",
       "         [ 2.4136e-01,  5.0137e-01,  2.3308e-01,  2.5098e-01, -1.8533e-01,\n",
       "          -3.8648e-01],\n",
       "         [ 1.8472e-02, -3.4471e-01,  5.1114e-01, -4.2365e-01,  4.4334e-01,\n",
       "          -2.8723e-01],\n",
       "         [ 2.3256e-01, -3.3519e-01,  2.9358e-03, -2.9405e-01, -2.1673e-01,\n",
       "          -3.1806e-01]],\n",
       "\n",
       "        [[ 1.1785e-01, -1.4604e-01,  7.0851e-02, -6.5584e-01, -4.9865e-02,\n",
       "          -1.1975e-01],\n",
       "         [-3.6672e-02, -1.9171e-01,  1.6763e-01,  5.4781e-01,  1.7626e-01,\n",
       "           4.8465e-01],\n",
       "         [ 1.3641e-01, -7.0025e-02, -7.9465e-02, -2.5610e-01, -1.5840e-01,\n",
       "           4.3348e-02],\n",
       "         [-7.6075e-03,  2.6761e-01,  6.5117e-02,  4.3361e-01,  5.3129e-02,\n",
       "           3.9122e-01],\n",
       "         [ 5.7912e-02,  2.0940e-01,  1.1140e-02, -7.9445e-02,  7.0228e-02,\n",
       "           3.5490e-02],\n",
       "         [ 1.4748e-01, -2.5728e-02,  1.1700e-01, -8.0765e-03,  4.0824e-02,\n",
       "           5.5861e-02],\n",
       "         [ 1.6962e-01, -4.4992e-02,  1.1099e-01, -5.3810e-03,  2.6127e-02,\n",
       "           6.7811e-02]],\n",
       "\n",
       "        [[ 9.9962e-02, -2.3608e-02, -1.0992e-01, -4.4958e-01,  4.8536e-02,\n",
       "          -1.8776e-01],\n",
       "         [ 9.0359e-04, -2.3155e-01,  2.1184e-01,  1.5096e-01,  2.1253e-01,\n",
       "           6.4502e-01],\n",
       "         [ 6.8004e-02, -9.0131e-02,  1.2566e-01,  4.2934e-02,  2.1420e-01,\n",
       "          -2.8259e-03],\n",
       "         [ 2.0776e-01, -1.0171e-02, -5.6361e-04, -1.3657e-02, -6.5022e-02,\n",
       "          -4.9913e-02],\n",
       "         [ 1.1465e-01,  1.6193e-02,  1.9248e-01,  2.5167e-02,  8.1777e-01,\n",
       "           1.1057e+00],\n",
       "         [ 7.3895e-01,  9.0249e-01,  8.4493e-01,  1.1823e+00,  1.3523e-01,\n",
       "           1.0275e-01],\n",
       "         [-3.4258e-02,  1.1516e-02, -6.8629e-01, -4.7910e-01,  6.7145e-01,\n",
       "           6.0221e-01]],\n",
       "\n",
       "        [[ 8.1377e-01, -1.8316e-01, -4.1642e-01,  3.2509e-01, -2.9863e-01,\n",
       "           1.2115e-01],\n",
       "         [ 9.4568e-01, -2.9666e-01,  1.4174e-02,  1.2143e-01, -3.3511e-01,\n",
       "           1.9188e-01],\n",
       "         [ 1.6921e-01,  2.8109e-01, -7.2202e-01, -3.3734e-01,  1.0383e-01,\n",
       "          -6.7826e-02],\n",
       "         [ 1.3117e-01,  1.5058e-02,  2.2473e-01,  1.0083e-01, -1.8115e-01,\n",
       "          -5.0933e-02],\n",
       "         [ 2.0564e-01,  1.4939e-01,  2.0860e-01,  2.4120e-01,  1.0622e-01,\n",
       "          -1.6560e-01],\n",
       "         [ 3.6570e-01, -1.5936e-02,  1.0001e-01, -1.8757e-01,  2.5087e-01,\n",
       "          -3.4962e-01],\n",
       "         [ 3.3758e-01, -1.4944e-01,  4.9752e-01, -3.2856e-01,  4.7603e-01,\n",
       "          -9.4943e-02]]], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muti-head Attention 机制的实现\n",
    "## 哦，这里其实不是真正的multi-head，\n",
    "## 这里只是概念验证一下self-att怎么变成multi-att.az\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "\n",
    "class Self_Attention_Muti_Head(nn.Module):\n",
    "    # input : batch_size * seq_len * input_dim\n",
    "    # q : batch_size * input_dim * dim_k\n",
    "    # k : batch_size * input_dim * dim_k\n",
    "    # v : batch_size * input_dim * dim_v\n",
    "    def __init__(self,input_dim,dim_k,dim_v,nums_head):\n",
    "        ## 我们假设输入的每一个字的embedding维度为2，\n",
    "        ## dim_k, dim_v为9，6\n",
    "        ## 3个头。\n",
    "        super(Self_Attention_Muti_Head,self).__init__()\n",
    "        assert dim_k % nums_head == 0\n",
    "        assert dim_v % nums_head == 0\n",
    "        self.q = nn.Linear(input_dim,dim_k)\n",
    "        self.k = nn.Linear(input_dim,dim_k)\n",
    "        self.v = nn.Linear(input_dim,dim_v)\n",
    "        \n",
    "        self.nums_head = nums_head\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self._norm_fact = 1 / sqrt(dim_k)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        Q = self.q(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.nums_head) \n",
    "        K = self.k(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.nums_head) \n",
    "        V = self.v(x).reshape(-1, x.shape[0], x.shape[1], self.dim_v // self.nums_head)\n",
    "        print(x.shape) # torch.Size([4, 7, 2])\n",
    "        print(Q.size()) # torch.Size([3, 4, 7, 3])\n",
    "        print(Q)\n",
    "\n",
    "        atten = nn.Softmax(dim=-1)(torch.matmul(Q,K.permute(0,1,3,2))) # Q * K.T() # batch_size * seq_len * seq_len\n",
    "        \n",
    "        output = torch.matmul(atten,V).reshape(x.shape[0],x.shape[1],-1) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
    "        \n",
    "        return output\n",
    "    \n",
    "mh = Self_Attention_Muti_Head(2, 9, 6, 3) \n",
    "## 我大概明白是啥意思了。\n",
    "## dim_k, dim_v其实是多头之后的结果。\n",
    "## 相当于，这里分三个头，那么每一个头就应该是Self_Attention(2, 3, 2)。\n",
    "## 然后，（4，7，2）的输入，就会在每一个头变成（4，7，3）\n",
    "## 然后，每一个头都分开，那就有三个(4, 7, 3)，就变成(3, (4, 7, 3))-->(3, 4, 7, 3)\n",
    "\n",
    "mh(\n",
    "    torch.randn(4, 7, 2) ## 输入假设是一个七言绝句：一个batch 4句诗，一句7个字，一个字被embedding为2维。\n",
    ") ## 输出形状是(4, 9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fed695c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False],\n",
       "       [ True,  True, False, False, False],\n",
       "       [ True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tril(torch.ones((5, 5))) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "55c22e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutihead_Attention(nn.Module):\n",
    "    def __init__(self,d_model,dim_k,dim_v,n_heads):\n",
    "        super(Mutihead_Attention, self).__init__()\n",
    "        self.dim_v = dim_v\n",
    "        self.dim_k = dim_k\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.q = nn.Linear(d_model,dim_k)\n",
    "        self.k = nn.Linear(d_model,dim_k)\n",
    "        self.v = nn.Linear(d_model,dim_v)\n",
    "\n",
    "        self.o = nn.Linear(dim_v,d_model)\n",
    "        self.norm_fact = 1 / math.sqrt(d_model)\n",
    "\n",
    "    def generate_mask(self,dim):\n",
    "        # 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。\n",
    "        # padding mask 在数据输入模型之前完成。\n",
    "        matirx = np.ones((dim,dim))\n",
    "        mask = torch.Tensor(np.triu(matirx)) ## np.tril是下三角矩阵，下三角都是1；有人说应该用np.triu，不晓得对不对。\n",
    "\n",
    "        return mask==1 ## 转化一下，下三角都是True，其他地方是False。\n",
    "\n",
    "    def forward(self,x,y,requires_mask=False):\n",
    "        assert self.dim_k % self.n_heads == 0 and self.dim_v % self.n_heads == 0\n",
    "        # size of x : [batch_size * seq_len * batch_size]\n",
    "        # 对 x 进行自注意力\n",
    "        Q = self.q(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) \n",
    "        # n_heads * batch_size * seq_len * dim_k\n",
    "        K = self.k(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) \n",
    "        # n_heads * batch_size * seq_len * dim_k\n",
    "        V = self.v(y).reshape(-1,y.shape[0],y.shape[1],self.dim_v // self.n_heads) \n",
    "        # n_heads * batch_size * seq_len * dim_v\n",
    "        \n",
    "        # print(\"Attention V shape : {}\".format(V.shape))\n",
    "        attention_score = torch.matmul(\n",
    "            Q, # n_heads * batch_size * seq_len * dim_k\n",
    "            K.permute(0,1,3,2) # n_heads * batch_size * dim_k * seq_len\n",
    "        ) * self.norm_fact ## n_heads * batch_size * seq_len * seq_len\n",
    "        # print(\"attention score, \", attention_score)\n",
    "        if requires_mask:\n",
    "            mask = self.generate_mask(x.shape[1])\n",
    "            print(\"true的位置的数据会被填充为0。填充的机制没完全搞懂，这里只展示填充效果，对不对不保证哦。\", mask)\n",
    "            print(\"befor: \", attention_score[0][0])\n",
    "            attention_score = attention_score.masked_fill(mask, value=0) #float(\"-inf\")\n",
    "            print(\"after: \", attention_score[0][0])\n",
    "            # print(\"attention scorejj, \", attention_score)\n",
    "            # 注意这里的小Trick，不需要将Q,K,V 分别MASK,只MASKSoftmax之前的结果就好了\n",
    "        output = torch.matmul(attention_score, V).reshape(y.shape[0],y.shape[1],-1)\n",
    "        # print(\"Attention output shape : {}\".format(output.shape))\n",
    "\n",
    "        output = self.o(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "05963861",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "ipt = torch.randn((4, 7, 2))\n",
    "# ipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8449801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true的位置的数据会被填充为0。填充的机制没完全搞懂，这里只展示填充效果，对不对不保证哦。 tensor([[ True,  True,  True,  True,  True,  True,  True],\n",
      "        [False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True]])\n",
      "befor:  tensor([[ 1.0552, -1.0871,  1.6156,  0.3690, -0.5935,  0.4233, -0.6306],\n",
      "        [-0.3026,  0.1338,  1.0413, -0.2230,  0.2028,  0.7466,  0.0107],\n",
      "        [-0.4061,  0.5766, -0.9375, -0.1170,  0.2791, -0.3320,  0.2902],\n",
      "        [ 0.6132, -0.1430,  0.3142,  0.2647, -0.1610, -0.0699, -0.2552],\n",
      "        [-0.2740, -0.0920,  1.1373, -0.2178,  0.0974,  0.7678, -0.0310],\n",
      "        [-0.4445,  0.4359, -0.4868, -0.1705,  0.2546, -0.0664,  0.2438],\n",
      "        [-0.0863,  1.2084, -1.5245,  0.0819,  0.4727, -0.7355,  0.3026]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "after:  tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3026,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4061,  0.5766,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.6132, -0.1430,  0.3142,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2740, -0.0920,  1.1373, -0.2178,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4445,  0.4359, -0.4868, -0.1705,  0.2546,  0.0000,  0.0000],\n",
      "        [-0.0863,  1.2084, -1.5245,  0.0819,  0.4727, -0.7355,  0.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1720, -0.3831],\n",
       "         [-0.0140, -0.7989],\n",
       "         [-0.1939, -0.9337],\n",
       "         [ 0.2934, -0.0763],\n",
       "         [-0.2307, -1.9347],\n",
       "         [ 0.4391, -0.1857],\n",
       "         [ 0.1242, -0.0739]],\n",
       "\n",
       "        [[ 0.2572, -0.3474],\n",
       "         [ 0.9877,  0.1606],\n",
       "         [-0.3768, -1.0800],\n",
       "         [ 0.3564, -0.1386],\n",
       "         [ 0.2635, -0.5637],\n",
       "         [ 0.2286, -0.3488],\n",
       "         [ 0.3693, -0.2216]],\n",
       "\n",
       "        [[ 0.4169, -0.1140],\n",
       "         [ 0.0421, -0.0378],\n",
       "         [ 0.2863, -0.2530],\n",
       "         [ 0.0978, -0.3705],\n",
       "         [ 0.4165, -0.3668],\n",
       "         [ 0.3245, -0.1989],\n",
       "         [ 0.7155,  0.0987]],\n",
       "\n",
       "        [[ 0.2003, -0.3368],\n",
       "         [ 0.2071, -0.3556],\n",
       "         [ 0.3143, -0.3275],\n",
       "         [ 0.0312, -0.4139],\n",
       "         [ 0.7311, -0.0796],\n",
       "         [ 0.0417, -0.5087],\n",
       "         [-0.3894, -1.0257]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mutihead_Attention(2, 9, 6, 3)(torch.Tensor(ipt), torch.Tensor(ipt), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c0a9bf7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b969ac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_mask(dim):\n",
    "    # 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。\n",
    "    # padding mask 在数据输入模型之前完成。\n",
    "    matirx = np.ones((dim,dim))\n",
    "    mask = torch.Tensor(np.tril(matirx)) ## np.tril是下三角矩阵，下三角都是1\n",
    "\n",
    "    return mask==1 ## 转化一下，下三角都是True，其他地方是False。\n",
    "generate_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a8cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747e6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4773fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289350c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a2607bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `masked_fill` not found.\n"
     ]
    }
   ],
   "source": [
    "torch.Tensor(np.ones((5, 5))).masked_fill?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952651d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(np.ones((5, 5))).masked_fill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b47aa",
   "metadata": {},
   "source": [
    "# 第二种实现\n",
    "\n",
    "https://colab.research.google.com/drive/15yTJSjZpYuIWzL9hSbyThHLer4iaJjBD?usp=sharing#scrollTo=EFsf8gHrgAl1\n",
    "\n",
    "https://blog.csdn.net/qq_37236745/article/details/107352273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b22694eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by wmathor\n",
    "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "              https://github.com/JayParks/transformer\n",
    "'''\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = [\n",
    "        # enc_input           dec_input         dec_output\n",
    "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
    "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "src_len = 5 # enc_input max sequence length\n",
    "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
    "\n",
    "# Transformer Parameters\n",
    "d_model = 512  # Embedding Size\n",
    "d_ff = 2048 # FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_layers = 6  # number of Encoder of Decoder Layer\n",
    "n_heads = 8  # number of heads in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f39abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = [\n",
    "        # enc_input                dec_input            dec_output\n",
    "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
    "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "src_len = 5 # enc_input max sequence length\n",
    "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
    "\n",
    "def make_data(sentences):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for sentence in sentences:\n",
    "        enc_inputs.append(\n",
    "            [src_vocab[word] for word in sentence[0].split()]\n",
    "        )\n",
    "        dec_inputs.append(\n",
    "            [tgt_vocab[word] for word in sentence[1].split()]\n",
    "        )\n",
    "        dec_outputs.append(\n",
    "            [tgt_vocab[word] for word in sentence[2].split()]\n",
    "        )\n",
    "    # enc_inputs: [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
    "    # dec_inputs: [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
    "    # dec_outputs: [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe4d19",
   "metadata": {},
   "source": [
    "Furthermore, the outputs are scaled by a factor of \n",
    "    \n",
    "    :math:`\\frac{1}{1-p}` \n",
    "\n",
    "during\n",
    "training. This means that during evaluation the module simply computes an\n",
    "identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6c50ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0],\n",
       "        [   1],\n",
       "        [   2],\n",
       "        ...,\n",
       "        [4997],\n",
       "        [4998],\n",
       "        [4999]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 5000).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "818bf6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.00000000001"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(9.210340371976184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa47f160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.017988946039015984"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-math.log(10000.0) / 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "342129e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7736e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7736e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(\n",
    "    torch.arange(0, 512, 2).float() * (-math.log(10000.0) / 512)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "956f56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = torch.zeros(7, 2) ## 7 * 2, full of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "51cd72b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 7, dtype=torch.float).unsqueeze(1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8aef5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = torch.exp(torch.arange(0, 2, 2).float() * (-math.log(10000.0) / 2)) \n",
    "## 上面那个div_term，我自己手推导了一下，结果发现：div_term = 1/(10000**(2i / d_model))\n",
    "## 只不过为什么要这样搞，为什么要把10000转为exp(log10000)呢？\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bfb5134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403],\n",
       "        [ 0.9093, -0.4161],\n",
       "        [ 0.1411, -0.9900],\n",
       "        [-0.7568, -0.6536],\n",
       "        [-0.9589,  0.2837],\n",
       "        [-0.2794,  0.9602]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24984f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5119c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eafbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1fddd5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) ## 5000 * 512, full of zeros\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        '''\n",
    "        position looks like: \n",
    "            tensor(\n",
    "                [[   0],\n",
    "                [   1],\n",
    "                [   2],\n",
    "                ...,\n",
    "                [4997],\n",
    "                [4998],\n",
    "                [4999]]\n",
    "            )\n",
    "        '''\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) \n",
    "        ## 上面那个div_term，我自己手推导了一下，结果发现：div_term = 1/(10000**(2i / d_model))\n",
    "        ## 只不过为什么要这样搞，为什么要把10000转为exp(log10000)呢？\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    '''\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], False is masked\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n",
    "\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    '''\n",
    "    seq: [batch_size, tgt_len]\n",
    "    '''\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    return subsequence_mask # [batch_size, tgt_len, tgt_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29e7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32a968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ce6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5055423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a4b542ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequence_mask = np.triu(np.ones([7, 7]), k=1)\n",
    "torch.from_numpy(subsequence_mask).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d5d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v] \n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
    "        \n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        '''\n",
    "        input_Q: [batch_size, len_q, d_model]\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\n",
    "        ## transpose的功用和其与permute的差别：https://cloud.tencent.com/developer/article/1914024\n",
    "        output = self.fc(context) # [batch_size, len_q, d_model]\n",
    "        # return nn.LayerNorm(d_model).cuda()(output + residual), attn\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs: [batch_size, seq_len, d_model]\n",
    "        '''\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        # return nn.LayerNorm(d_model).cuda()(output + residual) # [batch_size, seq_len, d_model]\n",
    "        return nn.LayerNorm(d_model)(output + residual) # [batch_size, seq_len, d_model]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        '''\n",
    "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
    "        '''\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        '''\n",
    "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]\n",
    "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_intpus: [batch_size, src_len]\n",
    "        enc_outputs: [batsh_size, src_len, d_model]\n",
    "        '''\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]\n",
    "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1)#.cuda() # [batch_size, tgt_len, d_model]\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)#.cuda() # [batch_size, tgt_len, tgt_len]\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs)#.cuda() # [batch_size, tgt_len, tgt_len]\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0)#.cuda() # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) # [batc_size, tgt_len, src_len]\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()#.cuda()\n",
    "        self.decoder = Decoder()#.cuda()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)#.cuda()\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        # tensor to store decoder outputs\n",
    "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        # dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "model = Transformer()#.cuda()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb09cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss = 2.441665\n",
      "Epoch: 0002 loss = 2.350452\n",
      "Epoch: 0003 loss = 2.110575\n",
      "Epoch: 0004 loss = 1.802409\n",
      "Epoch: 0005 loss = 1.525224\n",
      "Epoch: 0006 loss = 1.441996\n",
      "Epoch: 0007 loss = 1.246899\n",
      "Epoch: 0008 loss = 1.004048\n",
      "Epoch: 0009 loss = 0.804919\n",
      "Epoch: 0010 loss = 0.665496\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        dec_outputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        enc_inputs, dec_inputs, dec_outputs = enc_inputs, dec_inputs, dec_outputs\n",
    "        # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "        loss = criterion(outputs, dec_outputs.view(-1))\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2403b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"\n",
    "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
    "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
    "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    :param model: Transformer Model\n",
    "    :param enc_input: The encoder input\n",
    "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
    "    :return: The target input\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
    "    terminal = False\n",
    "    next_symbol = start_symbol\n",
    "    while not terminal:         \n",
    "        dec_input = torch.cat([dec_input.detach(),torch.tensor([[next_symbol]],dtype=enc_input.dtype)],-1)\n",
    "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "        projected = model.projection(dec_outputs)\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        next_word = prob.data[-1]\n",
    "        next_symbol = next_word\n",
    "        if next_symbol == tgt_vocab[\".\"]:\n",
    "            terminal = True\n",
    "        print(next_word)            \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9024e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(4)\n",
      "tensor(8)\n",
      "tensor([1, 2, 3, 5, 0]) -> ['i', 'want', 'a', 'beer', '.']\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(4)\n",
      "tensor(8)\n",
      "tensor([1, 2, 3, 4, 0]) -> ['i', 'want', 'a', 'beer', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "enc_inputs, _, _ = next(iter(loader))\n",
    "enc_inputs = enc_inputs#.cuda()\n",
    "for i in range(len(enc_inputs)):\n",
    "    greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\n",
    "    predict, _, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e2c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
